{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e137fec2",
   "metadata": {},
   "source": [
    "### Notebook hotfix (applied 2025-08-20T11:37:08 UTC)\n",
    "\n",
    "- Fixed CUDA device-side assert during evaluation by sanitizing token IDs in `score_next_items`.\n",
    "- Masked PAD/MASK predictions and clipped logits to `num_items`.\n",
    "- Light hyperparameter tweaks intended to keep training ≤10% slower while improving Recall@10 / NDCG@10:\n",
    "  - `mask_prob=0.20`\n",
    "  - `weight_decay=0.01`, `lr_scheduler='cosine'`, `warmup_ratio=0.10`\n",
    "  - `dropout` +0.05 (cap 0.35)\n",
    "  - `max_len` increased by ~10%\n",
    "  - learning rate +15% with warmup/decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980720fa",
   "metadata": {},
   "source": [
    "# ML-20M — BERT4Rec Baseline + Collective Experiments (RQ2-style)\n",
    "**Built:** 2025-08-13 14:54\n",
    "\n",
    "This notebook **keeps the baseline model and training loop from `ml-20bert4rec`** and adds **all collective experimental conditions** inspired by `bert4rec_collectives_rq2_paper`:\n",
    "- Embedding-based user clustering (KMeans), farthest-cluster seeding\n",
    "- Collective construction with homogeneity **p**\n",
    "- Promote/Demote scenarios for two collectives (A/B)\n",
    "- Deterministic rating edits and **retraining from the baseline weights**\n",
    "- Relative HR@K on targeted item sets (A and B)\n",
    "- Results export + quick plot\n",
    "\n",
    "Printing/log messages mirror the paper notebook (e.g., *Device:*, *Running RQ2 grid...*, *Seed clusters: ...*, *Saved rq2_results_relative_hr.csv*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4796458",
   "metadata": {},
   "source": [
    "\n",
    "> **Fix applied (Aug 20, 2025):** Validation Recall@K/NDCG@K previously evaluated on single-item sequences,\n",
    "> which produced zeros. `prepare_baseline` now evaluates on `user_valid_full[u] = user_train[u] + user_valid[u]`,\n",
    "> ensuring there is prefix context (cond) and a target. The evaluator `recall_ndcg_at_k` expects sequences with\n",
    "> length ≥ 2 and uses `cond = seq[:-1]`, `target = seq[-1]`. No change to the dataset split itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdd2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edit_ratings(df, users_idx, target_items_raw, action, user_decoder, promote_value=5.0, demote_value=1.0):\n",
    "    \"\"\"Return a new ratings DataFrame where all (user,item) in the target set\n",
    "    for the given users are overwritten (and added if missing) with a fixed value.\n",
    "    users_idx: list of user indices; user_decoder maps idx -> raw userId.\n",
    "    action: 'promote' (set to promote_value) or 'demote' (set to demote_value).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "    if not len(target_items_raw):\n",
    "        return df\n",
    "\n",
    "    target_items_raw = set(target_items_raw)\n",
    "    user_raw_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_raw_ids:\n",
    "        return df\n",
    "\n",
    "    value = promote_value if action == 'promote' else demote_value\n",
    "\n",
    "    # Remove existing rows for these (user,item) pairs\n",
    "    mask_users = df['userId'].isin(user_raw_ids)\n",
    "    mask_items = df['movieId'].isin(target_items_raw)\n",
    "    df = df.loc[~(mask_users & mask_items)].reset_index(drop=True)\n",
    "\n",
    "    # Add overwritten pairs\n",
    "    new_rows = pd.DataFrame({\n",
    "        'userId': np.repeat(user_raw_ids, len(target_items_raw)),\n",
    "        'movieId': np.tile(list(target_items_raw), len(user_raw_ids)),\n",
    "        'rating':  value\n",
    "    })\n",
    "    df = pd.concat([df, new_rows], ignore_index=True)\n",
    "    # enforce integer dtypes for ids\n",
    "    df = df.astype({'userId': 'int64', 'movieId': 'int64'})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83037f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", DEVICE)\n",
    "SEED_MODE = \"maxdist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c08804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# --- Baseline config (from ml-20bert4rec) ---\n",
    "config = {\n",
    "    'data_path' : r'C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m',  # ML-20M\n",
    "    'max_len' : 50,\n",
    "    'hidden_units' : 256,\n",
    "    'num_heads' : 2,\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate' : 0.1,\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 128,\n",
    "    'num_epochs' : 17,\n",
    "    'num_workers' : 2,\n",
    "    'mask_prob' : 0.15,\n",
    "    'seed' : 42,\n",
    "    \n",
    "}\n",
    "\n",
    "def fix_seed(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed(config['seed'])\n",
    "print(\"Seed set to\", config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "117c403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class MakeSequenceDataSet():\n",
    "    def __init__(self, config):\n",
    "        dat1 = os.path.join(config['data_path'], 'ratings.dat')\n",
    "        dat2 = os.path.join(config['data_path'], 'rating.dat')\n",
    "        dat_path = dat1 if os.path.exists(dat1) else dat2\n",
    "\n",
    "        if not os.path.exists(dat_path):\n",
    "            raise FileNotFoundError(f\"Could not find ratings.dat at {config['data_path']}\")\n",
    "\n",
    "        print(\"Loading:\", dat_path)\n",
    "\n",
    "        # Correct read for MovieLens 1M .dat format\n",
    "        self.df = pd.read_csv(\n",
    "            dat_path,\n",
    "            sep=\"::\",\n",
    "            engine=\"python\",      # Required for multi-char separator\n",
    "            header=None,          # No header row in the file\n",
    "            names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "            encoding=\"latin-1\"    # Avoids encoding errors\n",
    "        )\n",
    "\n",
    "        must = {'userId','movieId','rating','timestamp'}\n",
    "        missing = must - set(self.df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['movieId'].apply(lambda x: self.item_encoder[x] + 1)  # 1..num_item\n",
    "        self.df['user_idx'] = self.df['userId'].apply(lambda x: self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp'])\n",
    "\n",
    "        # temporal train/valid/test split\n",
    "        self.user_train, self.user_valid, self.user_test = self.generate_sequence_data()\n",
    "        print(\"Users with sequences:\", len(self.user_train), \"| Items:\", self.num_item)\n",
    "\n",
    "\n",
    "    def generate_encoder_decoder(self, col:str):\n",
    "        encoder, decoder = {}, {}\n",
    "        ids = self.df[col].unique()\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "        return encoder, decoder\n",
    "\n",
    "    def generate_sequence_data(self):\n",
    "        users = defaultdict(list)\n",
    "        user_train, user_valid, user_test = {}, {}, {}\n",
    "        for user, g in self.df.groupby('user_idx'):\n",
    "            seq = g['item_idx'].tolist()\n",
    "            if len(seq) < 3:\n",
    "                continue\n",
    "            users[user] = seq\n",
    "        for user, seq in users.items():\n",
    "            user_train[user] = seq[:-2]\n",
    "            user_valid[user] = [seq[-2]]\n",
    "            user_test[user]  = [seq[-1]]\n",
    "        return user_train, user_valid, user_test\n",
    "\n",
    "    def get_splits(self):\n",
    "        return self.user_train, self.user_valid, self.user_test\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=4.0):\n",
    "    # Keep only ratings >= threshold as positive interactions\n",
    "    df = df.copy()\n",
    "    df = df[df['rating'] >= threshold].sort_values(['userId','timestamp']).reset_index(drop=True)\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        user_pos[row['user_idx']].append(row['item_idx'])\n",
    "    # filter out empties\n",
    "    return {u: seq for u, seq in user_pos.items() if len(seq) >= 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Validation metrics: next-item Recall@K and NDCG@K (correct target handling) ===\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_ndcg_at_k(model, user_pos, num_items, max_len, k=10, batch_size=2048, device=None):\n",
    "    \"\"\"\n",
    "    Standard next-item eval:\n",
    "      cond = seq[:-1], target = seq[-1].\n",
    "    We DO NOT mask the target even if it appeared earlier in cond.\n",
    "    \"\"\"\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    MASK = num_items + 1  # IMPORTANT: this repo uses MASK = num_items + 1\n",
    "\n",
    "    rows, targets, seen_lists = [], [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        cond = seq[:-1]\n",
    "        tgt  = seq[-1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1:\n",
    "            s = s[-(max_len - 1):]  # leave room for [MASK]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        rows.append([PAD]*pad + list(s) + [MASK])\n",
    "        targets.append(tgt)\n",
    "        seen_lists.append(list(set(s)))  # we'll unmask target below\n",
    "\n",
    "    if not rows:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    X = torch.tensor(rows, dtype=torch.long, device=dev)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=dev)\n",
    "\n",
    "    # Determine logits width V from model\n",
    "    V = model(X[:1])[:, -1, :].shape[1]\n",
    "\n",
    "    # Build suppression mask (B,V) for PAD/MASK/seen EXCEPT the target\n",
    "    B = X.shape[0]\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V:\n",
    "        seen_mask[:, MASK] = True\n",
    "\n",
    "    r_idx, c_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        items_set = set(it for it in items if 0 <= it < V)\n",
    "        tgt_i = int(targets[i].item())\n",
    "        if 0 <= tgt_i < V and tgt_i in items_set:\n",
    "            items_set.remove(tgt_i)\n",
    "        if items_set:\n",
    "            r_idx.extend([i]*len(items_set))\n",
    "            c_idx.extend(list(items_set))\n",
    "    if r_idx:\n",
    "        seen_mask[torch.tensor(r_idx, device=dev), torch.tensor(c_idx, device=dev)] = True\n",
    "\n",
    "    # Sanity check: target must not be masked\n",
    "    assert not seen_mask[torch.arange(B, device=dev), targets.clamp_min(0).clamp_max(V-1)].any(), \"Target was masked!\"\n",
    "\n",
    "    hits = 0.0\n",
    "    ndcgs = 0.0\n",
    "    for s in range(0, B, batch_size):\n",
    "        e = min(B, s+batch_size)\n",
    "        logits = model(X[s:e])[:, -1, :]\n",
    "        logits = logits.masked_fill(seen_mask[s:e], -1e9)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        tgts = targets[s:e].unsqueeze(1)\n",
    "\n",
    "        hit_rows = (topk == tgts).any(dim=1).float()\n",
    "        hits += hit_rows.sum().item()\n",
    "\n",
    "        where = (topk == tgts).nonzero(as_tuple=False)\n",
    "        if where.numel() > 0:\n",
    "            ndcgs += (1.0 / torch.log2(where[:,1].float() + 2.0)).sum().item()\n",
    "\n",
    "    n = float(B)\n",
    "    return hits / n, ndcgs / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5a74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, max_len, num_item, mask_prob=0.15):\n",
    "        self.max_len = max_len\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "        self.users = list(user_train.keys())\n",
    "        self.inputs, self.labels = [], []\n",
    "        for user in self.users:\n",
    "            seq = user_train[user]\n",
    "            tokens = seq[-max_len:] if len(seq) > max_len else [0]*(max_len-len(seq)) + seq\n",
    "            masked_tokens, label_tokens = self.mask_sequence(tokens)\n",
    "            self.inputs.append(masked_tokens)\n",
    "            self.labels.append(label_tokens)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def mask_sequence(self, tokens):\n",
    "        masked_tokens = tokens.copy()\n",
    "        labels = [0]*len(tokens)  # PAD=0\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == 0:\n",
    "                continue\n",
    "            if random.random() < self.mask_prob:\n",
    "                labels[i] = tokens[i]  # store original ID\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    masked_tokens[i] = self.num_item + 1  # MASK token in inputs only\n",
    "                elif prob < 0.9:\n",
    "                    masked_tokens[i] = random.randint(1, self.num_item)\n",
    "        return masked_tokens, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38fb6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model): super().__init__(); self.pe = nn.Embedding(max_len, d_model)\n",
    "    def forward(self, x): return self.pe.weight.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512): super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None: p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__(); assert d_model % h == 0\n",
    "        self.d_k = d_model // h; self.h = h\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model); self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B = query.size(0)\n",
    "        query, key, value = [l(x).view(B, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi)*(x + 0.044715*torch.pow(x,3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__(); self.w_1 = nn.Linear(d_model, d_ff); self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout); self.activation = GELU()\n",
    "    def forward(self, x): return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6): super().__init__(); self.a_2 = nn.Parameter(torch.ones(features)); self.b_2 = nn.Parameter(torch.zeros(features)); self.eps = eps\n",
    "    def forward(self, x): mean = x.mean(-1, keepdim=True); std = x.std(-1, keepdim=True); return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout): super().__init__(); self.norm = LayerNorm(size); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward); return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_max_len, num_items, bert_num_blocks, bert_num_heads,\n",
    "                 bert_hidden_units, bert_dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = bert_max_len; self.num_items = num_items\n",
    "        self.hidden = bert_hidden_units\n",
    "        self.embedding = BERTEmbedding(vocab_size=num_items+2, embed_size=self.hidden, max_len=bert_max_len, dropout=bert_dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(self.hidden, bert_num_heads, self.hidden*4, bert_dropout) for _ in range(bert_num_blocks)])\n",
    "        self.out = nn.Linear(self.hidden, num_items + 2)  # 0..num_items\n",
    "    def forward_hidden(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        h = self.embedding(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            h = transformer.forward(h, mask)\n",
    "        return h\n",
    "    def forward(self, x):\n",
    "        h = self.forward_hidden(x)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ca1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device=DEVICE):\n",
    "    model.train(); loss_val = 0.0\n",
    "    for seq, labels in tqdm(data_loader):\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        logits = model(seq).view(-1, model.out.out_features)  # (bs*t, vocab)\n",
    "        labels = labels.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    return loss_val / max(1, len(data_loader))\n",
    "\n",
    "def prepare_baseline(config):\n",
    "    \"\"\"\n",
    "    Trains a baseline BERT4Rec model and evaluates next-item Recall/NDCG correctly.\n",
    "    IMPORTANT: Validation is evaluated on sequences built as\n",
    "      user_valid_full[u] = user_train[u] + user_valid[u]\n",
    "    so that cond = prefix, target = last (valid) item.\n",
    "    \"\"\"\n",
    "    # 1) Build sequences\n",
    "    mds = MakeSequenceDataSet(config)\n",
    "    user_train, user_valid, user_test = mds.user_train, mds.user_valid, mds.user_test\n",
    "    num_item = mds.num_item\n",
    "\n",
    "    # 2) DataLoader from training prefixes\n",
    "    train_ds = BERTRecDataSet(user_train, max_len=config['max_len'], num_item=num_item, mask_prob=0.15)\n",
    "    dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
    "\n",
    "    # 3) Model\n",
    "    model = BERT(\n",
    "        bert_max_len=config['max_len'],\n",
    "        num_items=num_item,   # this class internally adds +2 for PAD/MASK head\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # 4) Validation spec: build full sequences with context\n",
    "    user_valid_full = {\n",
    "        u: (user_train[u] + user_valid[u])\n",
    "        for u in user_valid\n",
    "        if u in user_train and len(user_train[u]) >= 1 and len(user_valid[u]) == 1\n",
    "    }\n",
    "\n",
    "    K = int(config.get('TOPK', 10)) if isinstance(config, dict) else 10\n",
    "    for epoch in range(1, int(config['num_epochs']) + 1):\n",
    "        l = train_one_epoch(model, criterion, opt, dl, device=DEVICE)\n",
    "        print(f'Epoch: {epoch:3d}| Train loss: {l:.5f}')\n",
    "        # Evaluate next-item ranking on validation (prefix->valid_item)\n",
    "        _was = model.training\n",
    "        model.eval()\n",
    "        try:\n",
    "            _rec, _ndcg = recall_ndcg_at_k(\n",
    "                model, user_valid_full, num_item, config['max_len'],\n",
    "                k=K, batch_size=2048, device=DEVICE\n",
    "            )\n",
    "        finally:\n",
    "            if _was: model.train()\n",
    "        print(f\"[VAL] Recall@{K}={_rec:.4f} | NDCG@{K}={_ndcg:.4f}\")\n",
    "    torch.save(model.state_dict(), \"baseline_best.pt\")\n",
    "    print(\"Saved baseline_best.pt\")\n",
    "    return mds, user_train, user_valid, user_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee7bd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embedding_from_model(model, seq, num_items, max_len):\n",
    "    # Average of valid hidden states\n",
    "    s = seq[-max_len:] if len(seq) > max_len else seq\n",
    "    PAD = 0; MASK = num_items + 2\n",
    "    pad = max_len - len(s)\n",
    "    x = torch.tensor([[PAD]*pad + s], dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model.forward_hidden(x)[0]  # (L, H)\n",
    "        valid = h[pad:pad+len(s)]\n",
    "        return valid.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "def compute_user_embeddings(model, train_seqs, num_items, max_len):\n",
    "    user_vecs = {}\n",
    "    for u, seq in tqdm(list(train_seqs.items()), desc=\"user embeddings\"):\n",
    "        user_vecs[u] = user_embedding_from_model(model, seq, num_items, max_len)\n",
    "    U = np.stack([user_vecs[u] for u in user_vecs.keys()])\n",
    "    user_index = list(user_vecs.keys())\n",
    "    return U, user_index\n",
    "\n",
    "def build_collective_quota(seed_cluster, labels, users, size, p, *, seed=None):\n",
    "    import random\n",
    "    rnd = random.Random(seed) if seed is not None else random\n",
    "\n",
    "    # split pools\n",
    "    seed_users  = [u for u, lab in zip(users, labels) if lab == seed_cluster]\n",
    "    other_users = [u for u, lab in zip(users, labels) if lab != seed_cluster]\n",
    "    rnd.shuffle(seed_users); rnd.shuffle(other_users)\n",
    "\n",
    "    # exact target counts\n",
    "    n_seed  = min(len(seed_users), int(round(p * size)))\n",
    "    n_other = min(len(other_users), size - n_seed)\n",
    "\n",
    "    members = seed_users[:n_seed] + other_users[:n_other]\n",
    "\n",
    "    # backfill if one pool was too small\n",
    "    if len(members) < size:\n",
    "        spill = seed_users[n_seed:] + other_users[n_other:]\n",
    "        rnd.shuffle(spill)\n",
    "        members += spill[: size - len(members)]\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "def top_items_for_collective(members, train_seqs, topn=10):\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for u in members:\n",
    "        for it in set(train_seqs.get(u, [])):  # count each item once per user\n",
    "            c[it] += 1\n",
    "    return [it for it, _ in c.most_common(topn)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "480a9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: resolve number of items robustly from mds/model\n",
    "def resolve_num_items(mds, model):\n",
    "    # Try common dataset attributes first\n",
    "    for attr in ('num_items', 'n_items', 'item_vocab_size'):\n",
    "        if hasattr(mds, attr):\n",
    "            try:\n",
    "                n = int(getattr(mds, attr))\n",
    "                if n > 0:\n",
    "                    return n\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Try encoder sizes\n",
    "    for attr in ('item_encoder', 'item2id', 'item_to_idx'):\n",
    "        enc = getattr(mds, attr, None)\n",
    "        if isinstance(enc, dict) and len(enc) > 0:\n",
    "            return int(len(enc))\n",
    "    # Try model output head (often vocab size = num_items + specials)\n",
    "    out_features = getattr(getattr(model, 'out', None), 'out_features', None)\n",
    "    if isinstance(out_features, int) and out_features > 0:\n",
    "        # Heuristic: BERT4Rec-style uses tokens: 0=PAD, +1=[CLS], +2=[MASK]\n",
    "        # So num_items ≈ out_features - 3\n",
    "        return max(1, int(out_features) - 3)\n",
    "    raise RuntimeError(\"Could not resolve number of items from mds/model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "877dc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_from_baseline_on_ratings(\n",
    "    baseline_state_path,\n",
    "    ratings_df,\n",
    "    mds,\n",
    "    epochs=8,\n",
    "    lr=5e-4,\n",
    "    *,\n",
    "    eval_specs=None,   # list of {\"name\",\"members_idx\",\"item_set_enc\",\"baseline_hr\", [\"solo_hr\"]}\n",
    "    k=None,\n",
    "    device=None,\n",
    "    **kwargs           # swallow unused args for compatibility\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrain from baseline weights on a modified ratings_df and print, per epoch:\n",
    "      - training loss\n",
    "      - HR@K for each eval spec\n",
    "      - relative HR (vs baseline_hr)\n",
    "      - constructiveness CT = rel_joint - rel_solo (if solo_hr provided)\n",
    "\n",
    "    Returns: (mds, user_pos_mod, None, None, model)\n",
    "    \"\"\"\n",
    "    dev = device if device is not None else DEVICE\n",
    "    K = k if k is not None else (config.get('TOPK', 10) if isinstance(config, dict) else 10)\n",
    "\n",
    "    # 0) Defensive config defaults\n",
    "    max_len = int(config.get('max_len', 50))\n",
    "    mask_prob = float(config.get('mask_prob', 0.15))\n",
    "    batch_size = int(config.get('batch_size', 128))\n",
    "\n",
    "    # 1) Rebuild sequences from edited ratings with threshold=4.0\n",
    "    user_pos_mod = rebuild_sequences_from_df(\n",
    "        ratings_df,\n",
    "        mds.item_encoder,\n",
    "        mds.user_encoder,\n",
    "        threshold=4.0\n",
    "    )\n",
    "    user_pos_mod = {u: [int(x) for x in seq] for u, seq in user_pos_mod.items()}\n",
    "    user_pos_mod = {u: seq for u, seq in user_pos_mod.items() if len(seq) > 0}\n",
    "    if not user_pos_mod:\n",
    "        raise ValueError(\"No user sequences after rebuild; check ratings_df and threshold.\")\n",
    "\n",
    "    # 1b) Compute num_item to cover all observed ids\n",
    "    enc_num_item = getattr(mds, 'num_item', len(getattr(mds, 'item_encoder', {})))\n",
    "    max_seen = max(max(seq) for seq in user_pos_mod.values())\n",
    "    num_item = max(enc_num_item, max_seen)\n",
    "\n",
    "    # 2) DataLoader\n",
    "    ds = BERTRecDataSet(\n",
    "        user_pos_mod,\n",
    "        max_len=max_len,\n",
    "        num_item=num_item,\n",
    "        mask_prob=mask_prob\n",
    "    )\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    # 3) Model from baseline checkpoint\n",
    "    model = BERT(\n",
    "        bert_max_len=max_len,\n",
    "        num_items=num_item + 2,   # PAD=0 plus [CLS]/[MASK]\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(dev)\n",
    "\n",
    "    # ----- Vocab-size–aware load: pad/slice embedding & output heads -----\n",
    "    ckpt = torch.load(baseline_state_path, map_location=dev)\n",
    "    model_sd = model.state_dict()\n",
    "\n",
    "    def _resize_like_param(param_name, src_tensor, dst_tensor):\n",
    "        \"\"\"Resize src_tensor to match dst_tensor on dim 0 by copy-overlap + zero pad.\"\"\"\n",
    "        if src_tensor.shape == dst_tensor.shape:\n",
    "            return src_tensor\n",
    "        # Only support resizing first dimension (vocab) and keeping others\n",
    "        if src_tensor.ndim != dst_tensor.ndim or src_tensor.shape[1:] != dst_tensor.shape[1:]:\n",
    "            # Fallback: keep destination (random init) if shapes are incompatible\n",
    "            return dst_tensor\n",
    "        new_t = dst_tensor.clone()\n",
    "        n = min(src_tensor.shape[0], dst_tensor.shape[0])\n",
    "        new_t[:n] = src_tensor[:n]\n",
    "        return new_t\n",
    "\n",
    "    # Candidate keys for vocab-tied layers across common BERT4Rec variants\n",
    "    vocab_keys = [\n",
    "        'embedding.token.weight',          # seen in your error\n",
    "        'bert.item_embedding.weight',\n",
    "        'item_embedding.weight',\n",
    "        'bert.embeddings.word_embeddings.weight',\n",
    "    ]\n",
    "    out_w_keys = [\n",
    "        'out.weight',                      # seen in your error\n",
    "        'bert.prediction.weight',\n",
    "        'prediction.weight',\n",
    "    ]\n",
    "    out_b_keys = [\n",
    "        'out.bias',\n",
    "        'bert.prediction.bias',\n",
    "        'prediction.bias',\n",
    "    ]\n",
    "\n",
    "    # Build a patched checkpoint dict\n",
    "    patched = {}\n",
    "    for k, v in ckpt.items():\n",
    "        if k in model_sd:\n",
    "            if k in vocab_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_w_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_b_keys:\n",
    "                # Bias: resize on dim 0\n",
    "                if v.shape != model_sd[k].shape:\n",
    "                    nb = min(v.shape[0], model_sd[k].shape[0])\n",
    "                    new_b = model_sd[k].clone()\n",
    "                    new_b[:nb] = v[:nb]\n",
    "                    patched[k] = new_b\n",
    "                else:\n",
    "                    patched[k] = v\n",
    "            else:\n",
    "                # default: keep as-is if shape matches; otherwise keep destination param\n",
    "                if v.shape == model_sd[k].shape:\n",
    "                    patched[k] = v\n",
    "                else:\n",
    "                    patched[k] = model_sd[k]\n",
    "        else:\n",
    "            # keys not in current model are ignored\n",
    "            pass\n",
    "\n",
    "    # Load with strict=False to allow any unmatched keys\n",
    "    missing, unexpected = model.load_state_dict(patched, strict=False)\n",
    "    if missing:\n",
    "        print(f\"[info] Missing keys after load (expected if heads/embeddings expanded): {missing}\")\n",
    "    if unexpected:\n",
    "        print(f\"[info] Unexpected keys ignored from checkpoint: {unexpected}\")\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "    grad_clip = float(config.get('grad_clip', 1.0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def _train_one_epoch_nan_safe(model, crit, opt, dl):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for batch in dl:\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: (v.to(dev) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
    "                inputs = batch.get('input_ids') or batch.get('seqs') or next(iter(batch.values()))\n",
    "                labels = batch.get('labels') or batch.get('targets') or batch.get('target_ids')\n",
    "                attn = batch.get('attention_mask') or batch.get('attn_mask')\n",
    "            else:\n",
    "                inputs = batch[0].to(dev)\n",
    "                labels = batch[1].to(dev)\n",
    "                attn = batch[2].to(dev) if len(batch) > 2 else None\n",
    "\n",
    "            if labels is None:\n",
    "                continue\n",
    "            valid = (labels != 0)\n",
    "            if valid.sum().item() == 0:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(inputs)  # (B, T, V)\n",
    "            B, T, V = logits.shape\n",
    "            loss = crit(logits.view(B*T, V), labels.view(B*T))\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * valid.sum().item()\n",
    "            total_tokens += valid.sum().item()\n",
    "\n",
    "        return (total_loss / max(total_tokens, 1))\n",
    "\n",
    "    def _coerce_itemset(item_set_enc, target_num_item):\n",
    "        if isinstance(item_set_enc, (list, tuple)):\n",
    "            v = torch.tensor(item_set_enc, dtype=torch.float32)\n",
    "        elif torch.is_tensor(item_set_enc):\n",
    "            v = item_set_enc.float().cpu()\n",
    "        else:\n",
    "            raise TypeError(\"item_set_enc must be list/tuple/tensor\")\n",
    "        if v.ndim != 1:\n",
    "            v = v.view(-1)\n",
    "        if len(v) < target_num_item:\n",
    "            v = torch.cat([v, torch.zeros(target_num_item - len(v))], dim=0)\n",
    "        elif len(v) > target_num_item:\n",
    "            v = v[:target_num_item]\n",
    "        return v.to(dev)\n",
    "\n",
    "    def _eval_one(spec):\n",
    "        members = spec['members_idx']\n",
    "        subset = {u: user_pos_mod[u] for u in members if u in user_pos_mod}\n",
    "        aligned_itemset = _coerce_itemset(spec['item_set_enc'], num_item)\n",
    "\n",
    "        hr = float(hr_for_itemset(\n",
    "            model,\n",
    "            subset,\n",
    "            aligned_itemset,\n",
    "            num_item,\n",
    "            max_len,\n",
    "            K\n",
    "        ))\n",
    "\n",
    "        rel = None\n",
    "        base = spec.get('baseline_hr', None)\n",
    "        if base is not None and base > 0:\n",
    "            rel = hr / base\n",
    "        ct = None\n",
    "        solo = spec.get('solo_hr', None)\n",
    "        if base is not None and base > 0 and solo is not None:\n",
    "            ct = (hr / base) - (solo / base)\n",
    "        return hr, rel, ct\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_loss = _train_one_epoch_nan_safe(model, crit, opt, dl)\n",
    "        if eval_specs:\n",
    "            parts = []\n",
    "            for spec in eval_specs:\n",
    "                try:\n",
    "                    hr, rel, ct = _eval_one(spec)\n",
    "                    msg = f\"{spec['name']}: HR@{K}={hr:.4f}\"\n",
    "                    base_local = spec.get('baseline_hr', None)\n",
    "                    if rel is not None:\n",
    "                        msg += f\" | rel={rel:.3f}\"\n",
    "                    elif base_local is not None and base_local == 0:\n",
    "                        msg += \" | rel=NA (baseline=0)\"\n",
    "                    if ct is not None:\n",
    "                        msg += f\" | CT={ct:.3f}\"\n",
    "                except Exception as e:\n",
    "                    msg = f\"{spec.get('name','?')}: EVAL-ERROR {type(e).__name__}: {e}\"\n",
    "                parts.append(msg)\n",
    "            print(f\"Epoch: {epoch:3d} | loss: {avg_loss:.5f} || \" + \" || \".join(parts), flush=True)\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch:3d} | Train loss: {avg_loss:.5f}\", flush=True)\n",
    "\n",
    "    return mds, user_pos_mod, None, None, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c83fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def hr_for_itemset(model, user_pos, item_set, num_items, max_len, k=10):\n",
    "    PAD = 0\n",
    "    V = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                V = int(m.num_embeddings); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    MASK = 1 if (V is not None and V > 1) else (num_items + 2)\n",
    "    if V is not None and not (0 <= MASK < V):\n",
    "        MASK = 1 if V > 1 else 0\n",
    "\n",
    "    dev = next(model.parameters()).device\n",
    "    inps, seen_lists = [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if not seq: continue\n",
    "        cond = seq[:-2] if len(seq) >= 2 else seq[:1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1: s = s[-(max_len - 1):]\n",
    "        if V is not None:\n",
    "            s = [int(t) for t in s if 0 <= int(t) < V]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        inps.append([PAD]*pad + list(s) + [MASK])\n",
    "        seen_lists.append(list(set(s)))\n",
    "    if not inps: return 0.0\n",
    "\n",
    "    X = torch.tensor(inps, dtype=torch.long, device=dev)\n",
    "    B = X.shape[0]\n",
    "\n",
    "    if V is None:\n",
    "        V = model(X[:1])[:, -1, :].shape[1]\n",
    "\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    if 0 <= PAD < V: seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V: seen_mask[:, MASK] = True\n",
    "\n",
    "    row_idx, col_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        valid = [it for it in items if 0 <= it < V]\n",
    "        if valid:\n",
    "            row_idx.extend([i]*len(valid))\n",
    "            col_idx.extend(valid)\n",
    "    if row_idx:\n",
    "        seen_mask[torch.tensor(row_idx, device=dev), torch.tensor(col_idx, device=dev)] = True\n",
    "\n",
    "    target_mask = torch.zeros(V, dtype=torch.bool, device=dev)\n",
    "    cand = list(set(int(i) for i in item_set))\n",
    "    cand = [i for i in cand if 0 <= i < V]\n",
    "    if cand:\n",
    "        target_mask[torch.tensor(cand, device=dev)] = True\n",
    "    if 0 <= PAD < V: target_mask[PAD] = False\n",
    "    if 0 <= MASK < V: target_mask[MASK] = False\n",
    "\n",
    "    hits = 0; total = 0\n",
    "    batch_size = 2048\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(B, start + batch_size)\n",
    "        Xb = X[start:end]\n",
    "        logits = model(Xb)[:, -1, :]\n",
    "        if isinstance(num_items, int) and num_items < logits.shape[1]:\n",
    "            logits[:, num_items:] = -1e9\n",
    "        logits = logits.masked_fill(seen_mask[start:end], -1e9)\n",
    "        topk_idx = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit_rows = target_mask[topk_idx].any(dim=1)\n",
    "        hits += hit_rows.sum().item()\n",
    "        total += Xb.shape[0]\n",
    "    return float(hits / max(1, total))\n",
    "\n",
    "@torch.no_grad()\n",
    "def relative_hr(model_variant, model_baseline, user_pos, item_set, num_items, max_len, k=10):\n",
    "    g_base = hr_for_itemset(model_baseline, user_pos, item_set, num_items, max_len, k=k)\n",
    "    g_var  = hr_for_itemset(model_variant,   user_pos, item_set, num_items, max_len, k=k)\n",
    "    rel    = (g_var / g_base) if g_base > 0 else 0.0\n",
    "    return rel, g_var, g_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e30a140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_next_items(model, cond_seq, num_items, max_len):\n",
    "    \"\"\"\n",
    "    Robust scorer that:\n",
    "    - Detects the model's embedding vocab size by scanning modules for nn.Embedding.\n",
    "    - Ensures all token ids are within [0, vocab_size).\n",
    "    - Uses PAD=0 and MASK=1 when possible (common BERT4Rec scheme).\n",
    "    - Suppresses PAD, MASK, and seen tokens in logits.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 1) Find an embedding to infer vocab_size\n",
    "    vocab_size = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                vocab_size = int(m.num_embeddings)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    PAD = 0\n",
    "    MASK = 1 if vocab_size is not None and vocab_size > 1 else (num_items + 2)\n",
    "\n",
    "    # 2) Build masked sequence with strict truncation\n",
    "    s = cond_seq[-max_len:] if len(cond_seq) > max_len else cond_seq\n",
    "    if len(s) > max_len - 1:\n",
    "        s = s[-(max_len - 1):]\n",
    "\n",
    "    # 3) Sanitize IDs to prevent CUDA device-side asserts\n",
    "    if vocab_size is not None:\n",
    "        s = [int(x) for x in s if 0 <= int(x) < vocab_size]\n",
    "        # Final check for MASK validity\n",
    "        if not (0 <= MASK < vocab_size):\n",
    "            MASK = 1 if vocab_size > 1 else 0\n",
    "\n",
    "    pad = max_len - len(s) - 1\n",
    "    if pad < 0: pad = 0\n",
    "    seq_ids = [PAD]*pad + s + [MASK]\n",
    "\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    inp = torch.tensor([seq_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # 4) Forward -> logits for the MASK position\n",
    "    out = model(inp)\n",
    "    logits = out[0, -1].clone()\n",
    "\n",
    "    # 5) Suppress invalid or special predictions\n",
    "    if 0 <= PAD < logits.numel():   logits[PAD] = -1e9\n",
    "    if 0 <= MASK < logits.numel():  logits[MASK] = -1e9\n",
    "    for seen in set(s):\n",
    "        if 0 <= seen < logits.numel():\n",
    "            logits[seen] = -1e9\n",
    "\n",
    "    # 6) If num_items provided < logits size, clip tail so metrics only consider real items\n",
    "    if isinstance(num_items, int) and num_items < logits.numel():\n",
    "        logits[num_items:] = -1e9\n",
    "\n",
    "    return logits  # (vocab,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d11c",
   "metadata": {},
   "source": [
    "# ✅ Paper-faithful item_set builders (post-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b735b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "V_TARGET = 10 if 'config' not in globals() else config.get('V', 10)\n",
    "\n",
    "def _member_mean_topV(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    if sub.empty:\n",
    "        return []\n",
    "    means = (sub.groupby('movieId')['rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(V).index.tolist())\n",
    "    return means\n",
    "\n",
    "def _member_popular(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    pop = (sub.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _global_popular(ratings_df, V):\n",
    "    pop = (ratings_df.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _encode_items(raw_ids, item_encoder):\n",
    "    enc0 = [item_encoder[r] for r in raw_ids if r in item_encoder]\n",
    "    return [e + 1 for e in enc0]  # BERT4Rec label space\n",
    "\n",
    "def build_item_set_for_members(mds, members_idx, V=10):\n",
    "    ratings_df = mds.df\n",
    "    user_decoder = mds.user_decoder\n",
    "    member_raw = [user_decoder[u] for u in members_idx if u in user_decoder]\n",
    "\n",
    "    raw = _member_mean_topV(ratings_df, member_raw, V)\n",
    "    if len(raw) < V:\n",
    "        for iid in _member_popular(ratings_df, member_raw, V*3):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "    if len(raw) < V:\n",
    "        for iid in _global_popular(ratings_df, V*5):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "\n",
    "    enc = _encode_items(raw[:V], mds.item_encoder)\n",
    "    return raw[:V], enc[:V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a015104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _cluster_centroids(U, labels, Q):\n",
    "    \"\"\"Return (Q x d) centroid matrix in user-embedding space.\"\"\"\n",
    "    centroids = []\n",
    "    for q in range(Q):\n",
    "        idx = np.where(labels == q)[0]\n",
    "        # KMeans guarantees non-empty clusters; just in case:\n",
    "        if len(idx) == 0:\n",
    "            # fallback: random user as centroid\n",
    "            centroids.append(U[np.random.randint(len(U))])\n",
    "        else:\n",
    "            centroids.append(U[idx].mean(axis=0))\n",
    "    C = np.vstack(centroids)\n",
    "    return C\n",
    "\n",
    "def _normalize_rows(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def _farthest_pair_indices(centroids, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Return indices (i, j) of the two centroids that are maximally distant.\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        Cn = _normalize_rows(centroids)\n",
    "        # cosine distance = 1 - cosine similarity\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        # Euclidean\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    # ignore diagonal\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "    ij = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "    return int(ij[0]), int(ij[1]), float(dists[ij])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c383f8",
   "metadata": {},
   "source": [
    "# 🚀 RQ2 Experiment (baseline → collectives → item_set → interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89661ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m\\ratings.dat\n",
      "Users with sequences: 6040 | Items: 3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 7.86362\n",
      "[VAL] Recall@10=0.0301 | NDCG@10=0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   2| Train loss: 7.14407\n",
      "[VAL] Recall@10=0.0300 | NDCG@10=0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   3| Train loss: 6.72533\n",
      "[VAL] Recall@10=0.0399 | NDCG@10=0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   4| Train loss: 6.23599\n",
      "[VAL] Recall@10=0.0541 | NDCG@10=0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   5| Train loss: 5.75535\n",
      "[VAL] Recall@10=0.0621 | NDCG@10=0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   6| Train loss: 5.33052\n",
      "[VAL] Recall@10=0.0742 | NDCG@10=0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   7| Train loss: 4.99274\n",
      "[VAL] Recall@10=0.0714 | NDCG@10=0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   8| Train loss: 4.70466\n",
      "[VAL] Recall@10=0.0801 | NDCG@10=0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   9| Train loss: 4.45841\n",
      "[VAL] Recall@10=0.0823 | NDCG@10=0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10| Train loss: 4.22730\n",
      "[VAL] Recall@10=0.0816 | NDCG@10=0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11| Train loss: 4.01124\n",
      "[VAL] Recall@10=0.0833 | NDCG@10=0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12| Train loss: 3.80999\n",
      "[VAL] Recall@10=0.0906 | NDCG@10=0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13| Train loss: 3.61568\n",
      "[VAL] Recall@10=0.0881 | NDCG@10=0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14| Train loss: 3.45412\n",
      "[VAL] Recall@10=0.0891 | NDCG@10=0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15| Train loss: 3.29249\n",
      "[VAL] Recall@10=0.0909 | NDCG@10=0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16| Train loss: 3.13714\n",
      "[VAL] Recall@10=0.0974 | NDCG@10=0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17| Train loss: 2.98054\n",
      "[VAL] Recall@10=0.1003 | NDCG@10=0.0494\n",
      "Saved baseline_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user embeddings: 100%|██████████| 6040/6040 [00:30<00:00, 200.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[item_set] A: raw=10 enc=10  |  B: raw=10 enc=10\n",
      "0.07301324503311259\n",
      "0.11572847682119206\n",
      "Mean frequency — A: 814.4  | B: 480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.06941 || A: HR@10=0.0793 | rel=1.087\n",
      "Epoch:   2 | loss: 6.22178 || A: HR@10=0.0450 | rel=0.617\n",
      "Epoch:   3 | loss: 5.81769 || A: HR@10=0.0530 | rel=0.726\n",
      "Epoch:   4 | loss: 5.49383 || A: HR@10=0.0543 | rel=0.744\n",
      "Epoch:   5 | loss: 5.22234 || A: HR@10=0.0603 | rel=0.826\n",
      "Epoch:   6 | loss: 4.98016 || A: HR@10=0.0578 | rel=0.792\n",
      "Epoch:   7 | loss: 4.75049 || A: HR@10=0.0523 | rel=0.717\n",
      "Epoch:   8 | loss: 4.54529 || A: HR@10=0.0480 | rel=0.658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.09683 || A: HR@10=0.0689 | rel=0.944\n",
      "Epoch:   2 | loss: 6.26239 || A: HR@10=0.0580 | rel=0.794\n",
      "Epoch:   3 | loss: 5.85125 || A: HR@10=0.0807 | rel=1.105\n",
      "Epoch:   4 | loss: 5.52080 || A: HR@10=0.0792 | rel=1.084\n",
      "Epoch:   5 | loss: 5.24589 || A: HR@10=0.1214 | rel=1.663\n",
      "Epoch:   6 | loss: 5.00144 || A: HR@10=0.1414 | rel=1.937\n",
      "Epoch:   7 | loss: 4.76941 || A: HR@10=0.1946 | rel=2.665\n",
      "Epoch:   8 | loss: 4.56167 || A: HR@10=0.1555 | rel=2.130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.08251 || B: HR@10=0.2774 | rel=2.397\n",
      "Epoch:   2 | loss: 6.24821 || B: HR@10=0.2898 | rel=2.504\n",
      "Epoch:   3 | loss: 5.83983 || B: HR@10=0.2387 | rel=2.062\n",
      "Epoch:   4 | loss: 5.51225 || B: HR@10=0.1944 | rel=1.680\n",
      "Epoch:   5 | loss: 5.22506 || B: HR@10=0.1540 | rel=1.331\n",
      "Epoch:   6 | loss: 4.98419 || B: HR@10=0.1729 | rel=1.494\n",
      "Epoch:   7 | loss: 4.74940 || B: HR@10=0.1567 | rel=1.354\n",
      "Epoch:   8 | loss: 4.55459 || B: HR@10=0.1423 | rel=1.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.09683 || B: HR@10=0.2478 | rel=2.141\n",
      "Epoch:   2 | loss: 6.24371 || B: HR@10=0.2445 | rel=2.112\n",
      "Epoch:   3 | loss: 5.83080 || B: HR@10=0.1436 | rel=1.241\n",
      "Epoch:   4 | loss: 5.50462 || B: HR@10=0.1222 | rel=1.056\n",
      "Epoch:   5 | loss: 5.22498 || B: HR@10=0.1115 | rel=0.963\n",
      "Epoch:   6 | loss: 4.96495 || B: HR@10=0.1093 | rel=0.945\n",
      "Epoch:   7 | loss: 4.74419 || B: HR@10=0.0947 | rel=0.819\n",
      "Epoch:   8 | loss: 4.52963 || B: HR@10=0.1002 | rel=0.866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.09217 || A: HR@10=0.0717 | rel=0.982 | CT=0.982 || B: HR@10=0.3198 | rel=2.763 | CT=1.899\n",
      "Epoch:   2 | loss: 6.24641 || A: HR@10=0.0525 | rel=0.719 | CT=0.719 || B: HR@10=0.3776 | rel=3.263 | CT=2.399\n",
      "Epoch:   3 | loss: 5.84316 || A: HR@10=0.0472 | rel=0.646 | CT=0.646 || B: HR@10=0.4127 | rel=3.566 | CT=2.702\n",
      "Epoch:   4 | loss: 5.51284 || A: HR@10=0.0431 | rel=0.590 | CT=0.590 || B: HR@10=0.3456 | rel=2.987 | CT=2.123\n",
      "Epoch:   5 | loss: 5.23572 || A: HR@10=0.0280 | rel=0.383 | CT=0.383 || B: HR@10=0.3587 | rel=3.100 | CT=2.236\n",
      "Epoch:   6 | loss: 4.98014 || A: HR@10=0.0346 | rel=0.474 | CT=0.474 || B: HR@10=0.3297 | rel=2.849 | CT=1.985\n",
      "Epoch:   7 | loss: 4.77440 || A: HR@10=0.0255 | rel=0.349 | CT=0.349 || B: HR@10=0.2632 | rel=2.274 | CT=1.410\n",
      "Epoch:   8 | loss: 4.55201 || A: HR@10=0.0349 | rel=0.479 | CT=0.479 || B: HR@10=0.2729 | rel=2.358 | CT=1.494\n",
      "Epoch:   9 | loss: 4.36070 || A: HR@10=0.0245 | rel=0.336 | CT=0.336 || B: HR@10=0.2299 | rel=1.986 | CT=1.122\n",
      "Epoch:  10 | loss: 4.17875 || A: HR@10=0.0278 | rel=0.381 | CT=0.381 || B: HR@10=0.2761 | rel=2.386 | CT=1.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.11088 || A: HR@10=0.1116 | rel=1.529 | CT=0.159 || B: HR@10=0.2406 | rel=2.079 | CT=2.079\n",
      "Epoch:   2 | loss: 6.25129 || A: HR@10=0.0725 | rel=0.994 | CT=-0.376 || B: HR@10=0.1583 | rel=1.368 | CT=1.368\n",
      "Epoch:   3 | loss: 5.83199 || A: HR@10=0.0904 | rel=1.239 | CT=-0.131 || B: HR@10=0.1292 | rel=1.116 | CT=1.116\n",
      "Epoch:   4 | loss: 5.51357 || A: HR@10=0.1313 | rel=1.799 | CT=0.429 || B: HR@10=0.1078 | rel=0.932 | CT=0.932\n",
      "Epoch:   5 | loss: 5.22459 || A: HR@10=0.1338 | rel=1.833 | CT=0.463 || B: HR@10=0.0750 | rel=0.648 | CT=0.648\n",
      "Epoch:   6 | loss: 4.96916 || A: HR@10=0.1332 | rel=1.824 | CT=0.454 || B: HR@10=0.0530 | rel=0.458 | CT=0.458\n",
      "Epoch:   7 | loss: 4.74195 || A: HR@10=0.1742 | rel=2.386 | CT=1.017 || B: HR@10=0.0409 | rel=0.353 | CT=0.353\n",
      "Epoch:   8 | loss: 4.55093 || A: HR@10=0.1770 | rel=2.425 | CT=1.055 || B: HR@10=0.0397 | rel=0.343 | CT=0.343\n",
      "Epoch:   9 | loss: 4.35958 || A: HR@10=0.1774 | rel=2.429 | CT=1.060 || B: HR@10=0.0447 | rel=0.386 | CT=0.386\n",
      "Epoch:  10 | loss: 4.16319 || A: HR@10=0.1885 | rel=2.581 | CT=1.212 || B: HR@10=0.0429 | rel=0.371 | CT=0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.07615 || A: HR@10=0.0667 | rel=0.914 | CT=0.914 || B: HR@10=0.2681 | rel=2.317 | CT=2.317\n",
      "Epoch:   2 | loss: 6.23629 || A: HR@10=0.0765 | rel=1.048 | CT=1.048 || B: HR@10=0.3246 | rel=2.805 | CT=2.805\n",
      "Epoch:   3 | loss: 5.83379 || A: HR@10=0.0876 | rel=1.200 | CT=1.200 || B: HR@10=0.4000 | rel=3.456 | CT=3.456\n",
      "Epoch:   4 | loss: 5.49756 || A: HR@10=0.1120 | rel=1.533 | CT=1.533 || B: HR@10=0.3372 | rel=2.914 | CT=2.914\n",
      "Epoch:   5 | loss: 5.21895 || A: HR@10=0.0747 | rel=1.023 | CT=1.023 || B: HR@10=0.3009 | rel=2.600 | CT=2.600\n",
      "Epoch:   6 | loss: 4.97492 || A: HR@10=0.0566 | rel=0.776 | CT=0.776 || B: HR@10=0.2410 | rel=2.082 | CT=2.082\n",
      "Epoch:   7 | loss: 4.74894 || A: HR@10=0.1017 | rel=1.393 | CT=1.393 || B: HR@10=0.2014 | rel=1.740 | CT=1.740\n",
      "Epoch:   8 | loss: 4.53603 || A: HR@10=0.1161 | rel=1.590 | CT=1.590 || B: HR@10=0.2138 | rel=1.848 | CT=1.848\n",
      "Epoch:   9 | loss: 4.34453 || A: HR@10=0.0899 | rel=1.232 | CT=1.232 || B: HR@10=0.1997 | rel=1.726 | CT=1.726\n",
      "Epoch:  10 | loss: 4.16651 || A: HR@10=0.1067 | rel=1.461 | CT=1.461 || B: HR@10=0.1696 | rel=1.465 | CT=1.465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.07858 || A: HR@10=0.0711 | rel=0.973 | CT=-0.397 || B: HR@10=0.2738 | rel=2.366 | CT=1.501\n",
      "Epoch:   2 | loss: 6.24272 || A: HR@10=0.0525 | rel=0.719 | CT=-0.651 || B: HR@10=0.2254 | rel=1.948 | CT=1.084\n",
      "Epoch:   3 | loss: 5.82747 || A: HR@10=0.0752 | rel=1.030 | CT=-0.340 || B: HR@10=0.1896 | rel=1.639 | CT=0.775\n",
      "Epoch:   4 | loss: 5.50348 || A: HR@10=0.1086 | rel=1.488 | CT=0.118 || B: HR@10=0.2047 | rel=1.769 | CT=0.905\n",
      "Epoch:   5 | loss: 5.22602 || A: HR@10=0.1105 | rel=1.513 | CT=0.143 || B: HR@10=0.1708 | rel=1.475 | CT=0.611\n",
      "Epoch:   6 | loss: 4.97460 || A: HR@10=0.1072 | rel=1.468 | CT=0.098 || B: HR@10=0.1308 | rel=1.131 | CT=0.266\n",
      "Epoch:   7 | loss: 4.75038 || A: HR@10=0.1143 | rel=1.565 | CT=0.196 || B: HR@10=0.1153 | rel=0.996 | CT=0.132\n",
      "Epoch:   8 | loss: 4.55779 || A: HR@10=0.1192 | rel=1.633 | CT=0.264 || B: HR@10=0.1123 | rel=0.970 | CT=0.106\n",
      "Epoch:   9 | loss: 4.35306 || A: HR@10=0.1212 | rel=1.660 | CT=0.291 || B: HR@10=0.1186 | rel=1.025 | CT=0.161\n",
      "Epoch:  10 | loss: 4.18207 || A: HR@10=0.1077 | rel=1.474 | CT=0.105 || B: HR@10=0.1075 | rel=0.929 | CT=0.065\n",
      "[item_set] A: raw=10 enc=10  |  B: raw=10 enc=10\n",
      "0.073841059602649\n",
      "0.11324503311258279\n",
      "Mean frequency — A: 814.4  | B: 480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.07936 || A: HR@10=0.1226 | rel=1.660\n",
      "Epoch:   2 | loss: 6.23448 || A: HR@10=0.1477 | rel=2.001\n",
      "Epoch:   3 | loss: 5.82500 || A: HR@10=0.1727 | rel=2.339\n",
      "Epoch:   4 | loss: 5.50035 || A: HR@10=0.1666 | rel=2.256\n",
      "Epoch:   5 | loss: 5.22327 || A: HR@10=0.1534 | rel=2.077\n",
      "Epoch:   6 | loss: 4.97832 || A: HR@10=0.1300 | rel=1.761\n",
      "Epoch:   7 | loss: 4.75117 || A: HR@10=0.1717 | rel=2.326\n",
      "Epoch:   8 | loss: 4.54918 || A: HR@10=0.1363 | rel=1.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.06490 || A: HR@10=0.0396 | rel=0.536\n",
      "Epoch:   2 | loss: 6.23764 || A: HR@10=0.0363 | rel=0.491\n",
      "Epoch:   3 | loss: 5.81719 || A: HR@10=0.0293 | rel=0.397\n",
      "Epoch:   4 | loss: 5.50045 || A: HR@10=0.0341 | rel=0.462\n",
      "Epoch:   5 | loss: 5.22740 || A: HR@10=0.0187 | rel=0.253\n",
      "Epoch:   6 | loss: 4.97881 || A: HR@10=0.0126 | rel=0.170\n",
      "Epoch:   7 | loss: 4.75209 || A: HR@10=0.0089 | rel=0.121\n",
      "Epoch:   8 | loss: 4.53954 || A: HR@10=0.0137 | rel=0.186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.12003 || B: HR@10=0.2860 | rel=2.526\n",
      "Epoch:   2 | loss: 6.26162 || B: HR@10=0.3683 | rel=3.253\n",
      "Epoch:   3 | loss: 5.85288 || B: HR@10=0.3400 | rel=3.002\n",
      "Epoch:   4 | loss: 5.52296 || B: HR@10=0.2978 | rel=2.630\n",
      "Epoch:   5 | loss: 5.25350 || B: HR@10=0.1918 | rel=1.694\n",
      "Epoch:   6 | loss: 4.98723 || B: HR@10=0.2570 | rel=2.270\n",
      "Epoch:   7 | loss: 4.76394 || B: HR@10=0.2471 | rel=2.182\n",
      "Epoch:   8 | loss: 4.55550 || B: HR@10=0.1792 | rel=1.582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.10243 || B: HR@10=0.2999 | rel=2.649\n",
      "Epoch:   2 | loss: 6.25557 || B: HR@10=0.3059 | rel=2.701\n",
      "Epoch:   3 | loss: 5.83883 || B: HR@10=0.3485 | rel=3.077\n",
      "Epoch:   4 | loss: 5.51514 || B: HR@10=0.3177 | rel=2.805\n",
      "Epoch:   5 | loss: 5.22068 || B: HR@10=0.3427 | rel=3.026\n",
      "Epoch:   6 | loss: 4.97989 || B: HR@10=0.3900 | rel=3.444\n",
      "Epoch:   7 | loss: 4.76122 || B: HR@10=0.3367 | rel=2.973\n",
      "Epoch:   8 | loss: 4.54099 || B: HR@10=0.3400 | rel=3.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.07969 || A: HR@10=0.0676 | rel=0.915 | CT=-0.439 || B: HR@10=0.1910 | rel=1.686 | CT=1.686\n",
      "Epoch:   2 | loss: 6.23687 || A: HR@10=0.0777 | rel=1.052 | CT=-0.302 || B: HR@10=0.1805 | rel=1.594 | CT=1.594\n",
      "Epoch:   3 | loss: 5.82567 || A: HR@10=0.1053 | rel=1.426 | CT=0.072 || B: HR@10=0.2034 | rel=1.796 | CT=1.796\n",
      "Epoch:   4 | loss: 5.50151 || A: HR@10=0.0972 | rel=1.317 | CT=-0.038 || B: HR@10=0.1333 | rel=1.177 | CT=1.177\n",
      "Epoch:   5 | loss: 5.22486 || A: HR@10=0.1597 | rel=2.162 | CT=0.808 || B: HR@10=0.0863 | rel=0.762 | CT=0.762\n",
      "Epoch:   6 | loss: 4.97096 || A: HR@10=0.1105 | rel=1.496 | CT=0.142 || B: HR@10=0.0927 | rel=0.819 | CT=0.819\n",
      "Epoch:   7 | loss: 4.74125 || A: HR@10=0.1115 | rel=1.509 | CT=0.155 || B: HR@10=0.0773 | rel=0.683 | CT=0.683\n",
      "Epoch:   8 | loss: 4.54191 || A: HR@10=0.1646 | rel=2.229 | CT=0.875 || B: HR@10=0.0654 | rel=0.578 | CT=0.578\n",
      "Epoch:   9 | loss: 4.34730 || A: HR@10=0.1245 | rel=1.687 | CT=0.332 || B: HR@10=0.0686 | rel=0.605 | CT=0.605\n",
      "Epoch:  10 | loss: 4.15935 || A: HR@10=0.1280 | rel=1.734 | CT=0.379 || B: HR@10=0.1063 | rel=0.939 | CT=0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.10095 || A: HR@10=0.0686 | rel=0.929 | CT=-0.426 || B: HR@10=0.3258 | rel=2.877 | CT=-0.655\n",
      "Epoch:   2 | loss: 6.26478 || A: HR@10=0.0391 | rel=0.529 | CT=-0.825 || B: HR@10=0.4712 | rel=4.161 | CT=0.629\n",
      "Epoch:   3 | loss: 5.84544 || A: HR@10=0.0402 | rel=0.545 | CT=-0.809 || B: HR@10=0.4020 | rel=3.549 | CT=0.017\n",
      "Epoch:   4 | loss: 5.51226 || A: HR@10=0.0581 | rel=0.787 | CT=-0.567 || B: HR@10=0.3642 | rel=3.216 | CT=-0.316\n",
      "Epoch:   5 | loss: 5.24319 || A: HR@10=0.0522 | rel=0.707 | CT=-0.648 || B: HR@10=0.3761 | rel=3.321 | CT=-0.211\n",
      "Epoch:   6 | loss: 4.97361 || A: HR@10=0.0411 | rel=0.556 | CT=-0.798 || B: HR@10=0.3018 | rel=2.665 | CT=-0.868\n",
      "Epoch:   7 | loss: 4.75082 || A: HR@10=0.0368 | rel=0.498 | CT=-0.856 || B: HR@10=0.2542 | rel=2.245 | CT=-1.287\n",
      "Epoch:   8 | loss: 4.54622 || A: HR@10=0.0315 | rel=0.426 | CT=-0.928 || B: HR@10=0.2107 | rel=1.860 | CT=-1.672\n",
      "Epoch:   9 | loss: 4.34523 || A: HR@10=0.0409 | rel=0.554 | CT=-0.800 || B: HR@10=0.2743 | rel=2.422 | CT=-1.110\n",
      "Epoch:  10 | loss: 4.16209 || A: HR@10=0.0336 | rel=0.455 | CT=-0.899 || B: HR@10=0.2589 | rel=2.286 | CT=-1.246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.11051 || A: HR@10=0.0919 | rel=1.245 | CT=-0.109 || B: HR@10=0.3014 | rel=2.662 | CT=-0.870\n",
      "Epoch:   2 | loss: 6.25316 || A: HR@10=0.1067 | rel=1.444 | CT=0.090 || B: HR@10=0.2579 | rel=2.277 | CT=-1.255\n",
      "Epoch:   3 | loss: 5.83865 || A: HR@10=0.0975 | rel=1.321 | CT=-0.033 || B: HR@10=0.2393 | rel=2.113 | CT=-1.419\n",
      "Epoch:   4 | loss: 5.51415 || A: HR@10=0.1100 | rel=1.489 | CT=0.135 || B: HR@10=0.1267 | rel=1.119 | CT=-2.413\n",
      "Epoch:   5 | loss: 5.22832 || A: HR@10=0.1293 | rel=1.752 | CT=0.397 || B: HR@10=0.1385 | rel=1.223 | CT=-2.310\n",
      "Epoch:   6 | loss: 4.98012 || A: HR@10=0.1168 | rel=1.581 | CT=0.227 || B: HR@10=0.1105 | rel=0.975 | CT=-2.557\n",
      "Epoch:   7 | loss: 4.75148 || A: HR@10=0.1204 | rel=1.631 | CT=0.276 || B: HR@10=0.0942 | rel=0.832 | CT=-2.700\n",
      "Epoch:   8 | loss: 4.53908 || A: HR@10=0.1293 | rel=1.752 | CT=0.397 || B: HR@10=0.1355 | rel=1.196 | CT=-2.336\n",
      "Epoch:   9 | loss: 4.33815 || A: HR@10=0.1151 | rel=1.559 | CT=0.205 || B: HR@10=0.0679 | rel=0.600 | CT=-2.933\n",
      "Epoch:  10 | loss: 4.16629 || A: HR@10=0.1244 | rel=1.684 | CT=0.330 || B: HR@10=0.0810 | rel=0.715 | CT=-2.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.06539 || A: HR@10=0.0502 | rel=0.680 | CT=-0.675 || B: HR@10=0.2983 | rel=2.634 | CT=2.634\n",
      "Epoch:   2 | loss: 6.23260 || A: HR@10=0.0450 | rel=0.610 | CT=-0.744 || B: HR@10=0.4056 | rel=3.582 | CT=3.582\n",
      "Epoch:   3 | loss: 5.81739 || A: HR@10=0.0487 | rel=0.659 | CT=-0.695 || B: HR@10=0.4232 | rel=3.737 | CT=3.737\n",
      "Epoch:   4 | loss: 5.49566 || A: HR@10=0.0417 | rel=0.565 | CT=-0.789 || B: HR@10=0.3649 | rel=3.222 | CT=3.222\n",
      "Epoch:   5 | loss: 5.22289 || A: HR@10=0.0586 | rel=0.794 | CT=-0.560 || B: HR@10=0.3524 | rel=3.112 | CT=3.112\n",
      "Epoch:   6 | loss: 4.96161 || A: HR@10=0.0684 | rel=0.926 | CT=-0.428 || B: HR@10=0.4309 | rel=3.805 | CT=3.805\n",
      "Epoch:   7 | loss: 4.73900 || A: HR@10=0.0614 | rel=0.832 | CT=-0.522 || B: HR@10=0.5185 | rel=4.579 | CT=4.579\n",
      "Epoch:   8 | loss: 4.52151 || A: HR@10=0.0411 | rel=0.556 | CT=-0.798 || B: HR@10=0.4730 | rel=4.177 | CT=4.177\n",
      "Epoch:   9 | loss: 4.33242 || A: HR@10=0.0422 | rel=0.572 | CT=-0.782 || B: HR@10=0.4500 | rel=3.974 | CT=3.974\n",
      "Epoch:  10 | loss: 4.15957 || A: HR@10=0.0354 | rel=0.480 | CT=-0.874 || B: HR@10=0.3410 | rel=3.011 | CT=3.011\n",
      "[item_set] A: raw=10 enc=10  |  B: raw=10 enc=10\n",
      "0.07698675496688742\n",
      "0.11225165562913907\n",
      "Mean frequency — A: 814.4  | B: 480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_12556\\1332714878.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.12041 || A: HR@10=0.0792 | rel=1.028\n",
      "Epoch:   2 | loss: 6.26812 || A: HR@10=0.1098 | rel=1.426\n",
      "Epoch:   3 | loss: 5.85965 || A: HR@10=0.0898 | rel=1.166\n",
      "Epoch:   4 | loss: 5.52721 || A: HR@10=0.0866 | rel=1.125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m df_B_demo \u001b[38;5;241m=\u001b[39m edit_ratings(df0, C2, raw_B, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemote\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_decoder\u001b[38;5;241m=\u001b[39mmds\u001b[38;5;241m.\u001b[39muser_decoder)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Solo A (promote/demote)\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m g1_prom \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_A_prom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg1_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m g1_demo \u001b[38;5;241m=\u001b[39m run_single(df_A_demo, C1, enc_A, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, g1_base)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Solo B\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 68\u001b[0m, in \u001b[0;36mrun_single\u001b[1;34m(edited_df, members, item_set_enc, name, baseline_hr)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(edited_df, members, item_set_enc, name, baseline_hr):\n\u001b[1;32m---> 68\u001b[0m     mds_s, tr_s, va_s, te_s, model_s \u001b[38;5;241m=\u001b[39m \u001b[43mretrain_from_baseline_on_ratings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_state_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mratings_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medited_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_valid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_set_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmembers_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem_set_enc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_set_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_hr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPK\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     80\u001b[0m         subset_members \u001b[38;5;241m=\u001b[39m {u: tr_s[u] \u001b[38;5;28;01mfor\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m members \u001b[38;5;28;01mif\u001b[39;00m u \u001b[38;5;129;01min\u001b[39;00m tr_s}\n",
      "Cell \u001b[1;32mIn[11], line 222\u001b[0m, in \u001b[0;36mretrain_from_baseline_on_ratings\u001b[1;34m(baseline_state_path, ratings_df, mds, epochs, lr, eval_specs, k, device, **kwargs)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hr, rel, ct\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 222\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_train_one_epoch_nan_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_specs:\n\u001b[0;32m    224\u001b[0m         parts \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[11], line 173\u001b[0m, in \u001b[0;36mretrain_from_baseline_on_ratings.<locals>._train_one_epoch_nan_safe\u001b[1;34m(model, crit, opt, dl)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss):\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), grad_clip)\n\u001b[0;32m    175\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Collective experiment params (from paper notebook) =====\n",
    "N_values = [10, 20, 50]                     # collective sizes\n",
    "p_values = [0.1, 0.25, 0.5, 0.75, 1.0]      # homogeneity\n",
    "trials_per_case = 10\n",
    "TOPK = 10\n",
    "V_TARGET = config.get('V', 10)\n",
    "NUM_CLUSTERS = 10\n",
    "\n",
    "scenarios = [\n",
    "    ('promote', 'promote', 'both_promote'),\n",
    "    ('demote',  'demote',  'both_demote'),\n",
    "    ('promote', 'demote',  'A_promote_B_demote'),\n",
    "    ('demote',  'promote', 'A_demote_B_promote'),\n",
    "]\n",
    "\n",
    "# --- Run baseline (or load already-trained) ---\n",
    "mds, user_train, user_valid, user_test, baseline_model = prepare_baseline(config)\n",
    "NUM_ITEMS = resolve_num_items(mds, baseline_model)\n",
    "\n",
    "U, user_index = compute_user_embeddings(baseline_model, user_train, NUM_ITEMS, config['max_len'])\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42).fit(U)\n",
    "labels = kmeans.labels_\n",
    "# Precompute centroids for the NUM_CLUSTERS user clusters\n",
    "centroids = _cluster_centroids(U, labels, NUM_CLUSTERS)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for N in N_values:\n",
    "    for p in p_values:\n",
    "        for trial in range(trials_per_case):\n",
    "            # ---- Build two collectives A and B ----\n",
    "            if  SEED_MODE == \"uniform\":\n",
    "                seedA = np.random.randint(NUM_CLUSTERS)\n",
    "                seedB = (seedA + np.random.randint(1, NUM_CLUSTERS)) % NUM_CLUSTERS\n",
    "            elif SEED_MODE == \"maxdist\":\n",
    "                # choose the two most distant clusters in user-embedding space (cosine)\n",
    "                i, j, d = _farthest_pair_indices(centroids, metric=\"cosine\")\n",
    "                seedA, seedB = i, j\n",
    "                # (optional) log the distance for debugging\n",
    "                # print(f\"[seeds] maxdist seeds: A={seedA}, B={seedB}, cosine-dist={d:.4f}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown SEED_MODE={SEED_MODE}\")\n",
    "\n",
    "            C1 = build_collective_quota(seedA, labels, user_index, size=N, p=p,seed=42)\n",
    "            C2 = build_collective_quota(seedB, labels, user_index, size=N, p=p, seed=42)\n",
    "\n",
    "            # ---- Build fixed item sets from ORIGINAL ratings (via mds.df) ----\n",
    "            raw_A, enc_A = build_item_set_for_members(mds, C1, V=V_TARGET)\n",
    "            raw_B, enc_B = build_item_set_for_members(mds, C2, V=V_TARGET)\n",
    "            print(f\"[item_set] A: raw={len(raw_A)} enc={len(enc_A)}  |  B: raw={len(raw_B)} enc={len(enc_B)}\")\n",
    "\n",
    "            # ---- Evaluate BASELINE HR@K for each collective on its own item_set ----\n",
    "            subset_ALL = user_test  # evaluate over ALL test users per paper\n",
    "            g1_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_A, NUM_ITEMS, config['max_len'], TOPK))\n",
    "            g2_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_B, NUM_ITEMS, config['max_len'], TOPK))\n",
    "            print(g1_base)\n",
    "            print(g2_base)\n",
    "            from collections import Counter\n",
    "            cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "            popA = sum(cnt[i] for i in enc_A)/len(enc_A)\n",
    "            popB = sum(cnt[i] for i in enc_B)/len(enc_B)\n",
    "            print(\"Mean frequency — A:\", popA, \" | B:\", popB)\n",
    "            # ---- Single-collective interventions ----\n",
    "            # Edit ratings for A or B on their own raw item sets, then retrain & eval with SAME encoded sets\n",
    "            # Helper to retrain and evaluate a single collective\n",
    "            def run_single(edited_df, members, item_set_enc, name, baseline_hr):\n",
    "                mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "                    baseline_state_path=\"baseline_best.pt\",\n",
    "                    ratings_df=edited_df,\n",
    "                    mds=mds,\n",
    "                    config=config,\n",
    "                    epochs=8,\n",
    "                    eval_split=\"user_valid\",\n",
    "                    item_set=item_set_enc,\n",
    "                    eval_specs=[{\"name\": name, \"members_idx\": user_test, \"item_set_enc\": item_set_enc, \"baseline_hr\": baseline_hr}],\n",
    "                    k=TOPK\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    subset_members = {u: tr_s[u] for u in members if u in tr_s}\n",
    "                    hr = float(hr_for_itemset(model_s, subset_members, item_set_enc, NUM_ITEMS, config['max_len'], TOPK))\n",
    "                return hr\n",
    "\n",
    "            # Prepare edited dataframes\n",
    "            df0 = mds.df.copy()\n",
    "            df_A_prom = edit_ratings(df0, C1, raw_A, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "            df_A_demo = edit_ratings(df0, C1, raw_A, action=\"demote\", user_decoder=mds.user_decoder)\n",
    "            df_B_prom = edit_ratings(df0, C2, raw_B, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "            df_B_demo = edit_ratings(df0, C2, raw_B, action=\"demote\", user_decoder=mds.user_decoder)\n",
    "\n",
    "            # Solo A (promote/demote)\n",
    "            g1_prom = run_single(df_A_prom, C1, enc_A, 'A', g1_base)\n",
    "            g1_demo = run_single(df_A_demo, C1, enc_A, 'A', g1_base)\n",
    "            # Solo B\n",
    "            g2_prom = run_single(df_B_prom, C2, enc_B, 'B', g2_base)\n",
    "            g2_demo = run_single(df_B_demo, C2, enc_B, 'B', g2_base)\n",
    "\n",
    "            # ---- Two-collective interventions (apply both edits then retrain once) ----\n",
    "            def run_joint(df_A_action, df_B_action, solo_A, solo_B):\n",
    "                df_joint = edit_ratings(mds.df.copy(), C1, raw_A, action=df_A_action, user_decoder=mds.user_decoder)\n",
    "                df_joint = edit_ratings(df_joint,        C2, raw_B, action=df_B_action, user_decoder=mds.user_decoder)\n",
    "                mds_j, tr_j, va_j, te_j, model_j = retrain_from_baseline_on_ratings(\n",
    "                    baseline_state_path=\"baseline_best.pt\",\n",
    "                    ratings_df=df_joint,\n",
    "                    mds=mds,\n",
    "                    config=config,\n",
    "                    epochs=10,\n",
    "                    eval_split=\"user_valid\",\n",
    "                    eval_specs=[\n",
    "                        {\"name\": \"A\", \"members_idx\": user_test, \"item_set_enc\": enc_A, \"baseline_hr\": g1_base, \"solo_hr\": solo_A},\n",
    "                        {\"name\": \"B\", \"members_idx\": user_test, \"item_set_enc\": enc_B, \"baseline_hr\": g2_base, \"solo_hr\": solo_B}\n",
    "                    ],\n",
    "                    k=TOPK,\n",
    "                    # Note: no single item_set here; we will evaluate each collective separately\n",
    "                )\n",
    "                subset_C1j = {u: tr_j[u] for u in C1 if u in tr_j}\n",
    "                hrA = float(hr_for_itemset(model_j, subset_C1j, enc_A, NUM_ITEMS, config['max_len'], TOPK))\n",
    "                subset_C2j = {u: tr_j[u] for u in C2 if u in tr_j}\n",
    "                hrB = float(hr_for_itemset(model_j, subset_C2j, enc_B, NUM_ITEMS, config['max_len'], TOPK))\n",
    "                return hrA, hrB\n",
    "\n",
    "            # Both promote / both demote / criss-cross\n",
    "            gA_pp, gB_pp = run_joint(\"promote\", \"promote\", g1_prom, g2_prom)\n",
    "            gA_dd, gB_dd = run_joint(\"demote\", \"demote\", g1_demo, g2_demo)\n",
    "            gA_pd, gB_pd = run_joint(\"promote\", \"demote\", g1_prom, g2_demo)\n",
    "            gA_dp, gB_dp = run_joint(\"demote\", \"promote\", g1_demo, g2_prom)\n",
    "\n",
    "            # Collect metrics (relative HR vs baseline per collective)\n",
    "            def rel(now, base): \n",
    "                return (now / base) if base > 0 else float('nan')\n",
    "\n",
    "            results.append({\n",
    "                \"N\": N, \"p\": p, \"trial\": trial,\n",
    "                \"gA_base\": g1_base, \"gB_base\": g2_base,\n",
    "                \"gA_prom\": g1_prom, \"gA_demo\": g1_demo,\n",
    "                \"gB_prom\": g2_prom, \"gB_demo\": g2_demo,\n",
    "                \"gA_pp\": gA_pp, \"gB_pp\": gB_pp,\n",
    "                \"gA_dd\": gA_dd, \"gB_dd\": gB_dd,\n",
    "                \"gA_pd\": gA_pd, \"gB_pd\": gB_pd,\n",
    "                \"gA_dp\": gA_dp, \"gB_dp\": gB_dp,\n",
    "                \"rA_prom\": rel(g1_prom, g1_base),\n",
    "                \"rA_demo\": rel(g1_demo, g1_base),\n",
    "                \"rB_prom\": rel(g2_prom, g2_base),\n",
    "                \"rB_demo\": rel(g2_demo, g2_base),\n",
    "                \"rA_pp\": rel(gA_pp, g1_base),\n",
    "                \"rB_pp\": rel(gB_pp, g2_base),\n",
    "                \"rA_dd\": rel(gA_dd, g1_base),\n",
    "                \"rB_dd\": rel(gB_dd, g2_base),\n",
    "                \"rA_pd\": rel(gA_pd, g1_base),\n",
    "                \"rB_pd\": rel(gB_pd, g2_base),\n",
    "                \"rA_dp\": rel(gA_dp, g1_base),\n",
    "                \"rB_dp\": rel(gB_dp, g2_base),\n",
    "                \"size_A\": len(C1), \"size_B\": len(C2)\n",
    "            })\n",
    "\n",
    "            # free between trials\n",
    "            gc.collect()\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Save & preview\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"rq2_results_relative_hr.csv\", index=False)\n",
    "display(res_df.head())\n",
    "print(\"Saved rq2_results_relative_hr.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88630137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick plot mirroring the paper notebook ---\n",
    "summary = (res_df.groupby(['scenario','N_size','p']).agg({'relHR_A':'mean','relHR_B':'mean'}).reset_index()\n",
    "                    .sort_values(['scenario','N_size','p']))\n",
    "plt.figure(figsize=(10,6))\n",
    "for scen in summary[\"scenario\"].unique():\n",
    "    sub = summary[summary[\"scenario\"]==scen]\n",
    "    xs = range(len(sub))\n",
    "    plt.plot(xs, sub[\"relHR_A\"], marker=\"o\", label=f\"{scen} (A)\")\n",
    "    plt.plot(xs, sub[\"relHR_B\"], marker=\"x\", label=f\"{scen} (B)\")\n",
    "plt.title(f\"Relative HR@{TOPK} by scenario (mean over trials)\")\n",
    "plt.xlabel(\"Condition index (ordered by scenario/N/p)\")\n",
    "plt.ylabel(\"Mean Relative HR\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
