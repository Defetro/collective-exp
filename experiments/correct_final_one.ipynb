{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e137fec2",
   "metadata": {},
   "source": [
    "### Notebook hotfix (applied 2025-08-20T11:37:08 UTC)\n",
    "\n",
    "- Fixed CUDA device-side assert during evaluation by sanitizing token IDs in `score_next_items`.\n",
    "- Masked PAD/MASK predictions and clipped logits to `num_items`.\n",
    "- Light hyperparameter tweaks intended to keep training ≤10% slower while improving Recall@10 / NDCG@10:\n",
    "  - `mask_prob=0.20`\n",
    "  - `weight_decay=0.01`, `lr_scheduler='cosine'`, `warmup_ratio=0.10`\n",
    "  - `dropout` +0.05 (cap 0.35)\n",
    "  - `max_len` increased by ~10%\n",
    "  - learning rate +15% with warmup/decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980720fa",
   "metadata": {},
   "source": [
    "# ML-20M — BERT4Rec Baseline + Collective Experiments (RQ2-style)\n",
    "**Built:** 2025-08-13 14:54\n",
    "\n",
    "This notebook **keeps the baseline model and training loop from `ml-20bert4rec`** and adds **all collective experimental conditions** inspired by `bert4rec_collectives_rq2_paper`:\n",
    "- Embedding-based user clustering (KMeans), farthest-cluster seeding\n",
    "- Collective construction with homogeneity **p**\n",
    "- Promote/Demote scenarios for two collectives (A/B)\n",
    "- Deterministic rating edits and **retraining from the baseline weights**\n",
    "- Relative HR@K on targeted item sets (A and B)\n",
    "- Results export + quick plot\n",
    "\n",
    "Printing/log messages mirror the paper notebook (e.g., *Device:*, *Running RQ2 grid...*, *Seed clusters: ...*, *Saved rq2_results_relative_hr.csv*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4796458",
   "metadata": {},
   "source": [
    "\n",
    "> **Fix applied (Aug 20, 2025):** Validation Recall@K/NDCG@K previously evaluated on single-item sequences,\n",
    "> which produced zeros. `prepare_baseline` now evaluates on `user_valid_full[u] = user_train[u] + user_valid[u]`,\n",
    "> ensuring there is prefix context (cond) and a target. The evaluator `recall_ndcg_at_k` expects sequences with\n",
    "> length ≥ 2 and uses `cond = seq[:-1]`, `target = seq[-1]`. No change to the dataset split itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3cdd2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def edit_ratings(df, users_idx, target_items_raw, action, user_decoder,\n",
    "                 promote_value=5.0, demote_value=1.0,\n",
    "                 add_competing=False, competing_item_raw=None):\n",
    "    \"\"\"\n",
    "    Campaign edits for BERT4Rec applied to TRAIN ONLY.\n",
    "\n",
    "    - promote: append each target item just before the user's valid/test boundary (ts_valid),\n",
    "               but strictly after the user's last train timestamp (max_train_ts)\n",
    "    - demote:  remove target items in the TRAIN region (ts < ts_valid), never touch the last two events\n",
    "               optionally append a competing interaction just before ts_valid\n",
    "\n",
    "    Returns a new DataFrame with the same schema; last-two events per user are preserved.\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Normalize inputs\n",
    "    target_items_raw = list({int(x) for x in (target_items_raw or [])})\n",
    "    if not target_items_raw:\n",
    "        return df\n",
    "\n",
    "    user_raw_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_raw_ids:\n",
    "        return df\n",
    "\n",
    "    target_set = set(target_items_raw)\n",
    "    user_set   = set(user_raw_ids)\n",
    "\n",
    "    # Stable sort to avoid row reordering among equal timestamps\n",
    "    df = df.sort_values(['userId', 'timestamp'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "    for uid, g in df.groupby('userId', sort=False):\n",
    "        if uid not in user_set:\n",
    "            out_parts.append(g)\n",
    "            continue\n",
    "\n",
    "        g = g.sort_values('timestamp', kind='mergesort').copy()\n",
    "        ts_vals = g['timestamp'].to_numpy()\n",
    "\n",
    "        # Define boundary using the ORIGINAL sequence per-user\n",
    "        if len(ts_vals) >= 2:\n",
    "            ts_valid = int(ts_vals[-2])  # anything < ts_valid is TRAIN; >= ts_valid is VALID/TEST tail\n",
    "            train_mask = ts_vals < ts_valid\n",
    "        else:\n",
    "            ts_valid = int(ts_vals[-1]) if len(ts_vals) else np.iinfo(np.int64).max\n",
    "            train_mask = np.ones(len(g), dtype=bool)\n",
    "\n",
    "        if action == 'promote':\n",
    "            kept = g.copy()\n",
    "\n",
    "            # choose timestamps for k new interactions within (max_train_ts, ts_valid)\n",
    "            k = len(target_items_raw)\n",
    "            if train_mask.any():\n",
    "                max_train_ts = int(g.loc[train_mask, 'timestamp'].max())\n",
    "            else:\n",
    "                # if no train events, place a window just below ts_valid\n",
    "                max_train_ts = ts_valid - (k + 10)\n",
    "\n",
    "            # initial window\n",
    "            start = max(max_train_ts + 1, ts_valid - k)   # try to end right under boundary\n",
    "            new_ts = np.arange(start, start + k, dtype=np.int64)\n",
    "\n",
    "            # ensure strictly below ts_valid\n",
    "            overflow = max(0, int(new_ts[-1] - (ts_valid - 1)))\n",
    "            if overflow > 0:\n",
    "                new_ts = new_ts - overflow\n",
    "\n",
    "            # ensure strictly above max_train_ts\n",
    "            if new_ts[0] <= max_train_ts:\n",
    "                shift = (max_train_ts - new_ts[0]) + 1\n",
    "                new_ts = new_ts + shift\n",
    "                # clip again below boundary if needed\n",
    "                overflow = max(0, int(new_ts[-1] - (ts_valid - 1)))\n",
    "                if overflow > 0:\n",
    "                    new_ts = new_ts - overflow\n",
    "                    # final guard: if we collapsed (degenerate window), step them backwards\n",
    "                    if np.any(new_ts <= max_train_ts):\n",
    "                        gap = (max_train_ts - new_ts[0]) + 1\n",
    "                        new_ts = new_ts - (gap + 1)\n",
    "\n",
    "            # build new rows\n",
    "            new_rows = pd.DataFrame({\n",
    "                'userId':   [uid] * k,\n",
    "                'movieId':  target_items_raw,\n",
    "                'rating':   [promote_value] * k,\n",
    "                'timestamp': new_ts\n",
    "            })\n",
    "\n",
    "            kept = pd.concat([kept, new_rows], ignore_index=True)\n",
    "            kept = kept.sort_values('timestamp', kind='mergesort')\n",
    "\n",
    "            out_parts.append(kept)\n",
    "\n",
    "        elif action == 'demote':\n",
    "            kept = g.copy()\n",
    "\n",
    "            # drop target items only in TRAIN\n",
    "            in_targets = kept['movieId'].isin(target_set)\n",
    "            drop_mask  = in_targets & train_mask\n",
    "            kept = kept.loc[~drop_mask].copy()\n",
    "\n",
    "            # optionally add one competing interaction right before ts_valid (but after max_train_ts)\n",
    "            if add_competing:\n",
    "                if train_mask.any():\n",
    "                    max_train_ts = int(g.loc[train_mask, 'timestamp'].max())\n",
    "                else:\n",
    "                    max_train_ts = ts_valid - 10\n",
    "\n",
    "                # choose competing item if not provided (global most frequent non-target)\n",
    "                if competing_item_raw is not None:\n",
    "                    cid = int(competing_item_raw)\n",
    "                else:\n",
    "                    global_counts = df.loc[~df['movieId'].isin(target_set), 'movieId'].value_counts()\n",
    "                    cid = int(global_counts.index[0]) if len(global_counts) else None\n",
    "\n",
    "                if cid is not None:\n",
    "                    comp_ts = min(ts_valid - 1, max_train_ts + 1)\n",
    "                    comp_row = pd.DataFrame({\n",
    "                        'userId':   [uid],\n",
    "                        'movieId':  [cid],\n",
    "                        'rating':   [promote_value],\n",
    "                        'timestamp':[comp_ts]\n",
    "                    })\n",
    "                    kept = pd.concat([kept, comp_row], ignore_index=True)\n",
    "\n",
    "            kept = kept.sort_values('timestamp', kind='mergesort')\n",
    "            out_parts.append(kept)\n",
    "\n",
    "        else:\n",
    "            # Unknown action: passthrough\n",
    "            out_parts.append(g)\n",
    "\n",
    "    out = pd.concat(out_parts, ignore_index=True)\n",
    "    # Normalize dtypes\n",
    "    return out.astype({'userId': 'int64', 'movieId': 'int64'})\n",
    "\n",
    "\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=3.5):\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    df = df.copy()\n",
    "    # Keep positives only\n",
    "    if threshold is not None:\n",
    "        df = df[df['rating'] >= float(threshold)].copy()\n",
    "    # Stable sort and keep last rating per (user,item)\n",
    "    df = df.sort_values(['userId','movieId','timestamp'], kind='mergesort').reset_index(drop=True)\n",
    "    df = df.groupby(['userId','movieId'], as_index=False).tail(1)\n",
    "    # Map to indices\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    # Build sequences\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.sort_values(['user_idx','timestamp'], kind='mergesort').iterrows():\n",
    "        user_pos[row['user_idx']].append(int(row['item_idx']))\n",
    "    # Enforce >=5 positives per user\n",
    "    user_pos = {u: seq for u, seq in user_pos.items() if len(seq) >= 5}\n",
    "    return user_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "83037f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", DEVICE)\n",
    "SEED_MODE = \"balanced_maxdist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02c08804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# --- Baseline config (from ml-20bert4rec) ---\n",
    "config = {\n",
    "    'data_path' : r'C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m',  # ML-20M\n",
    "    'max_len' : 50,\n",
    "    'hidden_units' : 256,\n",
    "    'num_heads' : 2,\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate' : 0.1,\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 128,\n",
    "    'num_epochs' : 17,\n",
    "    'num_workers' : 2,\n",
    "    'mask_prob' : 0.15,\n",
    "    'seed' : 42,\n",
    "    \n",
    "}\n",
    "\n",
    "def fix_seed(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed(config['seed'])\n",
    "print(\"Seed set to\", config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "117c403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class MakeSequenceDataSet():\n",
    "    def __init__(self, config):\n",
    "        dat1 = os.path.join(config['data_path'], 'ratings.dat')\n",
    "        dat2 = os.path.join(config['data_path'], 'rating.dat')\n",
    "        dat_path = dat1 if os.path.exists(dat1) else dat2\n",
    "\n",
    "        if not os.path.exists(dat_path):\n",
    "            raise FileNotFoundError(f\"Could not find ratings.dat at {config['data_path']}\")\n",
    "\n",
    "        print(\"Loading:\", dat_path)\n",
    "\n",
    "        # Correct read for MovieLens 1M .dat format\n",
    "        self.df = pd.read_csv(\n",
    "            dat_path,\n",
    "            sep=\"::\",\n",
    "            engine=\"python\",      # Required for multi-char separator\n",
    "            header=None,          # No header row in the file\n",
    "            names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "            encoding=\"latin-1\"    # Avoids encoding errors\n",
    "        )\n",
    "\n",
    "        must = {'userId','movieId','rating','timestamp'}\n",
    "        missing = must - set(self.df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['movieId'].apply(lambda x: self.item_encoder[x] + 1)  # 1..num_item\n",
    "        self.df['user_idx'] = self.df['userId'].apply(lambda x: self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp'])\n",
    "\n",
    "        # temporal train/valid/test split\n",
    "        self.user_train, self.user_valid, self.user_test = self.generate_sequence_data()\n",
    "        print(\"Users with sequences:\", len(self.user_train), \"| Items:\", self.num_item)\n",
    "\n",
    "\n",
    "    def generate_encoder_decoder(self, col:str):\n",
    "        encoder, decoder = {}, {}\n",
    "        ids = self.df[col].unique()\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "        return encoder, decoder\n",
    "\n",
    "    def generate_sequence_data(self):\n",
    "        users = defaultdict(list)\n",
    "        user_train, user_valid, user_test = {}, {}, {}\n",
    "        for user, g in self.df.groupby('user_idx'):\n",
    "            seq = g['item_idx'].tolist()\n",
    "            if len(seq) < 3:\n",
    "                continue\n",
    "            users[user] = seq\n",
    "        for user, seq in users.items():\n",
    "            user_train[user] = seq[:-2]\n",
    "            user_valid[user] = [seq[-2]]\n",
    "            user_test[user]  = [seq[-1]]\n",
    "        return user_train, user_valid, user_test\n",
    "\n",
    "    def get_splits(self):\n",
    "        return self.user_train, self.user_valid, self.user_test\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=3.5):\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    df = df.copy()\n",
    "    # Keep positives only\n",
    "    if threshold is not None:\n",
    "        df = df[df['rating'] >= float(threshold)].copy()\n",
    "    # Stable sort and keep last rating per (user,item)\n",
    "    df = df.sort_values(['userId','movieId','timestamp'], kind='mergesort').reset_index(drop=True)\n",
    "    df = df.groupby(['userId','movieId'], as_index=False).tail(1)\n",
    "    # Map to indices\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    # Build sequences\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.sort_values(['user_idx','timestamp'], kind='mergesort').iterrows():\n",
    "        user_pos[row['user_idx']].append(int(row['item_idx']))\n",
    "    # Enforce >=5 positives per user\n",
    "    user_pos = {u: seq for u, seq in user_pos.items() if len(seq) >= 5}\n",
    "    return user_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "518f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Validation metrics: next-item Recall@K and NDCG@K (correct target handling) ===\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_ndcg_at_k(model, user_pos, num_items, max_len, k=10, batch_size=2048, device=None):\n",
    "    \"\"\"\n",
    "    Standard next-item eval:\n",
    "      cond = seq[:-1], target = seq[-1].\n",
    "    We DO NOT mask the target even if it appeared earlier in cond.\n",
    "    \"\"\"\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    MASK = num_items + 1  # placeholder; will set to V-1 after probing V  # IMPORTANT: this repo uses MASK = num_items + 1  # placeholder; will set to V-1 after probing V\n",
    "\n",
    "    rows, targets, seen_lists = [], [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        cond = seq[:-1]\n",
    "        tgt  = seq[-1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1:\n",
    "            s = s[-(max_len - 1):]  # leave room for [MASK]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        rows.append([PAD]*pad + list(s) + [MASK])\n",
    "        targets.append(tgt)\n",
    "        seen_lists.append(list(set(s)))  # we'll unmask target below\n",
    "\n",
    "    if not rows:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    X = torch.tensor(rows, dtype=torch.long, device=dev)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=dev)\n",
    "\n",
    "    # Determine logits width V from model\n",
    "    V = model(X[:1])[:, -1, :].shape[1]\n",
    "    MASK = V - 1\n",
    "    X = X.clamp_max(V - 1)\n",
    "\n",
    "    # Build suppression mask (B,V) for PAD/MASK/seen EXCEPT the target\n",
    "    B = X.shape[0]\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V:\n",
    "        seen_mask[:, MASK] = True\n",
    "\n",
    "    r_idx, c_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        items_set = set(it for it in items if 0 <= it < V)\n",
    "        tgt_i = int(targets[i].item())\n",
    "        if 0 <= tgt_i < V and tgt_i in items_set:\n",
    "            items_set.remove(tgt_i)\n",
    "        if items_set:\n",
    "            r_idx.extend([i]*len(items_set))\n",
    "            c_idx.extend(list(items_set))\n",
    "    if r_idx:\n",
    "        seen_mask[torch.tensor(r_idx, device=dev), torch.tensor(c_idx, device=dev)] = True\n",
    "\n",
    "    # Sanity check: target must not be masked\n",
    "    assert not seen_mask[torch.arange(B, device=dev), targets.clamp_min(0).clamp_max(V-1)].any(), \"Target was masked!\"\n",
    "\n",
    "    hits = 0.0\n",
    "    ndcgs = 0.0\n",
    "    for s in range(0, B, batch_size):\n",
    "        e = min(B, s+batch_size)\n",
    "        logits = model(X[s:e])[:, -1, :]\n",
    "        logits = logits.masked_fill(seen_mask[s:e], -1e9)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        tgts = targets[s:e].unsqueeze(1)\n",
    "\n",
    "        hit_rows = (topk == tgts).any(dim=1).float()\n",
    "        hits += hit_rows.sum().item()\n",
    "\n",
    "        where = (topk == tgts).nonzero(as_tuple=False)\n",
    "        if where.numel() > 0:\n",
    "            ndcgs += (1.0 / torch.log2(where[:,1].float() + 2.0)).sum().item()\n",
    "\n",
    "    n = float(B)\n",
    "    return hits / n, ndcgs / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe5a74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, max_len, num_item, mask_prob=0.15):\n",
    "        self.max_len = max_len\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "        self.users = list(user_train.keys())\n",
    "        self.inputs, self.labels = [], []\n",
    "        for user in self.users:\n",
    "            seq = user_train[user]\n",
    "            tokens = seq[-max_len:] if len(seq) > max_len else [0]*(max_len-len(seq)) + seq\n",
    "            masked_tokens, label_tokens = self.mask_sequence(tokens)\n",
    "            self.inputs.append(masked_tokens)\n",
    "            self.labels.append(label_tokens)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def mask_sequence(self, tokens):\n",
    "        masked_tokens = tokens.copy()\n",
    "        labels = [0]*len(tokens)  # PAD=0\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == 0:\n",
    "                continue\n",
    "            if random.random() < self.mask_prob:\n",
    "                labels[i] = tokens[i]  # store original ID\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    masked_tokens[i] = self.num_item + 1  # MASK token in inputs only\n",
    "                elif prob < 0.9:\n",
    "                    masked_tokens[i] = random.randint(1, self.num_item)\n",
    "        return masked_tokens, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38fb6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model): super().__init__(); self.pe = nn.Embedding(max_len, d_model)\n",
    "    def forward(self, x): return self.pe.weight.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512): super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None: p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__(); assert d_model % h == 0\n",
    "        self.d_k = d_model // h; self.h = h\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model); self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B = query.size(0)\n",
    "        query, key, value = [l(x).view(B, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi)*(x + 0.044715*torch.pow(x,3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__(); self.w_1 = nn.Linear(d_model, d_ff); self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout); self.activation = GELU()\n",
    "    def forward(self, x): return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6): super().__init__(); self.a_2 = nn.Parameter(torch.ones(features)); self.b_2 = nn.Parameter(torch.zeros(features)); self.eps = eps\n",
    "    def forward(self, x): mean = x.mean(-1, keepdim=True); std = x.std(-1, keepdim=True); return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout): super().__init__(); self.norm = LayerNorm(size); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward); return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_max_len, num_items, bert_num_blocks, bert_num_heads,\n",
    "                 bert_hidden_units, bert_dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = bert_max_len; self.num_items = num_items\n",
    "        self.hidden = bert_hidden_units\n",
    "        self.embedding = BERTEmbedding(vocab_size=num_items+2, embed_size=self.hidden, max_len=bert_max_len, dropout=bert_dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(self.hidden, bert_num_heads, self.hidden*4, bert_dropout) for _ in range(bert_num_blocks)])\n",
    "        self.out = nn.Linear(self.hidden, num_items + 2)  # 0..num_items\n",
    "    def forward_hidden(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        h = self.embedding(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            h = transformer.forward(h, mask)\n",
    "        return h\n",
    "    def forward(self, x):\n",
    "        h = self.forward_hidden(x)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ca1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device=DEVICE):\n",
    "    model.train(); loss_val = 0.0\n",
    "    for seq, labels in tqdm(data_loader):\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        logits = model(seq).view(-1, model.out.out_features)  # (bs*t, vocab)\n",
    "        labels = labels.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    return loss_val / max(1, len(data_loader))\n",
    "\n",
    "def prepare_baseline(config):\n",
    "    \"\"\"\n",
    "    Trains a baseline BERT4Rec model and evaluates next-item Recall/NDCG correctly.\n",
    "    IMPORTANT: Validation is evaluated on sequences built as\n",
    "      user_valid_full[u] = user_train[u] + user_valid[u]\n",
    "    so that cond = prefix, target = last (valid) item.\n",
    "    \"\"\"\n",
    "    # 1) Build sequences\n",
    "    mds = MakeSequenceDataSet(config)\n",
    "    user_train, user_valid, user_test = mds.user_train, mds.user_valid, mds.user_test\n",
    "    num_item = mds.num_item\n",
    "\n",
    "    # 2) DataLoader from training prefixes\n",
    "    train_ds = BERTRecDataSet(user_train, max_len=config['max_len'], num_item=num_item, mask_prob=0.15)\n",
    "    dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
    "\n",
    "    # 3) Model\n",
    "    model = BERT(\n",
    "        bert_max_len=config['max_len'],\n",
    "        num_items=num_item,   # this class internally adds +2 for PAD/MASK head\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # 4) Validation spec: build full sequences with context\n",
    "    user_valid_full = {\n",
    "        u: (user_train[u] + user_valid[u])\n",
    "        for u in user_valid\n",
    "        if u in user_train and len(user_train[u]) >= 1 and len(user_valid[u]) == 1\n",
    "    }\n",
    "\n",
    "    K = int(config.get('TOPK', 10)) if isinstance(config, dict) else 10\n",
    "    for epoch in range(1, int(config['num_epochs']) + 1):\n",
    "        l = train_one_epoch(model, criterion, opt, dl, device=DEVICE)\n",
    "        print(f'Epoch: {epoch:3d}| Train loss: {l:.5f}')\n",
    "        # Evaluate next-item ranking on validation (prefix->valid_item)\n",
    "        _was = model.training\n",
    "        model.eval()\n",
    "        try:\n",
    "            _rec, _ndcg = recall_ndcg_at_k(\n",
    "                model, user_valid_full, num_item, config['max_len'],\n",
    "                k=K, batch_size=2048, device=DEVICE\n",
    "            )\n",
    "        finally:\n",
    "            if _was: model.train()\n",
    "        print(f\"[VAL] Recall@{K}={_rec:.4f} | NDCG@{K}={_ndcg:.4f}\")\n",
    "    torch.save(model.state_dict(), \"baseline_best.pt\")\n",
    "    print(\"Saved baseline_best.pt\")\n",
    "    return mds, user_train, user_valid, user_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ee7bd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embedding_from_model(model, seq, num_items, max_len):\n",
    "    # Average of valid hidden states\n",
    "    s = seq[-max_len:] if len(seq) > max_len else seq\n",
    "    PAD = 0; MASK = num_items + 2\n",
    "    pad = max_len - len(s)\n",
    "    x = torch.tensor([[PAD]*pad + s], dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model.forward_hidden(x)[0]  # (L, H)\n",
    "        valid = h[pad:pad+len(s)]\n",
    "        return valid.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "def compute_user_embeddings(model, train_seqs, num_items, max_len):\n",
    "    user_vecs = {}\n",
    "    for u, seq in tqdm(list(train_seqs.items()), desc=\"user embeddings\"):\n",
    "        user_vecs[u] = user_embedding_from_model(model, seq, num_items, max_len)\n",
    "    U = np.stack([user_vecs[u] for u in user_vecs.keys()])\n",
    "    user_index = list(user_vecs.keys())\n",
    "    return U, user_index\n",
    "\n",
    "def build_collective_quota(seed_cluster, labels, users, size, p, *, seed=None):\n",
    "    import random\n",
    "    rnd = random.Random(seed) if seed is not None else random\n",
    "\n",
    "    # split pools\n",
    "    seed_users  = [u for u, lab in zip(users, labels) if lab == seed_cluster]\n",
    "    other_users = [u for u, lab in zip(users, labels) if lab != seed_cluster]\n",
    "    rnd.shuffle(seed_users); rnd.shuffle(other_users)\n",
    "\n",
    "    # exact target counts\n",
    "    n_seed  = min(len(seed_users), int(round(p * size)))\n",
    "    n_other = min(len(other_users), size - n_seed)\n",
    "\n",
    "    members = seed_users[:n_seed] + other_users[:n_other]\n",
    "\n",
    "    # backfill if one pool was too small\n",
    "    if len(members) < size:\n",
    "        spill = seed_users[n_seed:] + other_users[n_other:]\n",
    "        rnd.shuffle(spill)\n",
    "        members += spill[: size - len(members)]\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "def top_items_for_collective(members, train_seqs, topn=10):\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for u in members:\n",
    "        for it in set(train_seqs.get(u, [])):  # count each item once per user\n",
    "            c[it] += 1\n",
    "    return [it for it, _ in c.most_common(topn)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "480a9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: resolve number of items robustly from mds/model\n",
    "def resolve_num_items(mds, model):\n",
    "    # Try common dataset attributes first\n",
    "    for attr in ('num_items', 'n_items', 'item_vocab_size'):\n",
    "        if hasattr(mds, attr):\n",
    "            try:\n",
    "                n = int(getattr(mds, attr))\n",
    "                if n > 0:\n",
    "                    return n\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Try encoder sizes\n",
    "    for attr in ('item_encoder', 'item2id', 'item_to_idx'):\n",
    "        enc = getattr(mds, attr, None)\n",
    "        if isinstance(enc, dict) and len(enc) > 0:\n",
    "            return int(len(enc))\n",
    "    # Try model output head (often vocab size = num_items + specials)\n",
    "    out_features = getattr(getattr(model, 'out', None), 'out_features', None)\n",
    "    if isinstance(out_features, int) and out_features > 0:\n",
    "        # Heuristic: BERT4Rec-style uses tokens: 0=PAD, +1=[CLS], +2=[MASK]\n",
    "        # So num_items ≈ out_features - 3\n",
    "        return max(1, int(out_features) - 3)\n",
    "    raise RuntimeError(\"Could not resolve number of items from mds/model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "877dc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_from_baseline_on_ratings(\n",
    "    baseline_state_path,\n",
    "    ratings_df,\n",
    "    mds,\n",
    "    epochs=8,\n",
    "    lr=5e-4,\n",
    "    *,\n",
    "    eval_specs=None,   # list of {\"name\",\"members_idx\",\"item_set_enc\",\"baseline_hr\", [\"solo_hr\"]}\n",
    "    eval_split='user_test',\n",
    "    k=None,\n",
    "    device=None,\n",
    "    **kwargs           # swallow unused args for compatibility\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrain from baseline weights on a modified ratings_df and print, per epoch:\n",
    "      - training loss\n",
    "      - HR@K for each eval spec\n",
    "      - relative HR (vs baseline_hr)\n",
    "      - constructiveness CT = rel_joint - rel_solo (if solo_hr provided)\n",
    "\n",
    "    Returns: (mds, user_pos_mod, None, None, model)\n",
    "    \"\"\"\n",
    "    # -----------------------------\n",
    "    # 0) Setup and defaults\n",
    "    # -----------------------------\n",
    "    dev = device if device is not None else DEVICE\n",
    "    K = k if k is not None else (config.get('TOPK', 10) if isinstance(config, dict) else 10)\n",
    "\n",
    "    # Paper protocol: evaluate HR over all users in chosen split\n",
    "    try:\n",
    "        global_eval_subset = getattr(mds, eval_split)\n",
    "    except Exception:\n",
    "        global_eval_subset = None\n",
    "\n",
    "    max_len = int(config.get('max_len', 50))\n",
    "    mask_prob = float(config.get('mask_prob', 0.15))\n",
    "    batch_size = int(config.get('batch_size', 128))\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Rebuild sequences from ratings_df (TRAIN-ONLY) to avoid leakage\n",
    "    # -----------------------------\n",
    "    try:\n",
    "        import pickle, numpy as _np\n",
    "        # Tier 1: load baseline splits\n",
    "        with open(SPLITS_PATH, \"rb\") as f:\n",
    "            _user_train, _user_valid, _user_test = pickle.load(f)\n",
    "        _train_len = {u: len(seq) for u, seq in _user_train.items()}\n",
    "    except Exception:\n",
    "        # Tier 2: try mds.user_train\n",
    "        _user_train_attr = getattr(mds, 'user_train', None)\n",
    "        if isinstance(_user_train_attr, dict) and len(_user_train_attr) > 0:\n",
    "            _train_len = {u: len(seq) for u, seq in _user_train_attr.items()}\n",
    "        else:\n",
    "            # Tier 3: heuristic fallback (drop last 2 interactions per user)\n",
    "            _train_len = None\n",
    "            _HEURISTIC_LAST_K = 2\n",
    "\n",
    "    _user_decoder = getattr(mds, 'user_decoder', None)\n",
    "    if _user_decoder is None and hasattr(mds, 'user_encoder'):\n",
    "        _user_decoder = {v: k for k, v in mds.user_encoder.items()}\n",
    "\n",
    "    _df_sorted = ratings_df.sort_values(['userId','timestamp'], kind='mergesort')\n",
    "\n",
    "    if isinstance(_train_len, dict):\n",
    "        # Exact cutoff using train lengths\n",
    "        _ts_valid_map = {}\n",
    "        for u_idx, raw_uid in _user_decoder.items():\n",
    "            trl = _train_len.get(u_idx, None)\n",
    "            if trl is None:\n",
    "                continue\n",
    "            u_df = _df_sorted[_df_sorted['userId'] == raw_uid]\n",
    "            if len(u_df) > trl:\n",
    "                _ts_valid_map[raw_uid] = int(u_df.iloc[trl]['timestamp'])\n",
    "            else:\n",
    "                _ts_valid_map[raw_uid] = int(1<<62)\n",
    "        df_train_only = _df_sorted[_df_sorted.apply(\n",
    "            lambda r: r['timestamp'] < _ts_valid_map.get(r['userId'], _np.inf), axis=1\n",
    "        )]\n",
    "    else:\n",
    "        # Heuristic fallback\n",
    "        _ranks = _df_sorted.groupby('userId')['timestamp'].rank(method='first', ascending=True)\n",
    "        _maxr = _df_sorted.groupby('userId')['timestamp'].transform('count')\n",
    "        df_train_only = _df_sorted[_ranks <= (_maxr - _HEURISTIC_LAST_K)]\n",
    "\n",
    "    # Build sequences\n",
    "    user_pos_mod = rebuild_sequences_from_df(\n",
    "        df_train_only,\n",
    "        mds.item_encoder,\n",
    "        mds.user_encoder,\n",
    "        threshold=None\n",
    "    )\n",
    "    user_pos_mod = {u: [int(x) for x in seq] for u, seq in user_pos_mod.items() if len(seq) > 0}\n",
    "    if not user_pos_mod:\n",
    "        raise ValueError(\"No user sequences after rebuild; check ratings_df and threshold.\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Prepare dataset and model\n",
    "    # -----------------------------\n",
    "    enc_num_item = getattr(mds, 'num_item', len(getattr(mds, 'item_encoder', {})))\n",
    "    max_seen = max(max(seq) for seq in user_pos_mod.values())\n",
    "    num_item = max(enc_num_item, max_seen)\n",
    "\n",
    "    ds = BERTRecDataSet(user_pos_mod, max_len=max_len, num_item=num_item, mask_prob=mask_prob)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "\n",
    "    model = BERT(\n",
    "        bert_max_len=max_len,\n",
    "        num_items=num_item,\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(dev)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Load baseline checkpoint (resize if needed)\n",
    "    # -----------------------------\n",
    "    \n",
    "    ckpt = torch.load(baseline_state_path, map_location=dev, weights_only=True)\n",
    "    model_sd = model.state_dict()\n",
    "    # Infer vocab size from checkpoint and freeze vocab\n",
    "    _sd = ckpt['state_dict'] if isinstance(ckpt, dict) and 'state_dict' in ckpt else ckpt\n",
    "    _V = None\n",
    "    for key in ['out.weight','bert.prediction.weight','prediction.weight',\n",
    "                'embedding.token.weight','bert.item_embedding.weight','item_embedding.weight',\n",
    "                'bert.embeddings.word_embeddings.weight']:\n",
    "        if key in _sd:\n",
    "            _V = _sd[key].shape[0]\n",
    "            break\n",
    "    if _V is not None:\n",
    "        _num_from_ckpt = int(_V) - 2  # PAD=0, MASK=last\n",
    "        if _num_from_ckpt != num_item:\n",
    "            print(f\"[vocab] aligning num_item {num_item} -> {_num_from_ckpt} based on checkpoint V={_V}\")\n",
    "            num_item = _num_from_ckpt\n",
    "            ds = BERTRecDataSet(user_pos_mod, max_len=max_len, num_item=num_item, mask_prob=mask_prob)\n",
    "            dl = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n",
    "            model = BERT(\n",
    "        bert_max_len=max_len,\n",
    "        num_items=num_item,\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(dev)\n",
    "    \n",
    "    \n",
    "        def _resize_like_param(src, dst):\n",
    "            if src.shape == dst.shape:\n",
    "                return src\n",
    "            if src.ndim != dst.ndim or src.shape[1:] != dst.shape[1:]:\n",
    "                return dst  # incompatible, keep destination\n",
    "            new_t = dst.clone()\n",
    "            n = min(src.shape[0], dst.shape[0])\n",
    "            new_t[:n] = src[:n]\n",
    "            return new_t\n",
    "    \n",
    "        vocab_keys = ['embedding.token.weight','bert.item_embedding.weight',\n",
    "                      'item_embedding.weight','bert.embeddings.word_embeddings.weight']\n",
    "        out_w_keys = ['out.weight','bert.prediction.weight','prediction.weight']\n",
    "        out_b_keys = ['out.bias','bert.prediction.bias','prediction.bias']\n",
    "    \n",
    "        patched = {}\n",
    "        for k, v in ckpt.items():\n",
    "            if k in model_sd:\n",
    "                if k in vocab_keys or k in out_w_keys:\n",
    "                    patched[k] = _resize_like_param(v, model_sd[k])\n",
    "                elif k in out_b_keys:\n",
    "                    if v.shape != model_sd[k].shape:\n",
    "                        nb = min(v.shape[0], model_sd[k].shape[0])\n",
    "                        new_b = model_sd[k].clone()\n",
    "                        new_b[:nb] = v[:nb]\n",
    "                        patched[k] = new_b\n",
    "                    else:\n",
    "                        patched[k] = v\n",
    "                else:\n",
    "                    patched[k] = v if v.shape == model_sd[k].shape else model_sd[k]\n",
    "    \n",
    "        missing, unexpected = model.load_state_dict(patched, strict=False)\n",
    "        if missing:\n",
    "            print(f\"[info] Missing keys after load: {missing}\")\n",
    "        if unexpected:\n",
    "            print(f\"[info] Unexpected keys ignored: {unexpected}\")\n",
    "    \n",
    "        # -----------------------------\n",
    "    # 4) Training setup\n",
    "    # -----------------------------\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "    grad_clip = float(config.get('grad_clip', 1.0))\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    def _train_one_epoch_nan_safe():\n",
    "        model.train()\n",
    "        total_loss, total_tokens = 0.0, 0\n",
    "        for batch in dl:\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: (v.to(dev) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
    "                inputs = batch.get('input_ids') or batch.get('seqs')\n",
    "                labels = batch.get('labels') or batch.get('targets')\n",
    "                attn = batch.get('attention_mask') or batch.get('attn_mask')\n",
    "            else:\n",
    "                inputs, labels = batch[0].to(dev), batch[1].to(dev)\n",
    "\n",
    "            if labels is None:\n",
    "                continue\n",
    "            valid = (labels != 0)\n",
    "            if valid.sum().item() == 0:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(inputs)  # (B, T, V)\n",
    "            B, T, V = logits.shape\n",
    "            loss = crit(logits.view(B*T, V), labels.view(B*T))\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * valid.sum().item()\n",
    "            total_tokens += valid.sum().item()\n",
    "        return total_loss / max(total_tokens, 1)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Evaluation helpers\n",
    "    # -----------------------------\n",
    "    def _coerce_itemset(item_set_enc):\n",
    "        if isinstance(item_set_enc, (list, tuple)):\n",
    "            v = torch.tensor(item_set_enc, dtype=torch.float32)\n",
    "        elif torch.is_tensor(item_set_enc):\n",
    "            v = item_set_enc.float().cpu()\n",
    "        else:\n",
    "            raise TypeError(\"item_set_enc must be list/tuple/tensor\")\n",
    "        if v.ndim != 1:\n",
    "            v = v.view(-1)\n",
    "        if len(v) < num_item:\n",
    "            v = torch.cat([v, torch.zeros(num_item - len(v))], dim=0)\n",
    "        elif len(v) > num_item:\n",
    "            v = v[:num_item]\n",
    "        return v.to(dev)\n",
    "\n",
    "    def _eval_one(spec):\n",
    "        aligned_itemset = _coerce_itemset(spec['item_set_enc'])\n",
    "        subset = global_eval_subset if global_eval_subset else user_pos_mod\n",
    "\n",
    "        seen_override = {u: (_user_train.get(u, [])[:-1][- (max_len-1):] if u in _user_train else []) for u in subset.keys()}\n",
    "        eff_targets = int((aligned_itemset[:num_item] > 0).sum().item())\n",
    "        print(f\"[eval parity] {spec['name']}: users={len(subset)} | targets={eff_targets} | V={getattr(getattr(model,'out',None),'out_features',-1)} | num_items_arg={num_item}\")\n",
    "        hr = float(hr_for_itemset(model, subset, aligned_itemset, num_item, max_len, K, seen_override=seen_override))\n",
    "\n",
    "        rel = hr / spec['baseline_hr'] if spec.get('baseline_hr', 0) > 0 else None\n",
    "        ct = None\n",
    "        if spec.get('solo_hr') is not None and spec['baseline_hr'] > 0:\n",
    "            ct = (hr / spec['baseline_hr']) - (spec['solo_hr'] / spec['baseline_hr'])\n",
    "        return hr, rel, ct\n",
    "\n",
    "    # -----------------------------\n",
    "    # 6) Main training loop\n",
    "    # -----------------------------\n",
    "    _per_spec = {spec.get('name','spec'): {'hr': [], 'rel': [], 'ct': []} for spec in eval_specs or []}\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        avg_loss = _train_one_epoch_nan_safe()\n",
    "        if eval_specs:\n",
    "            parts = []\n",
    "            for spec in eval_specs:\n",
    "                try:\n",
    "                    hr, rel, ct = _eval_one(spec)\n",
    "                    _per_spec[spec['name']]['hr'].append(hr)\n",
    "                    _per_spec[spec['name']]['rel'].append(rel)\n",
    "                    _per_spec[spec['name']]['ct'].append(ct)\n",
    "\n",
    "                    msg = f\"{spec['name']}: HR@{K}={hr:.4f}\"\n",
    "                    if rel is not None: msg += f\" | rel={rel:.3f}\"\n",
    "                    if ct is not None: msg += f\" | CT={ct:.3f}\"\n",
    "                except Exception as e:\n",
    "                    msg = f\"{spec.get('name','?')}: EVAL-ERROR {type(e).__name__}: {e}\"\n",
    "                parts.append(msg)\n",
    "            print(f\"Epoch {epoch:2d} | loss {avg_loss:.5f} || \" + \" || \".join(parts), flush=True)\n",
    "        else:\n",
    "            print(f\"Epoch {epoch:2d} | loss {avg_loss:.5f}\", flush=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 7) Save average stats\n",
    "    # -----------------------------\n",
    "    def _mean_ignore_none(xs): \n",
    "        vals = [x for x in xs if x is not None]\n",
    "        return sum(vals)/len(vals) if vals else None\n",
    "\n",
    "    avg_stats = {nm: {k: _mean_ignore_none(v) for k,v in d.items()} | {'epochs': len(d['hr'])}\n",
    "                 for nm, d in _per_spec.items()}\n",
    "    try:\n",
    "        model._avg_eval_stats = avg_stats\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return mds, user_pos_mod, None, None, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c83fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def hr_for_itemset(model, user_pos, item_set, num_items, max_len, k=10, batch_size=2048, device=None, seen_override=None):\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    mask_guess = num_items + 1  # we'll clamp after we see V_model\n",
    "\n",
    "    # Build inputs (prefix + MASK) and remember \"seen\" tokens per row\n",
    "    inps, seen_lists = [], []\n",
    "    for u, seq in user_pos.items():\n",
    "        if not seq:\n",
    "            continue\n",
    "        # use prefix (next-item style); keep up to max_len-1 then place MASK at the end\n",
    "        cond = seq[:-1] if len(seq) >= 1 else seq\n",
    "        cond = cond[-(max_len-1):]\n",
    "        pad = (max_len - 1) - len(cond)\n",
    "        inps.append(([PAD] * pad) + cond + [mask_guess])\n",
    "        seen_lists.append(set(seen_override.get(u, cond)) if isinstance(seen_override, dict) else set(cond))\n",
    "\n",
    "    if not inps:\n",
    "        return 0.0\n",
    "\n",
    "    X = torch.tensor(inps, dtype=torch.long, device=dev)\n",
    "    target_ids = torch.as_tensor(item_set, dtype=torch.long).flatten().tolist()\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    B = X.size(0)\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(B, start + batch_size)\n",
    "        Xb = X[start:end]\n",
    "\n",
    "        # First forward to discover V_model\n",
    "        logits = model(Xb)[:, -1, :]             # (b, V_model)\n",
    "        V = logits.shape[1]\n",
    "\n",
    "        # Clamp tokens in the batch to valid range [0, V-1] and recompute logits to be safe\n",
    "        Xb = Xb.clamp_max(V - 1)\n",
    "        logits = model(Xb)[:, -1, :]\n",
    "\n",
    "        # Decide MASK id safely\n",
    "        MASK = V - 1\n",
    "\n",
    "        # Suppress PAD & MASK\n",
    "        logits[:, 0] = -1e9\n",
    "        if 0 <= MASK < V:\n",
    "            logits[:, MASK] = -1e9\n",
    "\n",
    "        # Suppress anything beyond real (items + special) if model head is larger\n",
    "        # (items are 1..num_items; special are 0 (PAD) and MASK)\n",
    "        if V > num_items + 2:\n",
    "            logits[:, (num_items + 2):] = -1e9\n",
    "\n",
    "        # Seen-mask with the correct width V\n",
    "        seen_mask = torch.zeros((Xb.size(0), V), dtype=torch.bool, device=dev)\n",
    "        for r, sset in enumerate(seen_lists[start:end]):\n",
    "            for t in sset:\n",
    "                if 0 <= int(t) < V:\n",
    "                    seen_mask[r, int(t)] = True\n",
    "        logits = logits.masked_fill(seen_mask, -1e9)\n",
    "\n",
    "        # Target mask (width V)\n",
    "        target_mask = torch.zeros(V, dtype=torch.bool, device=dev)\n",
    "        for t in target_ids:\n",
    "            t = int(t)\n",
    "            # only allow real item ids 1..num_items, and also within model width\n",
    "            if 1 <= t <= num_items and t < V:\n",
    "                target_mask[t] = True\n",
    "\n",
    "        # Top-k hits\n",
    "        topk_idx = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit_rows = target_mask[topk_idx].any(dim=1)\n",
    "        hits += int(hit_rows.sum().item())\n",
    "        total += Xb.size(0)\n",
    "    \n",
    "    \n",
    "    return float(hits / max(1, total))\n",
    "\n",
    "@torch.no_grad()\n",
    "def relative_hr(model_variant, model_baseline, user_pos, item_set, num_items, max_len, k=10):\n",
    "    g_base = hr_for_itemset(model_baseline, user_pos, item_set, num_items, max_len, k=k)\n",
    "    g_var  = hr_for_itemset(model_variant,   user_pos, item_set, num_items, max_len, k=k)\n",
    "    eps = 1e-9\n",
    "    rel    = (g_var / max(g_base, eps)) if g_base > 0 else 0.0\n",
    "    return rel, g_var, g_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e30a140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_next_items(model, cond_seq, num_items, max_len):\n",
    "    \"\"\"\n",
    "    Robust scorer that:\n",
    "    - Detects the model's embedding vocab size by scanning modules for nn.Embedding.\n",
    "    - Ensures all token ids are within [0, vocab_size).\n",
    "    - Uses PAD=0 and MASK=1 when possible (common BERT4Rec scheme).\n",
    "    - Suppresses PAD, MASK, and seen tokens in logits.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 1) Find an embedding to infer vocab_size\n",
    "    vocab_size = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                vocab_size = int(m.num_embeddings)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    PAD = 0\n",
    "    MASK = 1 if vocab_size is not None and vocab_size > 1 else (num_items + 2)\n",
    "\n",
    "    # 2) Build masked sequence with strict truncation\n",
    "    s = cond_seq[-max_len:] if len(cond_seq) > max_len else cond_seq\n",
    "    if len(s) > max_len - 1:\n",
    "        s = s[-(max_len - 1):]\n",
    "\n",
    "    # 3) Sanitize IDs to prevent CUDA device-side asserts\n",
    "    if vocab_size is not None:\n",
    "        s = [int(x) for x in s if 0 <= int(x) < vocab_size]\n",
    "        # Final check for MASK validity\n",
    "        if not (0 <= MASK < vocab_size):\n",
    "            MASK = 1 if vocab_size > 1 else 0\n",
    "\n",
    "    pad = max_len - len(s) - 1\n",
    "    if pad < 0: pad = 0\n",
    "    seq_ids = [PAD]*pad + s + [MASK]\n",
    "\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    inp = torch.tensor([seq_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # 4) Forward -> logits for the MASK position\n",
    "    out = model(inp)\n",
    "    logits = out[0, -1].clone()\n",
    "\n",
    "    # 5) Suppress invalid or special predictions\n",
    "    if 0 <= PAD < logits.numel():   logits[PAD] = -1e9\n",
    "    if 0 <= MASK < logits.numel():  logits[MASK] = -1e9\n",
    "    for seen in set(s):\n",
    "        if 0 <= seen < logits.numel():\n",
    "            logits[seen] = -1e9\n",
    "\n",
    "    # 6) If num_items provided < logits size, clip tail so metrics only consider real items\n",
    "    if isinstance(num_items, int) and num_items < logits.numel():\n",
    "        logits[num_items:] = -1e9\n",
    "\n",
    "    return logits  # (vocab,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d11c",
   "metadata": {},
   "source": [
    "# ✅ Paper-faithful item_set builders (post-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b735b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "V_TARGET = 10 if 'config' not in globals() else config.get('V', 10)\n",
    "\n",
    "def _member_mean_topV(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    if sub.empty:\n",
    "        return []\n",
    "    means = (sub.groupby('movieId')['rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(V).index.tolist())\n",
    "    return means\n",
    "\n",
    "def _member_popular(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    pop = (sub.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _global_popular(ratings_df, V):\n",
    "    pop = (ratings_df.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _encode_items(raw_ids, item_encoder):\n",
    "    enc0 = [item_encoder[r] for r in raw_ids if r in item_encoder]\n",
    "    return [e + 1 for e in enc0]  # BERT4Rec label space\n",
    "\n",
    "def build_item_set_for_members(mds, members_idx, V=10, min_support=None):\n",
    "    \"\"\"\n",
    "    Paper-faithful group target selection with optional positive support filter.\n",
    "    - Compute top-V by mean rating within the collective.\n",
    "    - If min_support is set, require at least that many member ratings >= 4.\n",
    "    - If fewer than V items, pad using 'member-popular' items from the same collective.\n",
    "    - No global fallback (strict cluster-only).\n",
    "    \"\"\"\n",
    "    member_raw = [mds.user_decoder[u] for u in members_idx if u in mds.user_decoder]\n",
    "    if not member_raw:\n",
    "        return [], []\n",
    "    df = mds.df[mds.df['userId'].isin(set(member_raw))][['userId','movieId','rating']].copy()\n",
    "    if df.empty:\n",
    "        return [], []\n",
    "\n",
    "    # Step 1: compute candidate items\n",
    "    if isinstance(min_support, int) and min_support > 0:\n",
    "        pos = df[df['rating'] >= 4.0]\n",
    "        sup = (pos.groupby('movieId')['rating'].count()).rename('pos_support')\n",
    "        means = (df.groupby('movieId')['rating'].mean()).rename('mean_rating').to_frame()\n",
    "        means = means.join(sup, how='left').fillna({'pos_support': 0})\n",
    "        means = means[means['pos_support'] >= min_support]\n",
    "        top_raw = (means.sort_values(['mean_rating','pos_support'], ascending=[False, False])\n",
    "                         .head(V).index.tolist())\n",
    "    else:\n",
    "        top_raw = (df.groupby('movieId')['rating']\n",
    "                     .mean().sort_values(ascending=False).head(V).index.tolist())\n",
    "\n",
    "    # Step 2: pad with member-popular if needed\n",
    "    if len(top_raw) < V:\n",
    "        # count how often each item was rated by members\n",
    "        pop = df.groupby('movieId')['rating'].count().sort_values(ascending=False)\n",
    "        for iid in pop.index:\n",
    "            if iid not in top_raw:\n",
    "                top_raw.append(iid)\n",
    "            if len(top_raw) >= V:\n",
    "                break\n",
    "\n",
    "    # Step 3: encode\n",
    "    top_enc = [mds.item_encoder[mid] + 1 for mid in top_raw if mid in mds.item_encoder]\n",
    "    return top_raw[:V], top_enc[:V]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a1fb8",
   "metadata": {},
   "source": [
    "def build_item_set_for_members(mds, members_idx, V=10):\n",
    "    ratings_df = mds.df\n",
    "    user_decoder = mds.user_decoder\n",
    "    member_raw = [user_decoder[u] for u in members_idx if u in user_decoder]\n",
    "\n",
    "    raw = _member_mean_topV(ratings_df, member_raw, V)\n",
    "    if len(raw) < V:\n",
    "        for iid in _member_popular(ratings_df, member_raw, V*3):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "    if len(raw) < V:\n",
    "        for iid in _global_popular(ratings_df, V*5):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "\n",
    "    enc = _encode_items(raw[:V], mds.item_encoder)\n",
    "    return raw[:V], enc[:V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a015104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _cluster_centroids(U, labels, Q):\n",
    "    \"\"\"Return (Q x d) centroid matrix in user-embedding space.\"\"\"\n",
    "    centroids = []\n",
    "    for q in range(Q):\n",
    "        idx = np.where(labels == q)[0]\n",
    "        # KMeans guarantees non-empty clusters; just in case:\n",
    "        if len(idx) == 0:\n",
    "            # fallback: random user as centroid\n",
    "            centroids.append(U[np.random.randint(len(U))])\n",
    "        else:\n",
    "            centroids.append(U[idx].mean(axis=0))\n",
    "    C = np.vstack(centroids)\n",
    "    return C\n",
    "\n",
    "def _normalize_rows(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def _farthest_pair_indices(centroids, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Return indices (i, j) of the two centroids that are maximally distant.\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        Cn = _normalize_rows(centroids)\n",
    "        # cosine distance = 1 - cosine similarity\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        # Euclidean\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    # ignore diagonal\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "    ij = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "    return int(ij[0]), int(ij[1]), float(dists[ij])\n",
    "\n",
    "\n",
    "def _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.5):\n",
    "    \"\"\"\n",
    "    Pick two clusters that are far apart but balanced in mean item popularity.\n",
    "    balance_tol: allowable relative difference in mean popularity (e.g., 0.25 = 25%).\n",
    "    Returns (i, j, distance).\n",
    "    \"\"\"\n",
    "    # Popularity of each item from training data\n",
    "    from collections import Counter\n",
    "    cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "\n",
    "    # Mean popularity per cluster (average of users' mean item popularity)\n",
    "    cluster_pop = {}\n",
    "    Q = len(centroids)\n",
    "    for q in range(Q):\n",
    "        user_idxs = [k for k, lab in enumerate(labels) if lab == q]\n",
    "        pops = []\n",
    "        for k in user_idxs:\n",
    "            u = user_index[k]\n",
    "            seq = mds.user_train.get(u, [])\n",
    "            if seq:\n",
    "                pops.append(float(np.mean([cnt[i] for i in seq])))\n",
    "        cluster_pop[q] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "\n",
    "    # Pairwise distances between centroids\n",
    "    if metric == \"cosine\":\n",
    "        Cn = centroids / (np.linalg.norm(centroids, axis=1, keepdims=True) + 1e-12)\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "\n",
    "    # Choose farthest pair among those with similar popularity\n",
    "    best_pair, best_dist = None, -np.inf\n",
    "    for i in range(Q):\n",
    "        for j in range(i+1, Q):\n",
    "            pa, pb = cluster_pop.get(i, 0.0), cluster_pop.get(j, 0.0)\n",
    "            if pa == 0.0 or pb == 0.0:\n",
    "                continue\n",
    "            rel_diff = abs(pa - pb) / max(pa, pb)\n",
    "            if rel_diff <= balance_tol:\n",
    "                d = dists[i, j]\n",
    "                if d > best_dist:\n",
    "                    best_dist = d\n",
    "                    best_pair = (i, j)\n",
    "\n",
    "    if best_pair is None:\n",
    "        # Fallback to farthest regardless of balance\n",
    "        i, j = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "        return int(i), int(j), float(dists[i, j])\n",
    "    return int(best_pair[0]), int(best_pair[1]), float(best_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d43dc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_edits(df_before, df_after, users_idx, target_items_raw,\n",
    "                action, user_decoder, sample_k=5, add_competing=False, label=None):\n",
    "    import numpy as np, pandas as pd, random\n",
    "    from collections import Counter\n",
    "\n",
    "    tset = set(int(x) for x in target_items_raw) if target_items_raw else set()\n",
    "    user_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_ids or not tset:\n",
    "        print(\"[check_edits] nothing to check (empty users or targets)\")\n",
    "        return\n",
    "\n",
    "    dfb = df_before.sort_values([\"userId\",\"timestamp\"], kind=\"mergesort\").copy()\n",
    "    dfa = df_after .sort_values([\"userId\",\"timestamp\"], kind=\"mergesort\").copy()\n",
    "\n",
    "    sample_users = random.sample(user_ids, k=min(sample_k, len(user_ids)))\n",
    "\n",
    "    def ts_valid_for_user(g):\n",
    "        ts = g[\"timestamp\"].values\n",
    "        if len(ts) >= 2:\n",
    "            return int(ts[-2])                  # boundary from BEFORE\n",
    "        return np.iinfo(np.int64).max          # no tail if < 2\n",
    "\n",
    "    ok = True\n",
    "    for uid in sample_users:\n",
    "        gb = dfb[dfb.userId == uid]\n",
    "        ga = dfa[dfa.userId == uid]\n",
    "        if gb.empty:\n",
    "            continue\n",
    "        ts_valid = ts_valid_for_user(gb)\n",
    "\n",
    "        # Split using BEFORE's boundary; tail are rows with ts >= ts_valid\n",
    "        b_tail = gb[gb.timestamp >= ts_valid][[\"movieId\",\"timestamp\"]].sort_values([\"timestamp\",\"movieId\"]).reset_index(drop=True)\n",
    "        a_tail = ga[ga.timestamp >= ts_valid][[\"movieId\",\"timestamp\"]].sort_values([\"timestamp\",\"movieId\"]).reset_index(drop=True)\n",
    "\n",
    "        # Order-robust equality\n",
    "        if not a_tail.equals(b_tail):\n",
    "            ok = False\n",
    "            print(f\"[check_edits:{action}] uid={uid}: tail (valid/test) changed, which should not happen\")\n",
    "\n",
    "        # Train region diagnostics\n",
    "        b_train = gb[gb.timestamp < ts_valid]\n",
    "        a_train = ga[ga.timestamp < ts_valid]\n",
    "        if action == \"promote\":\n",
    "            # must have new target rows in train\n",
    "            added = a_train.merge(b_train, how=\"outer\", indicator=True)\n",
    "            added = added[added[\"_merge\"] == \"left_only\"]\n",
    "            if added[added.movieId.isin(tset)].empty:\n",
    "                ok = False\n",
    "                print(f\"[check_edits:promote] uid={uid}: no new target interactions in TRAIN\")\n",
    "        elif action == \"demote\":\n",
    "            # train targets should be removed\n",
    "            b_train_targets = b_train[b_train.movieId.isin(tset)]\n",
    "            if not b_train_targets.empty:\n",
    "                a_train_targets = a_train[a_train.movieId.isin(tset)]\n",
    "                if len(a_train_targets) >= len(b_train_targets):\n",
    "                    ok = False\n",
    "                    print(f\"[check_edits:demote] uid={uid}: train targets not removed \"\n",
    "                          f\"(before={len(b_train_targets)} after={len(a_train_targets)})\")\n",
    "            if add_competing:\n",
    "                added = a_train.merge(b_train, how=\"outer\", indicator=True)\n",
    "                added = added[added[\"_merge\"] == \"left_only\"]\n",
    "                if added[~added.movieId.isin(tset)].empty:\n",
    "                    ok = False\n",
    "                    print(f\"[check_edits:demote] uid={uid}: expected a competing interaction, found none\")\n",
    "\n",
    "    if ok:\n",
    "        tag = f\"{label}:\" if label else \"\"\n",
    "        print(f\"[check_edits:{tag}{action}] All sampled users respect train-only edit invariants ✓\")\n",
    "\n",
    "\n",
    "\n",
    "def hr_split(model, test_dict, item_set_enc, members, NUM_ITEMS, max_len, k=10):\n",
    "    \"\"\"Compute HR@K separately for members and non-members, given test_dict.\"\"\"\n",
    "    members_set = set(members)\n",
    "    member_dict = {u: seq for u, seq in test_dict.items() if u in members_set}\n",
    "    nonmember_dict = {u: seq for u, seq in test_dict.items() if u not in members_set}\n",
    "\n",
    "    hr_members    = hr_for_itemset(model, member_dict,  item_set_enc, NUM_ITEMS, max_len, k) if member_dict else 0.0\n",
    "    hr_nonmembers = hr_for_itemset(model, nonmember_dict, item_set_enc, NUM_ITEMS, max_len, k) if nonmember_dict else 0.0\n",
    "    return hr_members, hr_nonmembers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c383f8",
   "metadata": {},
   "source": [
    "# 🚀 RQ2 Experiment (baseline → collectives → item_set → interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89661ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline] Found existing baseline pack — loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user embeddings:  90%|████████▉ | 5412/6040 [00:28<00:03, 180.42it/s]"
     ]
    }
   ],
   "source": [
    "# ===== Collective experiment params (from paper notebook) =====\n",
    "N_values   = [ 10, 20, 50]            # collective sizes\n",
    "p_values   = [0.1,0.25, 0.5, 0.75, 1.0]  # homogeneity\n",
    "trials_per_case = 30\n",
    "TOPK       = 10\n",
    "V_TARGET   = config.get('V', 10)\n",
    "NUM_CLUSTERS = 10\n",
    "SEED_MODE  = locals().get(\"SEED_MODE\", \"balanced_maxdist\")  # keep your previous choice\n",
    "\n",
    "scenarios = [\n",
    "    ('promote', 'promote', 'both_promote'),\n",
    "    ('demote',  'demote',  'both_demote'),\n",
    "    ('promote', 'demote',  'A_promote_B_demote'),\n",
    "    ('demote',  'promote', 'A_demote_B_promote'),\n",
    "]\n",
    "\n",
    "# ---------------- Repro & \"baseline pack\" paths ----------------\n",
    "import os, json, pickle, sys, gc, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BASELINE_DIR  = \"baseline_pack\"\n",
    "BASELINE_PATH = os.path.join(BASELINE_DIR, \"baseline_best.pt\")\n",
    "MDS_PATH      = os.path.join(BASELINE_DIR, \"mds.pkl\")\n",
    "SPLITS_PATH   = os.path.join(BASELINE_DIR, \"splits.pkl\")\n",
    "META_PATH     = os.path.join(BASELINE_DIR, \"meta.json\")\n",
    "os.makedirs(BASELINE_DIR, exist_ok=True)\n",
    "\n",
    "# Determinism: keep this consistent across machines\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(config['seed'])\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # safe on CPU; improves CUDA determinism\n",
    "\n",
    "random.seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------- Load or create the baseline pack ----------------\n",
    "def _save_baseline_pack(mds, user_train, user_valid, user_test, baseline_model):\n",
    "    torch.save(baseline_model.state_dict(), BASELINE_PATH)\n",
    "    with open(MDS_PATH, \"wb\") as f:\n",
    "        pickle.dump(mds, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SPLITS_PATH, \"wb\") as f:\n",
    "        pickle.dump((user_train, user_valid, user_test), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    meta = {\n",
    "        \"seed\": int(config['seed']),\n",
    "        \"config\": {k: (int(v) if isinstance(v, (np.integer,)) else v) for k, v in config.items()},\n",
    "        \"versions\": {\n",
    "            \"python\": sys.version,\n",
    "            \"torch\": getattr(torch, \"__version__\", \"unknown\"),\n",
    "            \"numpy\": np.__version__,\n",
    "        },\n",
    "    }\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[baseline] Saved baseline pack to {BASELINE_DIR}/\")\n",
    "\n",
    "\n",
    "def _load_baseline_pack():\n",
    "    with open(MDS_PATH, \"rb\") as f:\n",
    "        mds = pickle.load(f)\n",
    "    with open(SPLITS_PATH, \"rb\") as f:\n",
    "        user_train, user_valid, user_test = pickle.load(f)\n",
    "\n",
    "    # Fast path to rebuild model from checkpoint with 0 training epochs\n",
    "    mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "        baseline_state_path=BASELINE_PATH,\n",
    "        ratings_df=mds.df,\n",
    "        mds=mds,\n",
    "        epochs=0,\n",
    "        eval_split=None,\n",
    "        eval_specs=[],\n",
    "        k=TOPK,\n",
    "    )\n",
    "\n",
    "    # Keep the original splits\n",
    "    return mds, user_train, user_valid, user_test, model_s\n",
    "\n",
    "\n",
    "if os.path.exists(BASELINE_PATH) and os.path.exists(MDS_PATH) and os.path.exists(SPLITS_PATH):\n",
    "    print(\"[baseline] Found existing baseline pack — loading.\")\n",
    "    mds, user_train, user_valid, user_test, baseline_model = _load_baseline_pack()\n",
    "else:\n",
    "    print(\"[baseline] No baseline pack found — training once and saving.\")\n",
    "    mds, user_train, user_valid, user_test, baseline_model = prepare_baseline(config)\n",
    "    _save_baseline_pack(mds, user_train, user_valid, user_test, baseline_model)\n",
    "\n",
    "# ---------------- Rest of your pipeline ----------------\n",
    "NUM_ITEMS = resolve_num_items(mds, baseline_model)\n",
    "\n",
    "U, user_index = compute_user_embeddings(baseline_model, user_train, NUM_ITEMS, config['max_len'])\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42).fit(U)\n",
    "labels = kmeans.labels_\n",
    "centroids = _cluster_centroids(U, labels, NUM_CLUSTERS)\n",
    "\n",
    "print(\"V_model =\", baseline_model.out.out_features)  # should be num_items+2\n",
    "print(\"num_items =\", NUM_ITEMS)\n",
    "print(\"expected V =\", NUM_ITEMS + 2)\n",
    "results = []\n",
    "\n",
    "for N in N_values:\n",
    "    for p in p_values:\n",
    "        # ---- Build two collectives A and B ----\n",
    "        if SEED_MODE == \"uniform\":\n",
    "            seedA = np.random.randint(NUM_CLUSTERS)\n",
    "            seedB = (seedA + np.random.randint(1, NUM_CLUSTERS)) % NUM_CLUSTERS\n",
    "        elif SEED_MODE == \"balanced_maxdist\":\n",
    "            i, j, d = _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.7)\n",
    "            seedA, seedB = i, j\n",
    "        elif SEED_MODE == \"maxdist\":\n",
    "            i, j, d = _farthest_pair_indices(centroids, metric=\"cosine\")\n",
    "            seedA, seedB = i, j\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SEED_MODE={SEED_MODE}\")\n",
    "\n",
    "        C1 = build_collective_quota(seedA, labels, user_index, size=N, p=p, seed=42)\n",
    "        C2 = build_collective_quota(seedB, labels, user_index, size=N, p=p, seed=42)\n",
    "\n",
    "        # ---- Item sets ----\n",
    "        raw_A, enc_A = build_item_set_for_members(mds, C1, V=V_TARGET, min_support=3)\n",
    "        raw_B, enc_B = build_item_set_for_members(mds, C2, V=V_TARGET, min_support=3)\n",
    "        print(f\"[item_set] A: raw={len(raw_A)} enc={len(enc_A)} | B: raw={len(raw_B)} enc={len(enc_B)}\")\n",
    "\n",
    "        from collections import Counter\n",
    "        cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "        popA = sum(cnt[i] for i in enc_A)/len(enc_A)\n",
    "        popB = sum(cnt[i] for i in enc_B)/len(enc_B)\n",
    "        print(\"Mean frequency — A:\", popA, \" | B:\", popB)\n",
    "\n",
    "        # ---- Baseline HR@K ----\n",
    "        subset_ALL = user_test\n",
    "        g1_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_A, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        g2_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_B, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        print(g1_base)\n",
    "        print(g2_base)\n",
    "\n",
    "        # ---- Edited dataframes ----\n",
    "        df0 = mds.df.copy()\n",
    "        df_A_prom = edit_ratings(df0, C1, raw_A, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_A_demo = edit_ratings(df0, C1, raw_A, action=\"demote\", user_decoder=mds.user_decoder)\n",
    "        df_B_prom = edit_ratings(df0, C2, raw_B, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_B_demo = edit_ratings(df0, C2, raw_B, action=\"demote\", user_decoder=mds.user_decoder)\n",
    "\n",
    "        # ---- Retraining helpers ----\n",
    "        def run_single(edited_df, members, item_set_enc, name, baseline_hr):\n",
    "            mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=BASELINE_PATH,\n",
    "                ratings_df=edited_df,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=5,\n",
    "                eval_split=\"user_test\",\n",
    "                item_set=item_set_enc,\n",
    "                eval_specs=[{\"name\": name, \"members_idx\": user_test,\n",
    "                             \"item_set_enc\": item_set_enc, \"baseline_hr\": baseline_hr}],\n",
    "                k=TOPK\n",
    "            )\n",
    "            stats = getattr(model_s, \"_avg_eval_stats\", {}) or {}\n",
    "            s = stats.get(name, {}) or {}\n",
    "            return s.get('hr'), s.get('rel'), s.get('ct'), model_s\n",
    "\n",
    "        def run_joint(df_A_action, df_B_action, solo_A, solo_B):\n",
    "            df_joint = edit_ratings(mds.df.copy(), C1, raw_A, action=df_A_action, user_decoder=mds.user_decoder)\n",
    "            df_joint = edit_ratings(df_joint, C2, raw_B, action=df_B_action, user_decoder=mds.user_decoder)\n",
    "            mds_j, tr_j, va_j, te_j, model_j = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=BASELINE_PATH,\n",
    "                ratings_df=df_joint,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=6,\n",
    "                eval_split=\"user_test\",\n",
    "                eval_specs=[\n",
    "                    {\"name\": \"A\", \"members_idx\": user_test, \"item_set_enc\": enc_A,\n",
    "                     \"baseline_hr\": g1_base, \"solo_hr\": solo_A},\n",
    "                    {\"name\": \"B\", \"members_idx\": user_test, \"item_set_enc\": enc_B,\n",
    "                     \"baseline_hr\": g2_base, \"solo_hr\": solo_B},\n",
    "                ],\n",
    "                k=TOPK,\n",
    "            )\n",
    "            stats = getattr(model_j, \"_avg_eval_stats\", {}) or {}\n",
    "            A = stats.get(\"A\", {}) or {}\n",
    "            B = stats.get(\"B\", {}) or {}\n",
    "            return A.get('hr'), A.get('rel'), A.get('ct'), B.get('hr'), B.get('rel'), B.get('ct'), model_j\n",
    "\n",
    "        # ---- Trials ----\n",
    "        for trial in range(trials_per_case):\n",
    "            # Solo A\n",
    "            g1_demo, g1_demo_rel, g1_demo_ct, model_A_demo = run_single(df_A_demo, C1, enc_A, 'A', g1_base)\n",
    "            hr_mem_A, hr_nonmem_A = hr_split(model_A_demo, user_test, enc_A, C1, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 A demote: members={hr_mem_A:.3f}, non-members={hr_nonmem_A:.3f}, overall={g1_demo:.3f}\")\n",
    "\n",
    "            g1_prom, g1_prom_rel, g1_prom_ct, model_A_prom = run_single(df_A_prom, C1, enc_A, 'A', g1_base)\n",
    "            hr_mem_A, hr_nonmem_A = hr_split(model_A_prom, user_test, enc_A, C1, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 A promote: members={hr_mem_A:.3f}, non-members={hr_nonmem_A:.3f}, overall={g1_prom:.3f}\")\n",
    "\n",
    "            # Solo B\n",
    "            g2_demo, g2_demo_rel, g2_demo_ct, model_B_demo = run_single(df_B_demo, C2, enc_B, 'B', g2_base)\n",
    "            hr_mem_B, hr_nonmem_B = hr_split(model_B_demo, user_test, enc_B, C2, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 B demote: members={hr_mem_B:.3f}, non-members={hr_nonmem_B:.3f}, overall={g2_demo:.3f}\")\n",
    "\n",
    "            g2_prom, g2_prom_rel, g2_prom_ct, model_B_prom = run_single(df_B_prom, C2, enc_B, 'B', g2_base)\n",
    "            hr_mem_B, hr_nonmem_B = hr_split(model_B_prom, user_test, enc_B, C2, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 B promote: members={hr_mem_B:.3f}, non-members={hr_nonmem_B:.3f}, overall={g2_prom:.3f}\")\n",
    "\n",
    "            # Joint interventions\n",
    "            gA_pp, gA_pp_rel, gA_pp_ct, gB_pp, gB_pp_rel, gB_pp_ct, model_joint = run_joint(\"promote\", \"promote\", g1_prom, g2_prom)\n",
    "            print(f\"[sanity] HR@10 Joint both promote: A={gA_pp:.3f}, B={gB_pp:.3f}\")\n",
    "\n",
    "            gA_dd, gA_dd_rel, gA_dd_ct, gB_dd, gB_dd_rel, gB_dd_ct, model_joint = run_joint(\"demote\", \"demote\", g1_demo, g2_demo)\n",
    "            print(f\"[sanity] HR@10 Joint both demote: A={gA_dd:.3f}, B={gB_dd:.3f}\")\n",
    "\n",
    "            gA_pd, gA_pd_rel, gA_pd_ct, gB_pd, gB_pd_rel, gB_pd_ct, model_joint = run_joint(\"promote\", \"demote\", g1_prom, g2_demo)\n",
    "            print(f\"[sanity] HR@10 Joint A promote / B demote: A={gA_pd:.3f}, B={gB_pd:.3f}\")\n",
    "\n",
    "            gA_dp, gA_dp_rel, gA_dp_ct, gB_dp, gB_dp_rel, gB_dp_ct, model_joint = run_joint(\"demote\", \"promote\", g1_demo, g2_prom)\n",
    "            print(f\"[sanity] HR@10 Joint A demote / B promote: A={gA_dp:.3f}, B={gB_dp:.3f}\")\n",
    "\n",
    "            # Collect results\n",
    "            results.append({\n",
    "                \"N\": N, \"p\": p, \"trial\": trial,\n",
    "                \"gA_base\": g1_base, \"gB_base\": g2_base,\n",
    "                \"gA_prom\": g1_prom, \"gA_demo\": g1_demo,\n",
    "                \"gB_prom\": g2_prom, \"gB_demo\": g2_demo,\n",
    "                \"gA_pp\": gA_pp, \"gB_pp\": gB_pp,\n",
    "                \"gA_dd\": gA_dd, \"gB_dd\": gB_dd,\n",
    "                \"gA_pd\": gA_pd, \"gB_pd\": gB_pd,\n",
    "                \"gA_dp\": gA_dp, \"gB_dp\": gB_dp,\n",
    "                \"rA_prom\": g1_prom_rel, \"ctA_prom\": g1_prom_ct,\n",
    "                \"rA_demo\": g1_demo_rel, \"ctA_demo\": g1_demo_ct,\n",
    "                \"rB_prom\": g2_prom_rel, \"ctB_prom\": g2_prom_ct,\n",
    "                \"rB_demo\": g2_demo_rel, \"ctB_demo\": g2_demo_ct,\n",
    "                \"rA_pp\": gA_pp_rel, \"ctA_pp\": gA_pp_ct,\n",
    "                \"rB_pp\": gB_pp_rel, \"ctB_pp\": gB_pp_ct,\n",
    "                \"rA_dd\": gA_dd_rel, \"ctA_dd\": gA_dd_ct,\n",
    "                \"rB_dd\": gB_dd_rel, \"ctB_dd\": gB_dd_ct,\n",
    "                \"rA_pd\": gA_pd_rel, \"ctA_pd\": gA_pd_ct,\n",
    "                \"rB_pd\": gB_pd_rel, \"ctB_pd\": gB_pd_ct,\n",
    "                \"size_A\": len(C1), \"size_B\": len(C2)\n",
    "            })\n",
    "\n",
    "            gc.collect()\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Save & preview\n",
    "import pandas as pd\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"rq2_results_relative_hr.csv\", index=False)\n",
    "display(res_df.head())\n",
    "print(\"Saved rq2_results_relative_hr.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_A: [803, 179, 155, 92, 147, 15, 355, 373, 493, 2247]\n",
      "enc_A: [1612, 2597, 1610, 1689, 2482, 1851, 1773, 2265, 1108, 1356]\n",
      "raw_B: [1545, 2073, 270, 907, 3079, 18, 3420, 1627, 3062, 1237]\n",
      "enc_B: [29, 2269, 3197, 1545, 250, 1959, 1234, 1920, 1473, 1458]\n",
      "A raw vs enc size: 10 10\n",
      "B raw vs enc size: 10 10\n"
     ]
    }
   ],
   "source": [
    "print(\"raw_A:\", raw_A)\n",
    "print(\"enc_A:\", enc_A)\n",
    "print(\"raw_B:\", raw_B)\n",
    "print(\"enc_B:\", enc_B)\n",
    "\n",
    "# Check how many items were lost during encoding\n",
    "print(\"A raw vs enc size:\", len(raw_A), len(enc_A))\n",
    "print(\"B raw vs enc size:\", len(raw_B), len(enc_B))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
