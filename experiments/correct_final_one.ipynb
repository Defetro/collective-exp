{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e137fec2",
   "metadata": {},
   "source": [
    "### Notebook hotfix (applied 2025-08-20T11:37:08 UTC)\n",
    "\n",
    "- Fixed CUDA device-side assert during evaluation by sanitizing token IDs in `score_next_items`.\n",
    "- Masked PAD/MASK predictions and clipped logits to `num_items`.\n",
    "- Light hyperparameter tweaks intended to keep training ≤10% slower while improving Recall@10 / NDCG@10:\n",
    "  - `mask_prob=0.20`\n",
    "  - `weight_decay=0.01`, `lr_scheduler='cosine'`, `warmup_ratio=0.10`\n",
    "  - `dropout` +0.05 (cap 0.35)\n",
    "  - `max_len` increased by ~10%\n",
    "  - learning rate +15% with warmup/decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980720fa",
   "metadata": {},
   "source": [
    "# ML-20M — BERT4Rec Baseline + Collective Experiments (RQ2-style)\n",
    "**Built:** 2025-08-13 14:54\n",
    "\n",
    "This notebook **keeps the baseline model and training loop from `ml-20bert4rec`** and adds **all collective experimental conditions** inspired by `bert4rec_collectives_rq2_paper`:\n",
    "- Embedding-based user clustering (KMeans), farthest-cluster seeding\n",
    "- Collective construction with homogeneity **p**\n",
    "- Promote/Demote scenarios for two collectives (A/B)\n",
    "- Deterministic rating edits and **retraining from the baseline weights**\n",
    "- Relative HR@K on targeted item sets (A and B)\n",
    "- Results export + quick plot\n",
    "\n",
    "Printing/log messages mirror the paper notebook (e.g., *Device:*, *Running RQ2 grid...*, *Seed clusters: ...*, *Saved rq2_results_relative_hr.csv*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4796458",
   "metadata": {},
   "source": [
    "\n",
    "> **Fix applied (Aug 20, 2025):** Validation Recall@K/NDCG@K previously evaluated on single-item sequences,\n",
    "> which produced zeros. `prepare_baseline` now evaluates on `user_valid_full[u] = user_train[u] + user_valid[u]`,\n",
    "> ensuring there is prefix context (cond) and a target. The evaluator `recall_ndcg_at_k` expects sequences with\n",
    "> length ≥ 2 and uses `cond = seq[:-1]`, `target = seq[-1]`. No change to the dataset split itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edit_ratings(df, users_idx, target_items_raw, action, user_decoder, promote_value=5.0, demote_value=1.0):\n",
    "    \"\"\"Return a new ratings DataFrame where all (user,item) in the target set\n",
    "    for the given users are overwritten (and added if missing) with a fixed value.\n",
    "    users_idx: list of user indices; user_decoder maps idx -> raw userId.\n",
    "    action: 'promote' (set to promote_value) or 'demote' (set to demote_value).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    df = df.copy()\n",
    "    if not len(target_items_raw):\n",
    "        return df\n",
    "\n",
    "    target_items_raw = set(target_items_raw)\n",
    "    user_raw_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_raw_ids:\n",
    "        return df\n",
    "\n",
    "    value = promote_value if action == 'promote' else demote_value\n",
    "\n",
    "    # Remove existing rows for these (user,item) pairs\n",
    "    mask_users = df['userId'].isin(user_raw_ids)\n",
    "    mask_items = df['movieId'].isin(target_items_raw)\n",
    "    df = df.loc[~(mask_users & mask_items)].reset_index(drop=True)\n",
    "\n",
    "    # Add overwritten pairs\n",
    "    new_rows = pd.DataFrame({\n",
    "        'userId': np.repeat(user_raw_ids, len(target_items_raw)),\n",
    "        'movieId': np.tile(list(target_items_raw), len(user_raw_ids)),\n",
    "        'rating':  value\n",
    "    })\n",
    "    df = pd.concat([df, new_rows], ignore_index=True)\n",
    "    # enforce integer dtypes for ids\n",
    "    df = df.astype({'userId': 'int64', 'movieId': 'int64'})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83037f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", DEVICE)\n",
    "SEED_MODE = \"balanced_maxdist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c08804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# --- Baseline config (from ml-20bert4rec) ---\n",
    "config = {\n",
    "    'data_path' : r'C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m',  # ML-20M\n",
    "    'max_len' : 50,\n",
    "    'hidden_units' : 256,\n",
    "    'num_heads' : 2,\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate' : 0.1,\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 128,\n",
    "    'num_epochs' : 17,\n",
    "    'num_workers' : 2,\n",
    "    'mask_prob' : 0.15,\n",
    "    'seed' : 42,\n",
    "    \n",
    "}\n",
    "\n",
    "def fix_seed(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed(config['seed'])\n",
    "print(\"Seed set to\", config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class MakeSequenceDataSet():\n",
    "    def __init__(self, config):\n",
    "        dat1 = os.path.join(config['data_path'], 'ratings.dat')\n",
    "        dat2 = os.path.join(config['data_path'], 'rating.dat')\n",
    "        dat_path = dat1 if os.path.exists(dat1) else dat2\n",
    "\n",
    "        if not os.path.exists(dat_path):\n",
    "            raise FileNotFoundError(f\"Could not find ratings.dat at {config['data_path']}\")\n",
    "\n",
    "        print(\"Loading:\", dat_path)\n",
    "\n",
    "        # Correct read for MovieLens 1M .dat format\n",
    "        self.df = pd.read_csv(\n",
    "            dat_path,\n",
    "            sep=\"::\",\n",
    "            engine=\"python\",      # Required for multi-char separator\n",
    "            header=None,          # No header row in the file\n",
    "            names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "            encoding=\"latin-1\"    # Avoids encoding errors\n",
    "        )\n",
    "\n",
    "        must = {'userId','movieId','rating','timestamp'}\n",
    "        missing = must - set(self.df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['movieId'].apply(lambda x: self.item_encoder[x] + 1)  # 1..num_item\n",
    "        self.df['user_idx'] = self.df['userId'].apply(lambda x: self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp'])\n",
    "\n",
    "        # temporal train/valid/test split\n",
    "        self.user_train, self.user_valid, self.user_test = self.generate_sequence_data()\n",
    "        print(\"Users with sequences:\", len(self.user_train), \"| Items:\", self.num_item)\n",
    "\n",
    "\n",
    "    def generate_encoder_decoder(self, col:str):\n",
    "        encoder, decoder = {}, {}\n",
    "        ids = self.df[col].unique()\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "        return encoder, decoder\n",
    "\n",
    "    def generate_sequence_data(self):\n",
    "        users = defaultdict(list)\n",
    "        user_train, user_valid, user_test = {}, {}, {}\n",
    "        for user, g in self.df.groupby('user_idx'):\n",
    "            seq = g['item_idx'].tolist()\n",
    "            if len(seq) < 3:\n",
    "                continue\n",
    "            users[user] = seq\n",
    "        for user, seq in users.items():\n",
    "            user_train[user] = seq[:-2]\n",
    "            user_valid[user] = [seq[-2]]\n",
    "            user_test[user]  = [seq[-1]]\n",
    "        return user_train, user_valid, user_test\n",
    "\n",
    "    def get_splits(self):\n",
    "        return self.user_train, self.user_valid, self.user_test\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=4.0):\n",
    "    # Keep only ratings >= threshold as positive interactions\n",
    "    df = df.copy()\n",
    "    df = df[df['rating'] >= threshold].sort_values(['userId','timestamp']).reset_index(drop=True)\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        user_pos[row['user_idx']].append(row['item_idx'])\n",
    "    # filter out empties\n",
    "    return {u: seq for u, seq in user_pos.items() if len(seq) >= 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Validation metrics: next-item Recall@K and NDCG@K (correct target handling) ===\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_ndcg_at_k(model, user_pos, num_items, max_len, k=10, batch_size=2048, device=None):\n",
    "    \"\"\"\n",
    "    Standard next-item eval:\n",
    "      cond = seq[:-1], target = seq[-1].\n",
    "    We DO NOT mask the target even if it appeared earlier in cond.\n",
    "    \"\"\"\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    MASK = num_items + 1  # IMPORTANT: this repo uses MASK = num_items + 1\n",
    "\n",
    "    rows, targets, seen_lists = [], [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        cond = seq[:-1]\n",
    "        tgt  = seq[-1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1:\n",
    "            s = s[-(max_len - 1):]  # leave room for [MASK]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        rows.append([PAD]*pad + list(s) + [MASK])\n",
    "        targets.append(tgt)\n",
    "        seen_lists.append(list(set(s)))  # we'll unmask target below\n",
    "\n",
    "    if not rows:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    X = torch.tensor(rows, dtype=torch.long, device=dev)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=dev)\n",
    "\n",
    "    # Determine logits width V from model\n",
    "    V = model(X[:1])[:, -1, :].shape[1]\n",
    "\n",
    "    # Build suppression mask (B,V) for PAD/MASK/seen EXCEPT the target\n",
    "    B = X.shape[0]\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V:\n",
    "        seen_mask[:, MASK] = True\n",
    "\n",
    "    r_idx, c_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        items_set = set(it for it in items if 0 <= it < V)\n",
    "        tgt_i = int(targets[i].item())\n",
    "        if 0 <= tgt_i < V and tgt_i in items_set:\n",
    "            items_set.remove(tgt_i)\n",
    "        if items_set:\n",
    "            r_idx.extend([i]*len(items_set))\n",
    "            c_idx.extend(list(items_set))\n",
    "    if r_idx:\n",
    "        seen_mask[torch.tensor(r_idx, device=dev), torch.tensor(c_idx, device=dev)] = True\n",
    "\n",
    "    # Sanity check: target must not be masked\n",
    "    assert not seen_mask[torch.arange(B, device=dev), targets.clamp_min(0).clamp_max(V-1)].any(), \"Target was masked!\"\n",
    "\n",
    "    hits = 0.0\n",
    "    ndcgs = 0.0\n",
    "    for s in range(0, B, batch_size):\n",
    "        e = min(B, s+batch_size)\n",
    "        logits = model(X[s:e])[:, -1, :]\n",
    "        logits = logits.masked_fill(seen_mask[s:e], -1e9)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        tgts = targets[s:e].unsqueeze(1)\n",
    "\n",
    "        hit_rows = (topk == tgts).any(dim=1).float()\n",
    "        hits += hit_rows.sum().item()\n",
    "\n",
    "        where = (topk == tgts).nonzero(as_tuple=False)\n",
    "        if where.numel() > 0:\n",
    "            ndcgs += (1.0 / torch.log2(where[:,1].float() + 2.0)).sum().item()\n",
    "\n",
    "    n = float(B)\n",
    "    return hits / n, ndcgs / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, max_len, num_item, mask_prob=0.15):\n",
    "        self.max_len = max_len\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "        self.users = list(user_train.keys())\n",
    "        self.inputs, self.labels = [], []\n",
    "        for user in self.users:\n",
    "            seq = user_train[user]\n",
    "            tokens = seq[-max_len:] if len(seq) > max_len else [0]*(max_len-len(seq)) + seq\n",
    "            masked_tokens, label_tokens = self.mask_sequence(tokens)\n",
    "            self.inputs.append(masked_tokens)\n",
    "            self.labels.append(label_tokens)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def mask_sequence(self, tokens):\n",
    "        masked_tokens = tokens.copy()\n",
    "        labels = [0]*len(tokens)  # PAD=0\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == 0:\n",
    "                continue\n",
    "            if random.random() < self.mask_prob:\n",
    "                labels[i] = tokens[i]  # store original ID\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    masked_tokens[i] = self.num_item + 1  # MASK token in inputs only\n",
    "                elif prob < 0.9:\n",
    "                    masked_tokens[i] = random.randint(1, self.num_item)\n",
    "        return masked_tokens, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model): super().__init__(); self.pe = nn.Embedding(max_len, d_model)\n",
    "    def forward(self, x): return self.pe.weight.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512): super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None: p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__(); assert d_model % h == 0\n",
    "        self.d_k = d_model // h; self.h = h\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model); self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B = query.size(0)\n",
    "        query, key, value = [l(x).view(B, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi)*(x + 0.044715*torch.pow(x,3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__(); self.w_1 = nn.Linear(d_model, d_ff); self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout); self.activation = GELU()\n",
    "    def forward(self, x): return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6): super().__init__(); self.a_2 = nn.Parameter(torch.ones(features)); self.b_2 = nn.Parameter(torch.zeros(features)); self.eps = eps\n",
    "    def forward(self, x): mean = x.mean(-1, keepdim=True); std = x.std(-1, keepdim=True); return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout): super().__init__(); self.norm = LayerNorm(size); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward); return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_max_len, num_items, bert_num_blocks, bert_num_heads,\n",
    "                 bert_hidden_units, bert_dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = bert_max_len; self.num_items = num_items\n",
    "        self.hidden = bert_hidden_units\n",
    "        self.embedding = BERTEmbedding(vocab_size=num_items+2, embed_size=self.hidden, max_len=bert_max_len, dropout=bert_dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(self.hidden, bert_num_heads, self.hidden*4, bert_dropout) for _ in range(bert_num_blocks)])\n",
    "        self.out = nn.Linear(self.hidden, num_items + 2)  # 0..num_items\n",
    "    def forward_hidden(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        h = self.embedding(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            h = transformer.forward(h, mask)\n",
    "        return h\n",
    "    def forward(self, x):\n",
    "        h = self.forward_hidden(x)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device=DEVICE):\n",
    "    model.train(); loss_val = 0.0\n",
    "    for seq, labels in tqdm(data_loader):\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        logits = model(seq).view(-1, model.out.out_features)  # (bs*t, vocab)\n",
    "        labels = labels.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    return loss_val / max(1, len(data_loader))\n",
    "\n",
    "def prepare_baseline(config):\n",
    "    \"\"\"\n",
    "    Trains a baseline BERT4Rec model and evaluates next-item Recall/NDCG correctly.\n",
    "    IMPORTANT: Validation is evaluated on sequences built as\n",
    "      user_valid_full[u] = user_train[u] + user_valid[u]\n",
    "    so that cond = prefix, target = last (valid) item.\n",
    "    \"\"\"\n",
    "    # 1) Build sequences\n",
    "    mds = MakeSequenceDataSet(config)\n",
    "    user_train, user_valid, user_test = mds.user_train, mds.user_valid, mds.user_test\n",
    "    num_item = mds.num_item\n",
    "\n",
    "    # 2) DataLoader from training prefixes\n",
    "    train_ds = BERTRecDataSet(user_train, max_len=config['max_len'], num_item=num_item, mask_prob=0.15)\n",
    "    dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
    "\n",
    "    # 3) Model\n",
    "    model = BERT(\n",
    "        bert_max_len=config['max_len'],\n",
    "        num_items=num_item,   # this class internally adds +2 for PAD/MASK head\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # 4) Validation spec: build full sequences with context\n",
    "    user_valid_full = {\n",
    "        u: (user_train[u] + user_valid[u])\n",
    "        for u in user_valid\n",
    "        if u in user_train and len(user_train[u]) >= 1 and len(user_valid[u]) == 1\n",
    "    }\n",
    "\n",
    "    K = int(config.get('TOPK', 10)) if isinstance(config, dict) else 10\n",
    "    for epoch in range(1, int(config['num_epochs']) + 1):\n",
    "        l = train_one_epoch(model, criterion, opt, dl, device=DEVICE)\n",
    "        print(f'Epoch: {epoch:3d}| Train loss: {l:.5f}')\n",
    "        # Evaluate next-item ranking on validation (prefix->valid_item)\n",
    "        _was = model.training\n",
    "        model.eval()\n",
    "        try:\n",
    "            _rec, _ndcg = recall_ndcg_at_k(\n",
    "                model, user_valid_full, num_item, config['max_len'],\n",
    "                k=K, batch_size=2048, device=DEVICE\n",
    "            )\n",
    "        finally:\n",
    "            if _was: model.train()\n",
    "        print(f\"[VAL] Recall@{K}={_rec:.4f} | NDCG@{K}={_ndcg:.4f}\")\n",
    "    torch.save(model.state_dict(), \"baseline_best.pt\")\n",
    "    print(\"Saved baseline_best.pt\")\n",
    "    return mds, user_train, user_valid, user_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embedding_from_model(model, seq, num_items, max_len):\n",
    "    # Average of valid hidden states\n",
    "    s = seq[-max_len:] if len(seq) > max_len else seq\n",
    "    PAD = 0; MASK = num_items + 2\n",
    "    pad = max_len - len(s)\n",
    "    x = torch.tensor([[PAD]*pad + s], dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model.forward_hidden(x)[0]  # (L, H)\n",
    "        valid = h[pad:pad+len(s)]\n",
    "        return valid.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "def compute_user_embeddings(model, train_seqs, num_items, max_len):\n",
    "    user_vecs = {}\n",
    "    for u, seq in tqdm(list(train_seqs.items()), desc=\"user embeddings\"):\n",
    "        user_vecs[u] = user_embedding_from_model(model, seq, num_items, max_len)\n",
    "    U = np.stack([user_vecs[u] for u in user_vecs.keys()])\n",
    "    user_index = list(user_vecs.keys())\n",
    "    return U, user_index\n",
    "\n",
    "def build_collective_quota(seed_cluster, labels, users, size, p, *, seed=None):\n",
    "    import random\n",
    "    rnd = random.Random(seed) if seed is not None else random\n",
    "\n",
    "    # split pools\n",
    "    seed_users  = [u for u, lab in zip(users, labels) if lab == seed_cluster]\n",
    "    other_users = [u for u, lab in zip(users, labels) if lab != seed_cluster]\n",
    "    rnd.shuffle(seed_users); rnd.shuffle(other_users)\n",
    "\n",
    "    # exact target counts\n",
    "    n_seed  = min(len(seed_users), int(round(p * size)))\n",
    "    n_other = min(len(other_users), size - n_seed)\n",
    "\n",
    "    members = seed_users[:n_seed] + other_users[:n_other]\n",
    "\n",
    "    # backfill if one pool was too small\n",
    "    if len(members) < size:\n",
    "        spill = seed_users[n_seed:] + other_users[n_other:]\n",
    "        rnd.shuffle(spill)\n",
    "        members += spill[: size - len(members)]\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "def top_items_for_collective(members, train_seqs, topn=10):\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for u in members:\n",
    "        for it in set(train_seqs.get(u, [])):  # count each item once per user\n",
    "            c[it] += 1\n",
    "    return [it for it, _ in c.most_common(topn)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: resolve number of items robustly from mds/model\n",
    "def resolve_num_items(mds, model):\n",
    "    # Try common dataset attributes first\n",
    "    for attr in ('num_items', 'n_items', 'item_vocab_size'):\n",
    "        if hasattr(mds, attr):\n",
    "            try:\n",
    "                n = int(getattr(mds, attr))\n",
    "                if n > 0:\n",
    "                    return n\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Try encoder sizes\n",
    "    for attr in ('item_encoder', 'item2id', 'item_to_idx'):\n",
    "        enc = getattr(mds, attr, None)\n",
    "        if isinstance(enc, dict) and len(enc) > 0:\n",
    "            return int(len(enc))\n",
    "    # Try model output head (often vocab size = num_items + specials)\n",
    "    out_features = getattr(getattr(model, 'out', None), 'out_features', None)\n",
    "    if isinstance(out_features, int) and out_features > 0:\n",
    "        # Heuristic: BERT4Rec-style uses tokens: 0=PAD, +1=[CLS], +2=[MASK]\n",
    "        # So num_items ≈ out_features - 3\n",
    "        return max(1, int(out_features) - 3)\n",
    "    raise RuntimeError(\"Could not resolve number of items from mds/model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877dc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_from_baseline_on_ratings(\n",
    "    baseline_state_path,\n",
    "    ratings_df,\n",
    "    mds,\n",
    "    epochs=8,\n",
    "    lr=5e-4,\n",
    "    *,\n",
    "    eval_specs=None,   # list of {\"name\",\"members_idx\",\"item_set_enc\",\"baseline_hr\", [\"solo_hr\"]}\n",
    "    eval_split='user_test',\n",
    "    k=None,\n",
    "    device=None,\n",
    "    **kwargs           # swallow unused args for compatibility\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrain from baseline weights on a modified ratings_df and print, per epoch:\n",
    "      - training loss\n",
    "      - HR@K for each eval spec\n",
    "      - relative HR (vs baseline_hr)\n",
    "      - constructiveness CT = rel_joint - rel_solo (if solo_hr provided)\n",
    "\n",
    "    Returns: (mds, user_pos_mod, None, None, model)\n",
    "    \"\"\"\n",
    "    dev = device if device is not None else DEVICE\n",
    "    K = k if k is not None else (config.get('TOPK', 10) if isinstance(config, dict) else 10)\n",
    "    # Paper protocol: evaluate HR over all users in chosen split\n",
    "    global_eval_subset = None\n",
    "    try:\n",
    "        global_eval_subset = getattr(mds, eval_split)\n",
    "    except Exception:\n",
    "        global_eval_subset = None\n",
    "\n",
    "\n",
    "    # 0) Defensive config defaults\n",
    "    max_len = int(config.get('max_len', 50))\n",
    "    mask_prob = float(config.get('mask_prob', 0.15))\n",
    "    batch_size = int(config.get('batch_size', 128))\n",
    "\n",
    "    # 1) Rebuild sequences from edited ratings with threshold=4.0\n",
    "    user_pos_mod = rebuild_sequences_from_df(\n",
    "        ratings_df,\n",
    "        mds.item_encoder,\n",
    "        mds.user_encoder,\n",
    "        threshold=4.0\n",
    "    )\n",
    "    user_pos_mod = {u: [int(x) for x in seq] for u, seq in user_pos_mod.items()}\n",
    "    user_pos_mod = {u: seq for u, seq in user_pos_mod.items() if len(seq) > 0}\n",
    "    if not user_pos_mod:\n",
    "        raise ValueError(\"No user sequences after rebuild; check ratings_df and threshold.\")\n",
    "\n",
    "    # 1b) Compute num_item to cover all observed ids\n",
    "    enc_num_item = getattr(mds, 'num_item', len(getattr(mds, 'item_encoder', {})))\n",
    "    max_seen = max(max(seq) for seq in user_pos_mod.values())\n",
    "    num_item = max(enc_num_item, max_seen)\n",
    "\n",
    "    # 2) DataLoader\n",
    "    ds = BERTRecDataSet(\n",
    "        user_pos_mod,\n",
    "        max_len=max_len,\n",
    "        num_item=num_item,\n",
    "        mask_prob=mask_prob\n",
    "    )\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    # 3) Model from baseline checkpoint\n",
    "    model = BERT(\n",
    "        bert_max_len=max_len,\n",
    "        num_items=num_item + 2,   # PAD=0 plus [CLS]/[MASK]\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(dev)\n",
    "\n",
    "    # ----- Vocab-size–aware load: pad/slice embedding & output heads -----\n",
    "    ckpt = torch.load(baseline_state_path, map_location=dev)\n",
    "    model_sd = model.state_dict()\n",
    "\n",
    "    def _resize_like_param(param_name, src_tensor, dst_tensor):\n",
    "        \"\"\"Resize src_tensor to match dst_tensor on dim 0 by copy-overlap + zero pad.\"\"\"\n",
    "        if src_tensor.shape == dst_tensor.shape:\n",
    "            return src_tensor\n",
    "        # Only support resizing first dimension (vocab) and keeping others\n",
    "        if src_tensor.ndim != dst_tensor.ndim or src_tensor.shape[1:] != dst_tensor.shape[1:]:\n",
    "            # Fallback: keep destination (random init) if shapes are incompatible\n",
    "            return dst_tensor\n",
    "        new_t = dst_tensor.clone()\n",
    "        n = min(src_tensor.shape[0], dst_tensor.shape[0])\n",
    "        new_t[:n] = src_tensor[:n]\n",
    "        return new_t\n",
    "\n",
    "    # Candidate keys for vocab-tied layers across common BERT4Rec variants\n",
    "    vocab_keys = [\n",
    "        'embedding.token.weight',          # seen in your error\n",
    "        'bert.item_embedding.weight',\n",
    "        'item_embedding.weight',\n",
    "        'bert.embeddings.word_embeddings.weight',\n",
    "    ]\n",
    "    out_w_keys = [\n",
    "        'out.weight',                      # seen in your error\n",
    "        'bert.prediction.weight',\n",
    "        'prediction.weight',\n",
    "    ]\n",
    "    out_b_keys = [\n",
    "        'out.bias',\n",
    "        'bert.prediction.bias',\n",
    "        'prediction.bias',\n",
    "    ]\n",
    "\n",
    "    # Build a patched checkpoint dict\n",
    "    patched = {}\n",
    "    for k, v in ckpt.items():\n",
    "        if k in model_sd:\n",
    "            if k in vocab_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_w_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_b_keys:\n",
    "                # Bias: resize on dim 0\n",
    "                if v.shape != model_sd[k].shape:\n",
    "                    nb = min(v.shape[0], model_sd[k].shape[0])\n",
    "                    new_b = model_sd[k].clone()\n",
    "                    new_b[:nb] = v[:nb]\n",
    "                    patched[k] = new_b\n",
    "                else:\n",
    "                    patched[k] = v\n",
    "            else:\n",
    "                # default: keep as-is if shape matches; otherwise keep destination param\n",
    "                if v.shape == model_sd[k].shape:\n",
    "                    patched[k] = v\n",
    "                else:\n",
    "                    patched[k] = model_sd[k]\n",
    "        else:\n",
    "            # keys not in current model are ignored\n",
    "            pass\n",
    "\n",
    "    # Load with strict=False to allow any unmatched keys\n",
    "    missing, unexpected = model.load_state_dict(patched, strict=False)\n",
    "    if missing:\n",
    "        print(f\"[info] Missing keys after load (expected if heads/embeddings expanded): {missing}\")\n",
    "    if unexpected:\n",
    "        print(f\"[info] Unexpected keys ignored from checkpoint: {unexpected}\")\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "    grad_clip = float(config.get('grad_clip', 1.0))\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    def _train_one_epoch_nan_safe(model, crit, opt, dl):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for batch in dl:\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: (v.to(dev) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
    "                inputs = batch.get('input_ids') or batch.get('seqs') or next(iter(batch.values()))\n",
    "                labels = batch.get('labels') or batch.get('targets') or batch.get('target_ids')\n",
    "                attn = batch.get('attention_mask') or batch.get('attn_mask')\n",
    "            else:\n",
    "                inputs = batch[0].to(dev)\n",
    "                labels = batch[1].to(dev)\n",
    "                attn = batch[2].to(dev) if len(batch) > 2 else None\n",
    "\n",
    "            if labels is None:\n",
    "                continue\n",
    "            valid = (labels != 0)\n",
    "            if valid.sum().item() == 0:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(inputs)  # (B, T, V)\n",
    "            B, T, V = logits.shape\n",
    "            loss = crit(logits.view(B*T, V), labels.view(B*T))\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * valid.sum().item()\n",
    "            total_tokens += valid.sum().item()\n",
    "\n",
    "        return (total_loss / max(total_tokens, 1))\n",
    "\n",
    "    def _coerce_itemset(item_set_enc, target_num_item):\n",
    "        if isinstance(item_set_enc, (list, tuple)):\n",
    "            v = torch.tensor(item_set_enc, dtype=torch.float32)\n",
    "        elif torch.is_tensor(item_set_enc):\n",
    "            v = item_set_enc.float().cpu()\n",
    "        else:\n",
    "            raise TypeError(\"item_set_enc must be list/tuple/tensor\")\n",
    "        if v.ndim != 1:\n",
    "            v = v.view(-1)\n",
    "        if len(v) < target_num_item:\n",
    "            v = torch.cat([v, torch.zeros(target_num_item - len(v))], dim=0)\n",
    "        elif len(v) > target_num_item:\n",
    "            v = v[:target_num_item]\n",
    "        return v.to(dev)\n",
    "\n",
    "    def _eval_one(spec):\n",
    "        members = spec['members_idx']\n",
    "        subset = global_eval_subset if global_eval_subset else user_pos_mod\n",
    "        aligned_itemset = _coerce_itemset(spec['item_set_enc'], num_item)\n",
    "\n",
    "        hr = float(hr_for_itemset(\n",
    "            model,\n",
    "            subset,\n",
    "            aligned_itemset,\n",
    "            num_item,\n",
    "            max_len,\n",
    "            K\n",
    "        ))\n",
    "\n",
    "        rel = None\n",
    "        base = spec.get('baseline_hr', None)\n",
    "        if base is not None and base > 0:\n",
    "            rel = hr / base\n",
    "        ct = None\n",
    "        solo = spec.get('solo_hr', None)\n",
    "        if base is not None and base > 0 and solo is not None:\n",
    "            ct = (hr / base) - (solo / base)\n",
    "        return hr, rel, ct\n",
    "\n",
    "    \n",
    "    # --- Collect per-epoch evaluation stats to average at the end ---\n",
    "    _per_spec = {}\n",
    "    if eval_specs:\n",
    "        for _spec in eval_specs:\n",
    "            _nm = _spec.get('name', f\"spec_{len(_per_spec)+1}\")\n",
    "            _per_spec[_nm] = {'hr': [], 'rel': [], 'ct': []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_loss = _train_one_epoch_nan_safe(model, crit, opt, dl)\n",
    "        if eval_specs:\n",
    "            parts = []\n",
    "            for spec in eval_specs:\n",
    "                try:\n",
    "                    hr, rel, ct = _eval_one(spec)\n",
    "                    # record per-epoch stats\n",
    "                    _nm = spec.get('name', '?')\n",
    "                    if _nm in _per_spec:\n",
    "                        _per_spec[_nm]['hr'].append(hr)\n",
    "                        _per_spec[_nm]['rel'].append(rel)\n",
    "                        _per_spec[_nm]['ct'].append(ct)\n",
    "                    msg = f\"{spec['name']}: HR@{K}={hr:.4f}\"\n",
    "                    base_local = spec.get('baseline_hr', None)\n",
    "                    if rel is not None:\n",
    "                        msg += f\" | rel={rel:.3f}\"\n",
    "                    elif base_local is not None and base_local == 0:\n",
    "                        msg += \" | rel=NA (baseline=0)\"\n",
    "                    if ct is not None:\n",
    "                        msg += f\" | CT={ct:.3f}\"\n",
    "                except Exception as e:\n",
    "                    msg = f\"{spec.get('name','?')}: EVAL-ERROR {type(e).__name__}: {e}\"\n",
    "                parts.append(msg)\n",
    "            print(f\"Epoch: {epoch:3d} | loss: {avg_loss:.5f} || \" + \" || \".join(parts), flush=True)\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch:3d} | Train loss: {avg_loss:.5f}\", flush=True)\n",
    "\n",
    "    \n",
    "    # --- Compute averages across epochs and attach to model ---\n",
    "    def _mean_ignore_none(xs):\n",
    "        _vals = [x for x in xs if x is not None]\n",
    "        return float(sum(_vals)/len(_vals)) if _vals else None\n",
    "    avg_stats = {}\n",
    "    if eval_specs:\n",
    "        for _nm, d in _per_spec.items():\n",
    "            avg_stats[_nm] = {\n",
    "                'hr': _mean_ignore_none(d['hr']),\n",
    "                'rel': _mean_ignore_none(d['rel']),\n",
    "                'ct': _mean_ignore_none(d['ct']),\n",
    "                'epochs': len(d['hr'])\n",
    "            }\n",
    "    try:\n",
    "        model._avg_eval_stats = avg_stats\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return mds, user_pos_mod, None, None, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def hr_for_itemset(model, user_pos, item_set, num_items, max_len, k=10):\n",
    "    PAD = 0\n",
    "    V = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                V = int(m.num_embeddings); break\n",
    "            except Exception:\n",
    "                pass\n",
    "    MASK = 1 if (V is not None and V > 1) else (num_items + 2)\n",
    "    if V is not None and not (0 <= MASK < V):\n",
    "        MASK = 1 if V > 1 else 0\n",
    "\n",
    "    dev = next(model.parameters()).device\n",
    "    inps, seen_lists = [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if not seq: continue\n",
    "        cond = seq[:-2] if len(seq) >= 2 else seq[:1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1: s = s[-(max_len - 1):]\n",
    "        if V is not None:\n",
    "            s = [int(t) for t in s if 0 <= int(t) < V]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        inps.append([PAD]*pad + list(s) + [MASK])\n",
    "        seen_lists.append(list(set(s)))\n",
    "    if not inps: return 0.0\n",
    "\n",
    "    X = torch.tensor(inps, dtype=torch.long, device=dev)\n",
    "    B = X.shape[0]\n",
    "\n",
    "    if V is None:\n",
    "        V = model(X[:1])[:, -1, :].shape[1]\n",
    "\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    if 0 <= PAD < V: seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V: seen_mask[:, MASK] = True\n",
    "\n",
    "    row_idx, col_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        valid = [it for it in items if 0 <= it < V]\n",
    "        if valid:\n",
    "            row_idx.extend([i]*len(valid))\n",
    "            col_idx.extend(valid)\n",
    "    if row_idx:\n",
    "        seen_mask[torch.tensor(row_idx, device=dev), torch.tensor(col_idx, device=dev)] = True\n",
    "\n",
    "    target_mask = torch.zeros(V, dtype=torch.bool, device=dev)\n",
    "    cand = list(set(int(i) for i in item_set))\n",
    "    cand = [i for i in cand if 0 <= i < V]\n",
    "    if cand:\n",
    "        target_mask[torch.tensor(cand, device=dev)] = True\n",
    "    if 0 <= PAD < V: target_mask[PAD] = False\n",
    "    if 0 <= MASK < V: target_mask[MASK] = False\n",
    "\n",
    "    hits = 0; total = 0\n",
    "    batch_size = 2048\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(B, start + batch_size)\n",
    "        Xb = X[start:end]\n",
    "        logits = model(Xb)[:, -1, :]\n",
    "        if isinstance(num_items, int) and num_items < logits.shape[1]:\n",
    "            logits[:, num_items:] = -1e9\n",
    "        logits = logits.masked_fill(seen_mask[start:end], -1e9)\n",
    "        topk_idx = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit_rows = target_mask[topk_idx].any(dim=1)\n",
    "        hits += hit_rows.sum().item()\n",
    "        total += Xb.shape[0]\n",
    "    return float(hits / max(1, total))\n",
    "\n",
    "@torch.no_grad()\n",
    "def relative_hr(model_variant, model_baseline, user_pos, item_set, num_items, max_len, k=10):\n",
    "    g_base = hr_for_itemset(model_baseline, user_pos, item_set, num_items, max_len, k=k)\n",
    "    g_var  = hr_for_itemset(model_variant,   user_pos, item_set, num_items, max_len, k=k)\n",
    "    rel    = (g_var / g_base) if g_base > 0 else 0.0\n",
    "    return rel, g_var, g_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_next_items(model, cond_seq, num_items, max_len):\n",
    "    \"\"\"\n",
    "    Robust scorer that:\n",
    "    - Detects the model's embedding vocab size by scanning modules for nn.Embedding.\n",
    "    - Ensures all token ids are within [0, vocab_size).\n",
    "    - Uses PAD=0 and MASK=1 when possible (common BERT4Rec scheme).\n",
    "    - Suppresses PAD, MASK, and seen tokens in logits.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 1) Find an embedding to infer vocab_size\n",
    "    vocab_size = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                vocab_size = int(m.num_embeddings)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    PAD = 0\n",
    "    MASK = 1 if vocab_size is not None and vocab_size > 1 else (num_items + 2)\n",
    "\n",
    "    # 2) Build masked sequence with strict truncation\n",
    "    s = cond_seq[-max_len:] if len(cond_seq) > max_len else cond_seq\n",
    "    if len(s) > max_len - 1:\n",
    "        s = s[-(max_len - 1):]\n",
    "\n",
    "    # 3) Sanitize IDs to prevent CUDA device-side asserts\n",
    "    if vocab_size is not None:\n",
    "        s = [int(x) for x in s if 0 <= int(x) < vocab_size]\n",
    "        # Final check for MASK validity\n",
    "        if not (0 <= MASK < vocab_size):\n",
    "            MASK = 1 if vocab_size > 1 else 0\n",
    "\n",
    "    pad = max_len - len(s) - 1\n",
    "    if pad < 0: pad = 0\n",
    "    seq_ids = [PAD]*pad + s + [MASK]\n",
    "\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    inp = torch.tensor([seq_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # 4) Forward -> logits for the MASK position\n",
    "    out = model(inp)\n",
    "    logits = out[0, -1].clone()\n",
    "\n",
    "    # 5) Suppress invalid or special predictions\n",
    "    if 0 <= PAD < logits.numel():   logits[PAD] = -1e9\n",
    "    if 0 <= MASK < logits.numel():  logits[MASK] = -1e9\n",
    "    for seen in set(s):\n",
    "        if 0 <= seen < logits.numel():\n",
    "            logits[seen] = -1e9\n",
    "\n",
    "    # 6) If num_items provided < logits size, clip tail so metrics only consider real items\n",
    "    if isinstance(num_items, int) and num_items < logits.numel():\n",
    "        logits[num_items:] = -1e9\n",
    "\n",
    "    return logits  # (vocab,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d11c",
   "metadata": {},
   "source": [
    "# ✅ Paper-faithful item_set builders (post-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "V_TARGET = 10 if 'config' not in globals() else config.get('V', 10)\n",
    "\n",
    "def _member_mean_topV(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    if sub.empty:\n",
    "        return []\n",
    "    means = (sub.groupby('movieId')['rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(V).index.tolist())\n",
    "    return means\n",
    "\n",
    "def _member_popular(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    pop = (sub.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _global_popular(ratings_df, V):\n",
    "    pop = (ratings_df.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _encode_items(raw_ids, item_encoder):\n",
    "    enc0 = [item_encoder[r] for r in raw_ids if r in item_encoder]\n",
    "    return [e + 1 for e in enc0]  # BERT4Rec label space\n",
    "\n",
    "def build_item_set_for_members(mds, members_idx, V=10):\n",
    "    ratings_df = mds.df\n",
    "    user_decoder = mds.user_decoder\n",
    "    member_raw = [user_decoder[u] for u in members_idx if u in user_decoder]\n",
    "\n",
    "    raw = _member_mean_topV(ratings_df, member_raw, V)\n",
    "    if len(raw) < V:\n",
    "        for iid in _member_popular(ratings_df, member_raw, V*3):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "    if len(raw) < V:\n",
    "        for iid in _global_popular(ratings_df, V*5):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "\n",
    "    enc = _encode_items(raw[:V], mds.item_encoder)\n",
    "    return raw[:V], enc[:V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a015104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _cluster_centroids(U, labels, Q):\n",
    "    \"\"\"Return (Q x d) centroid matrix in user-embedding space.\"\"\"\n",
    "    centroids = []\n",
    "    for q in range(Q):\n",
    "        idx = np.where(labels == q)[0]\n",
    "        # KMeans guarantees non-empty clusters; just in case:\n",
    "        if len(idx) == 0:\n",
    "            # fallback: random user as centroid\n",
    "            centroids.append(U[np.random.randint(len(U))])\n",
    "        else:\n",
    "            centroids.append(U[idx].mean(axis=0))\n",
    "    C = np.vstack(centroids)\n",
    "    return C\n",
    "\n",
    "def _normalize_rows(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def _farthest_pair_indices(centroids, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Return indices (i, j) of the two centroids that are maximally distant.\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        Cn = _normalize_rows(centroids)\n",
    "        # cosine distance = 1 - cosine similarity\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        # Euclidean\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    # ignore diagonal\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "    ij = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "    return int(ij[0]), int(ij[1]), float(dists[ij])\n",
    "\n",
    "\n",
    "def _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.25):\n",
    "    \"\"\"\n",
    "    Pick two clusters that are far apart but balanced in mean item popularity.\n",
    "    balance_tol: allowable relative difference in mean popularity (e.g., 0.25 = 25%).\n",
    "    Returns (i, j, distance).\n",
    "    \"\"\"\n",
    "    # Popularity of each item from training data\n",
    "    from collections import Counter\n",
    "    cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "\n",
    "    # Mean popularity per cluster (average of users' mean item popularity)\n",
    "    cluster_pop = {}\n",
    "    Q = len(centroids)\n",
    "    for q in range(Q):\n",
    "        user_idxs = [k for k, lab in enumerate(labels) if lab == q]\n",
    "        pops = []\n",
    "        for k in user_idxs:\n",
    "            u = user_index[k]\n",
    "            seq = mds.user_train.get(u, [])\n",
    "            if seq:\n",
    "                pops.append(float(np.mean([cnt[i] for i in seq])))\n",
    "        cluster_pop[q] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "\n",
    "    # Pairwise distances between centroids\n",
    "    if metric == \"cosine\":\n",
    "        Cn = centroids / (np.linalg.norm(centroids, axis=1, keepdims=True) + 1e-12)\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "\n",
    "    # Choose farthest pair among those with similar popularity\n",
    "    best_pair, best_dist = None, -np.inf\n",
    "    for i in range(Q):\n",
    "        for j in range(i+1, Q):\n",
    "            pa, pb = cluster_pop.get(i, 0.0), cluster_pop.get(j, 0.0)\n",
    "            if pa == 0.0 or pb == 0.0:\n",
    "                continue\n",
    "            rel_diff = abs(pa - pb) / max(pa, pb)\n",
    "            if rel_diff <= balance_tol:\n",
    "                d = dists[i, j]\n",
    "                if d > best_dist:\n",
    "                    best_dist = d\n",
    "                    best_pair = (i, j)\n",
    "\n",
    "    if best_pair is None:\n",
    "        # Fallback to farthest regardless of balance\n",
    "        i, j = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "        return int(i), int(j), float(dists[i, j])\n",
    "    return int(best_pair[0]), int(best_pair[1]), float(best_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c383f8",
   "metadata": {},
   "source": [
    "# 🚀 RQ2 Experiment (baseline → collectives → item_set → interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89661ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m\\ratings.dat\n",
      "Users with sequences: 6040 | Items: 3706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1| Train loss: 7.86362\n",
      "[VAL] Recall@10=0.0301 | NDCG@10=0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   2| Train loss: 7.14407\n",
      "[VAL] Recall@10=0.0300 | NDCG@10=0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   3| Train loss: 6.72533\n",
      "[VAL] Recall@10=0.0399 | NDCG@10=0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   4| Train loss: 6.23599\n",
      "[VAL] Recall@10=0.0541 | NDCG@10=0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   5| Train loss: 5.75535\n",
      "[VAL] Recall@10=0.0621 | NDCG@10=0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:05<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   6| Train loss: 5.33052\n",
      "[VAL] Recall@10=0.0742 | NDCG@10=0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   7| Train loss: 4.99274\n",
      "[VAL] Recall@10=0.0714 | NDCG@10=0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   8| Train loss: 4.70466\n",
      "[VAL] Recall@10=0.0801 | NDCG@10=0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   9| Train loss: 4.45841\n",
      "[VAL] Recall@10=0.0823 | NDCG@10=0.0416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10| Train loss: 4.22730\n",
      "[VAL] Recall@10=0.0816 | NDCG@10=0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  11| Train loss: 4.01124\n",
      "[VAL] Recall@10=0.0833 | NDCG@10=0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  12| Train loss: 3.80999\n",
      "[VAL] Recall@10=0.0906 | NDCG@10=0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  13| Train loss: 3.61568\n",
      "[VAL] Recall@10=0.0881 | NDCG@10=0.0428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  14| Train loss: 3.45412\n",
      "[VAL] Recall@10=0.0891 | NDCG@10=0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  15| Train loss: 3.29249\n",
      "[VAL] Recall@10=0.0909 | NDCG@10=0.0444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  16| Train loss: 3.13714\n",
      "[VAL] Recall@10=0.0974 | NDCG@10=0.0474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:06<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  17| Train loss: 2.98054\n",
      "[VAL] Recall@10=0.1003 | NDCG@10=0.0494\n",
      "Saved baseline_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "user embeddings: 100%|██████████| 6040/6040 [00:29<00:00, 204.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[item_set] A: raw=10 enc=10  |  B: raw=10 enc=10\n",
      "0.07301324503311259\n",
      "0.11572847682119206\n",
      "Mean frequency — A: 814.4  | B: 480.0 814.4  | B: 480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.08982 || A: HR@10=0.1068 | rel=1.463\n",
      "Epoch:   2 | loss: 6.24227 || A: HR@10=0.0879 | rel=1.204\n",
      "Epoch:   3 | loss: 5.82692 || A: HR@10=0.0786 | rel=1.077\n",
      "Epoch:   4 | loss: 5.50966 || A: HR@10=0.0512 | rel=0.701\n",
      "Epoch:   5 | loss: 5.22854 || A: HR@10=0.0546 | rel=0.748\n",
      "Epoch:   6 | loss: 4.97514 || A: HR@10=0.0627 | rel=0.859\n",
      "Epoch:   7 | loss: 4.74478 || A: HR@10=0.0548 | rel=0.751\n",
      "Epoch:   8 | loss: 4.54951 || A: HR@10=0.0421 | rel=0.576\n",
      "Epoch:   9 | loss: 4.36264 || A: HR@10=0.0604 | rel=0.828\n",
      "Epoch:  10 | loss: 4.17725 || A: HR@10=0.0510 | rel=0.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.09901 || A: HR@10=0.1283 | rel=1.757\n",
      "Epoch:   2 | loss: 6.26375 || A: HR@10=0.1086 | rel=1.488\n",
      "Epoch:   3 | loss: 5.85726 || A: HR@10=0.1023 | rel=1.401\n",
      "Epoch:   4 | loss: 5.53642 || A: HR@10=0.0934 | rel=1.279\n",
      "Epoch:   5 | loss: 5.24539 || A: HR@10=0.0763 | rel=1.045\n",
      "Epoch:   6 | loss: 5.00331 || A: HR@10=0.0712 | rel=0.975\n",
      "Epoch:   7 | loss: 4.76561 || A: HR@10=0.0975 | rel=1.336\n",
      "Epoch:   8 | loss: 4.56365 || A: HR@10=0.1343 | rel=1.839\n",
      "Epoch:   9 | loss: 4.37029 || A: HR@10=0.1737 | rel=2.379\n",
      "Epoch:  10 | loss: 4.19055 || A: HR@10=0.1548 | rel=2.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.09807 || B: HR@10=0.1929 | rel=1.667\n",
      "Epoch:   2 | loss: 6.24519 || B: HR@10=0.1674 | rel=1.446\n",
      "Epoch:   3 | loss: 5.82782 || B: HR@10=0.1492 | rel=1.289\n",
      "Epoch:   4 | loss: 5.50742 || B: HR@10=0.0907 | rel=0.784\n",
      "Epoch:   5 | loss: 5.23586 || B: HR@10=0.1003 | rel=0.867\n",
      "Epoch:   6 | loss: 4.98029 || B: HR@10=0.1402 | rel=1.212\n",
      "Epoch:   7 | loss: 4.75513 || B: HR@10=0.1535 | rel=1.326\n",
      "Epoch:   8 | loss: 4.54245 || B: HR@10=0.1184 | rel=1.023\n",
      "Epoch:   9 | loss: 4.35421 || B: HR@10=0.1204 | rel=1.040\n",
      "Epoch:  10 | loss: 4.17680 || B: HR@10=0.1035 | rel=0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.10025 || B: HR@10=0.1869 | rel=1.615\n",
      "Epoch:   2 | loss: 6.24093 || B: HR@10=0.1399 | rel=1.209\n",
      "Epoch:   3 | loss: 5.82374 || B: HR@10=0.1167 | rel=1.009\n",
      "Epoch:   4 | loss: 5.50612 || B: HR@10=0.0955 | rel=0.825\n",
      "Epoch:   5 | loss: 5.22216 || B: HR@10=0.0820 | rel=0.708\n",
      "Epoch:   6 | loss: 4.97493 || B: HR@10=0.0533 | rel=0.461\n",
      "Epoch:   7 | loss: 4.73939 || B: HR@10=0.0308 | rel=0.266\n",
      "Epoch:   8 | loss: 4.53289 || B: HR@10=0.0313 | rel=0.270\n",
      "Epoch:   9 | loss: 4.32933 || B: HR@10=0.0207 | rel=0.179\n",
      "Epoch:  10 | loss: 4.15593 || B: HR@10=0.0377 | rel=0.326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.08975 || A: HR@10=0.1075 | rel=1.472 | CT=-0.090 || B: HR@10=0.2518 | rel=2.176 | CT=1.021\n",
      "Epoch:   2 | loss: 6.24589 || A: HR@10=0.0950 | rel=1.302 | CT=-0.260 || B: HR@10=0.2944 | rel=2.544 | CT=1.389\n",
      "Epoch:   3 | loss: 5.83492 || A: HR@10=0.0805 | rel=1.102 | CT=-0.460 || B: HR@10=0.2656 | rel=2.295 | CT=1.140\n",
      "Epoch:   4 | loss: 5.50931 || A: HR@10=0.0719 | rel=0.984 | CT=-0.578 || B: HR@10=0.2671 | rel=2.308 | CT=1.153\n",
      "Epoch:   5 | loss: 5.22828 || A: HR@10=0.0548 | rel=0.751 | CT=-0.811 || B: HR@10=0.2551 | rel=2.205 | CT=1.050\n",
      "Epoch:   6 | loss: 4.99153 || A: HR@10=0.0399 | rel=0.546 | CT=-1.015 || B: HR@10=0.2992 | rel=2.585 | CT=1.430\n",
      "Epoch:   7 | loss: 4.76865 || A: HR@10=0.0306 | rel=0.420 | CT=-1.142 || B: HR@10=0.3207 | rel=2.771 | CT=1.616\n",
      "Epoch:   8 | loss: 4.54664 || A: HR@10=0.0373 | rel=0.510 | CT=-1.052 || B: HR@10=0.2472 | rel=2.136 | CT=0.981\n",
      "Epoch:   9 | loss: 4.36048 || A: HR@10=0.0310 | rel=0.424 | CT=-1.138 || B: HR@10=0.2185 | rel=1.888 | CT=0.734\n",
      "Epoch:  10 | loss: 4.18233 || A: HR@10=0.0265 | rel=0.363 | CT=-1.199 || B: HR@10=0.1831 | rel=1.582 | CT=0.427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.10026 || A: HR@10=0.1310 | rel=1.794 | CT=0.903 || B: HR@10=0.1257 | rel=1.086 | CT=0.399\n",
      "Epoch:   2 | loss: 6.25796 || A: HR@10=0.1573 | rel=2.154 | CT=1.264 || B: HR@10=0.1326 | rel=1.146 | CT=0.459\n",
      "Epoch:   3 | loss: 5.83912 || A: HR@10=0.1464 | rel=2.005 | CT=1.114 || B: HR@10=0.0781 | rel=0.675 | CT=-0.012\n",
      "Epoch:   4 | loss: 5.51163 || A: HR@10=0.1320 | rel=1.807 | CT=0.917 || B: HR@10=0.1028 | rel=0.888 | CT=0.202\n",
      "Epoch:   5 | loss: 5.23218 || A: HR@10=0.1136 | rel=1.556 | CT=0.665 || B: HR@10=0.0823 | rel=0.711 | CT=0.024\n",
      "Epoch:   6 | loss: 4.97241 || A: HR@10=0.1392 | rel=1.907 | CT=1.017 || B: HR@10=0.0573 | rel=0.495 | CT=-0.192\n",
      "Epoch:   7 | loss: 4.75546 || A: HR@10=0.1843 | rel=2.524 | CT=1.633 || B: HR@10=0.0657 | rel=0.568 | CT=-0.119\n",
      "Epoch:   8 | loss: 4.54877 || A: HR@10=0.1672 | rel=2.290 | CT=1.400 || B: HR@10=0.0488 | rel=0.422 | CT=-0.265\n",
      "Epoch:   9 | loss: 4.34661 || A: HR@10=0.1652 | rel=2.263 | CT=1.373 || B: HR@10=0.0493 | rel=0.426 | CT=-0.261\n",
      "Epoch:  10 | loss: 4.17032 || A: HR@10=0.1455 | rel=1.993 | CT=1.103 || B: HR@10=0.0371 | rel=0.320 | CT=-0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.07631 || A: HR@10=0.0925 | rel=1.268 | CT=-0.294 || B: HR@10=0.1841 | rel=1.591 | CT=0.904\n",
      "Epoch:   2 | loss: 6.22793 || A: HR@10=0.0508 | rel=0.696 | CT=-0.866 || B: HR@10=0.2147 | rel=1.856 | CT=1.169\n",
      "Epoch:   3 | loss: 5.82446 || A: HR@10=0.0396 | rel=0.542 | CT=-1.020 || B: HR@10=0.1172 | rel=1.013 | CT=0.326\n",
      "Epoch:   4 | loss: 5.50001 || A: HR@10=0.0306 | rel=0.420 | CT=-1.142 || B: HR@10=0.0745 | rel=0.644 | CT=-0.043\n",
      "Epoch:   5 | loss: 5.22762 || A: HR@10=0.0278 | rel=0.381 | CT=-1.181 || B: HR@10=0.1041 | rel=0.900 | CT=0.213\n",
      "Epoch:   6 | loss: 4.96580 || A: HR@10=0.0318 | rel=0.435 | CT=-1.127 || B: HR@10=0.0800 | rel=0.691 | CT=0.004\n",
      "Epoch:   7 | loss: 4.75133 || A: HR@10=0.0401 | rel=0.549 | CT=-1.013 || B: HR@10=0.0922 | rel=0.797 | CT=0.110\n",
      "Epoch:   8 | loss: 4.52729 || A: HR@10=0.0396 | rel=0.542 | CT=-1.020 || B: HR@10=0.1311 | rel=1.133 | CT=0.446\n",
      "Epoch:   9 | loss: 4.34189 || A: HR@10=0.0397 | rel=0.544 | CT=-1.018 || B: HR@10=0.0724 | rel=0.625 | CT=-0.062\n",
      "Epoch:  10 | loss: 4.15983 || A: HR@10=0.0619 | rel=0.848 | CT=-0.714 || B: HR@10=0.0869 | rel=0.751 | CT=0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_40940\\2227248491.py:81: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.08984 || A: HR@10=0.1084 | rel=1.485 | CT=0.595 || B: HR@10=0.1679 | rel=1.451 | CT=0.296\n",
      "Epoch:   2 | loss: 6.24867 || A: HR@10=0.0863 | rel=1.181 | CT=0.291 || B: HR@10=0.1474 | rel=1.273 | CT=0.118\n",
      "Epoch:   3 | loss: 5.83085 || A: HR@10=0.1002 | rel=1.372 | CT=0.481 || B: HR@10=0.1462 | rel=1.263 | CT=0.108\n",
      "Epoch:   4 | loss: 5.51230 || A: HR@10=0.0632 | rel=0.866 | CT=-0.024 || B: HR@10=0.1427 | rel=1.233 | CT=0.078\n",
      "Epoch:   5 | loss: 5.23095 || A: HR@10=0.0434 | rel=0.594 | CT=-0.296 || B: HR@10=0.1642 | rel=1.419 | CT=0.264\n",
      "Epoch:   6 | loss: 4.98285 || A: HR@10=0.0503 | rel=0.689 | CT=-0.201 || B: HR@10=0.1358 | rel=1.173 | CT=0.018\n",
      "Epoch:   7 | loss: 4.75718 || A: HR@10=0.0588 | rel=0.805 | CT=-0.085 || B: HR@10=0.1742 | rel=1.505 | CT=0.350\n",
      "Epoch:   8 | loss: 4.54724 || A: HR@10=0.0379 | rel=0.519 | CT=-0.371 || B: HR@10=0.1887 | rel=1.631 | CT=0.476\n",
      "Epoch:   9 | loss: 4.36274 || A: HR@10=0.0464 | rel=0.635 | CT=-0.256 || B: HR@10=0.2129 | rel=1.840 | CT=0.685\n",
      "Epoch:  10 | loss: 4.17679 || A: HR@10=0.0346 | rel=0.474 | CT=-0.417 || B: HR@10=0.2369 | rel=2.047 | CT=0.892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N</th>\n",
       "      <th>p</th>\n",
       "      <th>trial</th>\n",
       "      <th>gA_base</th>\n",
       "      <th>gB_base</th>\n",
       "      <th>gA_prom</th>\n",
       "      <th>gA_demo</th>\n",
       "      <th>gB_prom</th>\n",
       "      <th>gB_demo</th>\n",
       "      <th>gA_pp</th>\n",
       "      <th>...</th>\n",
       "      <th>rA_pd</th>\n",
       "      <th>ctA_pd</th>\n",
       "      <th>rB_pd</th>\n",
       "      <th>ctB_pd</th>\n",
       "      <th>rA_dp</th>\n",
       "      <th>ctA_dp</th>\n",
       "      <th>rB_dp</th>\n",
       "      <th>ctB_dp</th>\n",
       "      <th>size_A</th>\n",
       "      <th>size_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073013</td>\n",
       "      <td>0.115728</td>\n",
       "      <td>0.11404</td>\n",
       "      <td>0.065017</td>\n",
       "      <td>0.133642</td>\n",
       "      <td>0.079487</td>\n",
       "      <td>0.057483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.622449</td>\n",
       "      <td>-0.939456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.313162</td>\n",
       "      <td>0.862132</td>\n",
       "      <td>-0.028345</td>\n",
       "      <td>1.483548</td>\n",
       "      <td>0.328755</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    N    p  trial   gA_base   gB_base  gA_prom   gA_demo   gB_prom   gB_demo  \\\n",
       "0  10  0.1      0  0.073013  0.115728  0.11404  0.065017  0.133642  0.079487   \n",
       "\n",
       "      gA_pp  ...     rA_pd    ctA_pd  rB_pd    ctB_pd     rA_dp    ctA_dp  \\\n",
       "0  0.057483  ...  0.622449 -0.939456    1.0  0.313162  0.862132 -0.028345   \n",
       "\n",
       "      rB_dp    ctB_dp size_A  size_B  \n",
       "0  1.483548  0.328755     10      10  \n",
       "\n",
       "[1 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved rq2_results_relative_hr.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Collective experiment params (from paper notebook) =====\n",
    "N_values = [10,20,50]                     # collective sizes\n",
    "p_values = [0.1,0.25,0.5,0.75,1]     # homogeneity\n",
    "trials_per_case = 20\n",
    "TOPK = 10\n",
    "V_TARGET = config.get('V', 10)\n",
    "NUM_CLUSTERS = 10\n",
    "\n",
    "scenarios = [\n",
    "    ('promote', 'promote', 'both_promote'),\n",
    "    ('demote',  'demote',  'both_demote'),\n",
    "    ('promote', 'demote',  'A_promote_B_demote'),\n",
    "    ('demote',  'promote', 'A_demote_B_promote'),\n",
    "]\n",
    "\n",
    "# --- Run baseline (or load already-trained) ---\n",
    "mds, user_train, user_valid, user_test, baseline_model = prepare_baseline(config)\n",
    "NUM_ITEMS = resolve_num_items(mds, baseline_model)\n",
    "\n",
    "U, user_index = compute_user_embeddings(baseline_model, user_train, NUM_ITEMS, config['max_len'])\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42).fit(U)\n",
    "labels = kmeans.labels_\n",
    "# Precompute centroids for the NUM_CLUSTERS user clusters\n",
    "centroids = _cluster_centroids(U, labels, NUM_CLUSTERS)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for N in N_values:\n",
    "    for p in p_values:\n",
    "\n",
    "        # ---- Build two collectives A and B ONCE per (N, p) ----\n",
    "        if SEED_MODE == \"uniform\":\n",
    "            seedA = np.random.randint(NUM_CLUSTERS)\n",
    "            seedB = (seedA + np.random.randint(1, NUM_CLUSTERS)) % NUM_CLUSTERS\n",
    "        elif SEED_MODE == \"balanced_maxdist\":\n",
    "            i, j, d = _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.25)\n",
    "            seedA, seedB = i, j\n",
    "        elif SEED_MODE == \"maxdist\":\n",
    "            i, j, d = _farthest_pair_indices(centroids, metric=\"cosine\")\n",
    "            seedA, seedB = i, j\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SEED_MODE={SEED_MODE}\")\n",
    "\n",
    "        C1 = build_collective_quota(seedA, labels, user_index, size=N, p=p, seed=42)\n",
    "        C2 = build_collective_quota(seedB, labels, user_index, size=N, p=p, seed=42)\n",
    "\n",
    "        # ---- Build fixed item sets from ORIGINAL ratings (via mds.df) ----\n",
    "        raw_A, enc_A = build_item_set_for_members(mds, C1, V=V_TARGET)\n",
    "        raw_B, enc_B = build_item_set_for_members(mds, C2, V=V_TARGET)\n",
    "        print(f\"[item_set] A: raw={len(raw_A)} enc={len(enc_A)}  |  B: raw={len(raw_B)} enc={len(enc_B)}\")\n",
    "\n",
    "        # ---- Evaluate BASELINE HR@K for each collective on its own item_set ----\n",
    "        subset_ALL = user_test  # paper: evaluate over all test users\n",
    "        g1_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_A, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        g2_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_B, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        print(g1_base)\n",
    "        print(g2_base)\n",
    "\n",
    "        from collections import Counter\n",
    "        cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "        popA = sum(cnt[i] for i in enc_A)/len(enc_A)\n",
    "        popB = sum(cnt[i] for i in enc_B)/len(enc_B)\n",
    "        print(\"Mean frequency — A:\", popA, \" | B:\", popB, popA, \" | B:\", popB )\n",
    "\n",
    "        # ---- Precompute edited dataframes ONCE per (N, p) ----\n",
    "        df0 = mds.df.copy()\n",
    "        df_A_prom = edit_ratings(df0, C1, raw_A, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_A_demo = edit_ratings(df0, C1, raw_A, action=\"demote\",  user_decoder=mds.user_decoder)\n",
    "        df_B_prom = edit_ratings(df0, C2, raw_B, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_B_demo = edit_ratings(df0, C2, raw_B, action=\"demote\",  user_decoder=mds.user_decoder)\n",
    "\n",
    "        # Helper to retrain and evaluate a single collective (averaged across epochs on TEST users)\n",
    "        def run_single(edited_df, members, item_set_enc, name, baseline_hr):\n",
    "            mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=\"baseline_best.pt\",\n",
    "                ratings_df=edited_df,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=10,\n",
    "                eval_split=\"user_test\",\n",
    "                item_set=item_set_enc,\n",
    "                eval_specs=[{\"name\": name, \"members_idx\": user_test, \"item_set_enc\": item_set_enc, \"baseline_hr\": baseline_hr}],\n",
    "                k=TOPK\n",
    "            )\n",
    "            stats = getattr(model_s, \"_avg_eval_stats\", {}) or {}\n",
    "            s = stats.get(name, {}) or {}\n",
    "            return s.get('hr'), s.get('rel'), s.get('ct')\n",
    "\n",
    "        # Two-collective interventions (apply both edits then retrain once)\n",
    "        def run_joint(df_A_action, df_B_action, solo_A, solo_B):\n",
    "            df_joint = edit_ratings(mds.df.copy(), C1, raw_A, action=df_A_action, user_decoder=mds.user_decoder)\n",
    "            df_joint = edit_ratings(df_joint,        C2, raw_B, action=df_B_action, user_decoder=mds.user_decoder)\n",
    "            mds_j, tr_j, va_j, te_j, model_j = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=\"baseline_best.pt\",\n",
    "                ratings_df=df_joint,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=10,\n",
    "                eval_split=\"user_test\",\n",
    "                eval_specs=[\n",
    "                    {\"name\": \"A\", \"members_idx\": user_test, \"item_set_enc\": enc_A, \"baseline_hr\": g1_base, \"solo_hr\": solo_A},\n",
    "                    {\"name\": \"B\", \"members_idx\": user_test, \"item_set_enc\": enc_B, \"baseline_hr\": g2_base, \"solo_hr\": solo_B},\n",
    "                ],\n",
    "                k=TOPK,\n",
    "            )\n",
    "            stats = getattr(model_j, \"_avg_eval_stats\", {}) or {}\n",
    "            A = stats.get(\"A\", {}) or {}\n",
    "            B = stats.get(\"B\", {}) or {}\n",
    "            return A.get('hr'), A.get('rel'), A.get('ct'), B.get('hr'), B.get('rel'), B.get('ct')\n",
    "\n",
    "        # ---- Trials: SAME groups, repeated trainings ----\n",
    "        for trial in range(trials_per_case):\n",
    "\n",
    "            # Solo A (promote/demote)\n",
    "            g1_demo, g1_demo_rel, g1_demo_ct = run_single(df_A_demo, C1, enc_A, 'A', g1_base)\n",
    "            g1_prom, g1_prom_rel, g1_prom_ct = run_single(df_A_prom, C1, enc_A, 'A', g1_base)\n",
    "            # Solo B\n",
    "            g2_prom, g2_prom_rel, g2_prom_ct = run_single(df_B_prom, C2, enc_B, 'B', g2_base)\n",
    "            g2_demo, g2_demo_rel, g2_demo_ct = run_single(df_B_demo, C2, enc_B, 'B', g2_base)\n",
    "\n",
    "            # Two-collective: both promote / both demote / criss-cross\n",
    "            gA_pp, gA_pp_rel, gA_pp_ct, gB_pp, gB_pp_rel, gB_pp_ct = run_joint(\"promote\", \"promote\", g1_prom, g2_prom)\n",
    "            gA_dd, gA_dd_rel, gA_dd_ct, gB_dd, gB_dd_rel, gB_dd_ct = run_joint(\"demote\", \"demote\", g1_demo, g2_demo)\n",
    "            gA_pd, gA_pd_rel, gA_pd_ct, gB_pd, gB_pd_rel, gB_pd_ct = run_joint(\"promote\", \"demote\", g1_prom, g2_demo)\n",
    "            gA_dp, gA_dp_rel, gA_dp_ct, gB_dp, gB_dp_rel, gB_dp_ct = run_joint(\"demote\", \"promote\", g1_demo, g2_prom)\n",
    "\n",
    "            # Collect metrics (relative HR vs baseline per collective)\n",
    "            def rel(now, base): \n",
    "                return (now / base) if base > 0 else float('nan')\n",
    "\n",
    "            results.append({\n",
    "                \"N\": N, \"p\": p, \"trial\": trial,\n",
    "                \"gA_base\": g1_base, \"gB_base\": g2_base,\n",
    "                \"gA_prom\": g1_prom, \"gA_demo\": g1_demo,\n",
    "                \"gB_prom\": g2_prom, \"gB_demo\": g2_demo,\n",
    "                \"gA_pp\": gA_pp, \"gB_pp\": gB_pp,\n",
    "                \"gA_dd\": gA_dd, \"gB_dd\": gB_dd,\n",
    "                \"gA_pd\": gA_pd, \"gB_pd\": gB_pd,\n",
    "                \"gA_dp\": gA_dp, \"gB_dp\": gB_dp,\n",
    "                \"rA_prom\": g1_prom_rel, \"ctA_prom\": g1_prom_ct,\n",
    "                \"rA_demo\": g1_demo_rel, \"ctA_demo\": g1_demo_ct,\n",
    "                \"rB_prom\": g2_prom_rel, \"ctB_prom\": g2_prom_ct,\n",
    "                \"rB_demo\": g2_demo_rel, \"ctB_demo\": g2_demo_ct,\n",
    "                \"rA_pp\": gA_pp_rel, \"ctA_pp\": gA_pp_ct,\n",
    "                \"rB_pp\": gB_pp_rel, \"ctB_pp\": gB_pp_ct,\n",
    "                \"rA_dd\": gA_dd_rel, \"ctA_dd\": gA_dd_ct,\n",
    "                \"rB_dd\": gB_dd_rel, \"ctB_dd\": gB_dd_ct,\n",
    "                \"rA_pd\": gA_pd_rel, \"ctA_pd\": gA_pd_ct,\n",
    "                \"rB_pd\": gB_pd_rel, \"ctB_pd\": gB_pd_ct,\n",
    "                \"rA_dp\": gA_dp_rel, \"ctA_dp\": gA_dp_ct,\n",
    "                \"rB_dp\": gB_dp_rel, \"ctB_dp\": gB_dp_ct,\n",
    "                \"size_A\": len(C1), \"size_B\": len(C2)\n",
    "            })\n",
    "\n",
    "            # free between trials\n",
    "            gc.collect()\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Save & preview\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"rq2_results_relative_hr.csv\", index=False)\n",
    "display(res_df.head())\n",
    "print(\"Saved rq2_results_relative_hr.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
