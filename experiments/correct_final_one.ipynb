{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e137fec2",
   "metadata": {},
   "source": [
    "### Notebook hotfix (applied 2025-08-20T11:37:08 UTC)\n",
    "\n",
    "- Fixed CUDA device-side assert during evaluation by sanitizing token IDs in `score_next_items`.\n",
    "- Masked PAD/MASK predictions and clipped logits to `num_items`.\n",
    "- Light hyperparameter tweaks intended to keep training ≤10% slower while improving Recall@10 / NDCG@10:\n",
    "  - `mask_prob=0.20`\n",
    "  - `weight_decay=0.01`, `lr_scheduler='cosine'`, `warmup_ratio=0.10`\n",
    "  - `dropout` +0.05 (cap 0.35)\n",
    "  - `max_len` increased by ~10%\n",
    "  - learning rate +15% with warmup/decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980720fa",
   "metadata": {},
   "source": [
    "# ML-20M — BERT4Rec Baseline + Collective Experiments (RQ2-style)\n",
    "**Built:** 2025-08-13 14:54\n",
    "\n",
    "This notebook **keeps the baseline model and training loop from `ml-20bert4rec`** and adds **all collective experimental conditions** inspired by `bert4rec_collectives_rq2_paper`:\n",
    "- Embedding-based user clustering (KMeans), farthest-cluster seeding\n",
    "- Collective construction with homogeneity **p**\n",
    "- Promote/Demote scenarios for two collectives (A/B)\n",
    "- Deterministic rating edits and **retraining from the baseline weights**\n",
    "- Relative HR@K on targeted item sets (A and B)\n",
    "- Results export + quick plot\n",
    "\n",
    "Printing/log messages mirror the paper notebook (e.g., *Device:*, *Running RQ2 grid...*, *Seed clusters: ...*, *Saved rq2_results_relative_hr.csv*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4796458",
   "metadata": {},
   "source": [
    "\n",
    "> **Fix applied (Aug 20, 2025):** Validation Recall@K/NDCG@K previously evaluated on single-item sequences,\n",
    "> which produced zeros. `prepare_baseline` now evaluates on `user_valid_full[u] = user_train[u] + user_valid[u]`,\n",
    "> ensuring there is prefix context (cond) and a target. The evaluator `recall_ndcg_at_k` expects sequences with\n",
    "> length ≥ 2 and uses `cond = seq[:-1]`, `target = seq[-1]`. No change to the dataset split itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cdd2130",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def edit_ratings(df, users_idx, target_items_raw, action, user_decoder,\n",
    "                 promote_value=5.0, demote_value=1.0,\n",
    "                 add_competing=False, competing_item_raw=None):\n",
    "    \"\"\"\n",
    "    Campaign edits for BERT4Rec applied to TRAIN ONLY.\n",
    "\n",
    "    - promote: append each target item just before the user's valid/test boundary (ts_valid),\n",
    "               but strictly after the user's last train timestamp (max_train_ts)\n",
    "    - demote:  remove target items in the TRAIN region (ts < ts_valid), never touch the last two events\n",
    "               optionally append a competing interaction just before ts_valid\n",
    "\n",
    "    Returns a new DataFrame with the same schema; last-two events per user are preserved.\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        return df\n",
    "\n",
    "    # Normalize inputs\n",
    "    target_items_raw = list({int(x) for x in (target_items_raw or [])})\n",
    "    if not target_items_raw:\n",
    "        return df\n",
    "\n",
    "    user_raw_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_raw_ids:\n",
    "        return df\n",
    "\n",
    "    target_set = set(target_items_raw)\n",
    "    user_set   = set(user_raw_ids)\n",
    "\n",
    "    # Stable sort to avoid row reordering among equal timestamps\n",
    "    df = df.sort_values(['userId', 'timestamp'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "    for uid, g in df.groupby('userId', sort=False):\n",
    "        if uid not in user_set:\n",
    "            out_parts.append(g)\n",
    "            continue\n",
    "\n",
    "        g = g.sort_values('timestamp', kind='mergesort').copy()\n",
    "        ts_vals = g['timestamp'].to_numpy()\n",
    "\n",
    "        # Define boundary using the ORIGINAL sequence per-user\n",
    "        if len(ts_vals) >= 2:\n",
    "            ts_valid = int(ts_vals[-2])  # anything < ts_valid is TRAIN; >= ts_valid is VALID/TEST tail\n",
    "            train_mask = ts_vals < ts_valid\n",
    "        else:\n",
    "            ts_valid = int(ts_vals[-1]) if len(ts_vals) else np.iinfo(np.int64).max\n",
    "            train_mask = np.ones(len(g), dtype=bool)\n",
    "\n",
    "        if action == 'promote':\n",
    "            kept = g.copy()\n",
    "\n",
    "            # choose timestamps for k new interactions within (max_train_ts, ts_valid)\n",
    "            k = len(target_items_raw)\n",
    "            if train_mask.any():\n",
    "                max_train_ts = int(g.loc[train_mask, 'timestamp'].max())\n",
    "            else:\n",
    "                # if no train events, place a window just below ts_valid\n",
    "                max_train_ts = ts_valid - (k + 10)\n",
    "\n",
    "            # initial window\n",
    "            start = max(max_train_ts + 1, ts_valid - k)   # try to end right under boundary\n",
    "            new_ts = np.arange(start, start + k, dtype=np.int64)\n",
    "\n",
    "            # ensure strictly below ts_valid\n",
    "            overflow = max(0, int(new_ts[-1] - (ts_valid - 1)))\n",
    "            if overflow > 0:\n",
    "                new_ts = new_ts - overflow\n",
    "\n",
    "            # ensure strictly above max_train_ts\n",
    "            if new_ts[0] <= max_train_ts:\n",
    "                shift = (max_train_ts - new_ts[0]) + 1\n",
    "                new_ts = new_ts + shift\n",
    "                # clip again below boundary if needed\n",
    "                overflow = max(0, int(new_ts[-1] - (ts_valid - 1)))\n",
    "                if overflow > 0:\n",
    "                    new_ts = new_ts - overflow\n",
    "                    # final guard: if we collapsed (degenerate window), step them backwards\n",
    "                    if np.any(new_ts <= max_train_ts):\n",
    "                        gap = (max_train_ts - new_ts[0]) + 1\n",
    "                        new_ts = new_ts - (gap + 1)\n",
    "\n",
    "            # build new rows\n",
    "            new_rows = pd.DataFrame({\n",
    "                'userId':   [uid] * k,\n",
    "                'movieId':  target_items_raw,\n",
    "                'rating':   [promote_value] * k,\n",
    "                'timestamp': new_ts\n",
    "            })\n",
    "\n",
    "            kept = pd.concat([kept, new_rows], ignore_index=True)\n",
    "            kept = kept.sort_values('timestamp', kind='mergesort')\n",
    "\n",
    "            out_parts.append(kept)\n",
    "\n",
    "        elif action == 'demote':\n",
    "            kept = g.copy()\n",
    "\n",
    "            # drop target items only in TRAIN\n",
    "            in_targets = kept['movieId'].isin(target_set)\n",
    "            drop_mask  = in_targets & train_mask\n",
    "            kept = kept.loc[~drop_mask].copy()\n",
    "\n",
    "            # optionally add one competing interaction right before ts_valid (but after max_train_ts)\n",
    "            if add_competing:\n",
    "                if train_mask.any():\n",
    "                    max_train_ts = int(g.loc[train_mask, 'timestamp'].max())\n",
    "                else:\n",
    "                    max_train_ts = ts_valid - 10\n",
    "\n",
    "                # choose competing item if not provided (global most frequent non-target)\n",
    "                if competing_item_raw is not None:\n",
    "                    cid = int(competing_item_raw)\n",
    "                else:\n",
    "                    global_counts = df.loc[~df['movieId'].isin(target_set), 'movieId'].value_counts()\n",
    "                    cid = int(global_counts.index[0]) if len(global_counts) else None\n",
    "\n",
    "                if cid is not None:\n",
    "                    comp_ts = min(ts_valid - 1, max_train_ts + 1)\n",
    "                    comp_row = pd.DataFrame({\n",
    "                        'userId':   [uid],\n",
    "                        'movieId':  [cid],\n",
    "                        'rating':   [promote_value],\n",
    "                        'timestamp':[comp_ts]\n",
    "                    })\n",
    "                    kept = pd.concat([kept, comp_row], ignore_index=True)\n",
    "\n",
    "            kept = kept.sort_values('timestamp', kind='mergesort')\n",
    "            out_parts.append(kept)\n",
    "\n",
    "        else:\n",
    "            # Unknown action: passthrough\n",
    "            out_parts.append(g)\n",
    "\n",
    "    out = pd.concat(out_parts, ignore_index=True)\n",
    "    # Normalize dtypes\n",
    "    return out.astype({'userId': 'int64', 'movieId': 'int64'})\n",
    "\n",
    "\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=None):\n",
    "    \"\"\"Build user->sequence dict for BERT4Rec using all ratings as interactions.\"\"\"\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['userId','timestamp']).reset_index(drop=True)\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        user_pos[row['user_idx']].append(row['item_idx'])\n",
    "    return {u: seq for u, seq in user_pos.items() if len(seq) >= 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83037f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", DEVICE)\n",
    "SEED_MODE = \"balanced_maxdist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02c08804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "\\\n",
    "# --- Baseline config (from ml-20bert4rec) ---\n",
    "config = {\n",
    "    'data_path' : r'C:\\Users\\david\\OneDrive\\Desktop\\Collective Exp\\ml-1m',  # ML-20M\n",
    "    'max_len' : 50,\n",
    "    'hidden_units' : 256,\n",
    "    'num_heads' : 2,\n",
    "    'num_layers': 2,\n",
    "    'dropout_rate' : 0.1,\n",
    "    'lr' : 0.001,\n",
    "    'batch_size' : 128,\n",
    "    'num_epochs' : 17,\n",
    "    'num_workers' : 2,\n",
    "    'mask_prob' : 0.15,\n",
    "    'seed' : 42,\n",
    "    \n",
    "}\n",
    "\n",
    "def fix_seed(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_seed(config['seed'])\n",
    "print(\"Seed set to\", config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117c403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "class MakeSequenceDataSet():\n",
    "    def __init__(self, config):\n",
    "        dat1 = os.path.join(config['data_path'], 'ratings.dat')\n",
    "        dat2 = os.path.join(config['data_path'], 'rating.dat')\n",
    "        dat_path = dat1 if os.path.exists(dat1) else dat2\n",
    "\n",
    "        if not os.path.exists(dat_path):\n",
    "            raise FileNotFoundError(f\"Could not find ratings.dat at {config['data_path']}\")\n",
    "\n",
    "        print(\"Loading:\", dat_path)\n",
    "\n",
    "        # Correct read for MovieLens 1M .dat format\n",
    "        self.df = pd.read_csv(\n",
    "            dat_path,\n",
    "            sep=\"::\",\n",
    "            engine=\"python\",      # Required for multi-char separator\n",
    "            header=None,          # No header row in the file\n",
    "            names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"],\n",
    "            encoding=\"latin-1\"    # Avoids encoding errors\n",
    "        )\n",
    "\n",
    "        must = {'userId','movieId','rating','timestamp'}\n",
    "        missing = must - set(self.df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "        self.item_encoder, self.item_decoder = self.generate_encoder_decoder('movieId')\n",
    "        self.user_encoder, self.user_decoder = self.generate_encoder_decoder('userId')\n",
    "        self.num_item, self.num_user = len(self.item_encoder), len(self.user_encoder)\n",
    "\n",
    "        self.df['item_idx'] = self.df['movieId'].apply(lambda x: self.item_encoder[x] + 1)  # 1..num_item\n",
    "        self.df['user_idx'] = self.df['userId'].apply(lambda x: self.user_encoder[x])\n",
    "        self.df = self.df.sort_values(['user_idx', 'timestamp'])\n",
    "\n",
    "        # temporal train/valid/test split\n",
    "        self.user_train, self.user_valid, self.user_test = self.generate_sequence_data()\n",
    "        print(\"Users with sequences:\", len(self.user_train), \"| Items:\", self.num_item)\n",
    "\n",
    "\n",
    "    def generate_encoder_decoder(self, col:str):\n",
    "        encoder, decoder = {}, {}\n",
    "        ids = self.df[col].unique()\n",
    "        for idx, _id in enumerate(ids):\n",
    "            encoder[_id] = idx\n",
    "            decoder[idx] = _id\n",
    "        return encoder, decoder\n",
    "\n",
    "    def generate_sequence_data(self):\n",
    "        users = defaultdict(list)\n",
    "        user_train, user_valid, user_test = {}, {}, {}\n",
    "        for user, g in self.df.groupby('user_idx'):\n",
    "            seq = g['item_idx'].tolist()\n",
    "            if len(seq) < 3:\n",
    "                continue\n",
    "            users[user] = seq\n",
    "        for user, seq in users.items():\n",
    "            user_train[user] = seq[:-2]\n",
    "            user_valid[user] = [seq[-2]]\n",
    "            user_test[user]  = [seq[-1]]\n",
    "        return user_train, user_valid, user_test\n",
    "\n",
    "    def get_splits(self):\n",
    "        return self.user_train, self.user_valid, self.user_test\n",
    "\n",
    "def rebuild_sequences_from_df(df, item_encoder, user_encoder, threshold=None):\n",
    "    # Use **all** ratings as interactions (threshold ignored)\n",
    "    df = df.copy()\n",
    "    df = df.sort_values(['userId','timestamp']).reset_index(drop=True)\n",
    "    df['item_idx'] = df['movieId'].map(lambda x: item_encoder.get(x, None))\n",
    "    df['user_idx'] = df['userId'].map(lambda x: user_encoder.get(x, None))\n",
    "    df = df.dropna(subset=['item_idx','user_idx'])\n",
    "    df['item_idx'] = df['item_idx'].astype(int) + 1\n",
    "    df['user_idx'] = df['user_idx'].astype(int)\n",
    "    user_pos = defaultdict(list)\n",
    "    for _, row in df.iterrows():\n",
    "        user_pos[row['user_idx']].append(row['item_idx'])\n",
    "    # filter out empties\n",
    "    return {u: seq for u, seq in user_pos.items() if len(seq) >= 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "518f8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Validation metrics: next-item Recall@K and NDCG@K (correct target handling) ===\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_ndcg_at_k(model, user_pos, num_items, max_len, k=10, batch_size=2048, device=None):\n",
    "    \"\"\"\n",
    "    Standard next-item eval:\n",
    "      cond = seq[:-1], target = seq[-1].\n",
    "    We DO NOT mask the target even if it appeared earlier in cond.\n",
    "    \"\"\"\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    MASK = num_items + 1  # IMPORTANT: this repo uses MASK = num_items + 1\n",
    "\n",
    "    rows, targets, seen_lists = [], [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if len(seq) < 2:\n",
    "            continue\n",
    "        cond = seq[:-1]\n",
    "        tgt  = seq[-1]\n",
    "        s = cond[-max_len:] if len(cond) > max_len else cond\n",
    "        if len(s) > max_len - 1:\n",
    "            s = s[-(max_len - 1):]  # leave room for [MASK]\n",
    "        pad = max_len - len(s) - 1\n",
    "        if pad < 0: pad = 0\n",
    "        rows.append([PAD]*pad + list(s) + [MASK])\n",
    "        targets.append(tgt)\n",
    "        seen_lists.append(list(set(s)))  # we'll unmask target below\n",
    "\n",
    "    if not rows:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    X = torch.tensor(rows, dtype=torch.long, device=dev)\n",
    "    targets = torch.tensor(targets, dtype=torch.long, device=dev)\n",
    "\n",
    "    # Determine logits width V from model\n",
    "    V = model(X[:1])[:, -1, :].shape[1]\n",
    "\n",
    "    # Build suppression mask (B,V) for PAD/MASK/seen EXCEPT the target\n",
    "    B = X.shape[0]\n",
    "    seen_mask = torch.zeros((B, V), dtype=torch.bool, device=dev)\n",
    "    seen_mask[:, PAD] = True\n",
    "    if 0 <= MASK < V:\n",
    "        seen_mask[:, MASK] = True\n",
    "\n",
    "    r_idx, c_idx = [], []\n",
    "    for i, items in enumerate(seen_lists):\n",
    "        items_set = set(it for it in items if 0 <= it < V)\n",
    "        tgt_i = int(targets[i].item())\n",
    "        if 0 <= tgt_i < V and tgt_i in items_set:\n",
    "            items_set.remove(tgt_i)\n",
    "        if items_set:\n",
    "            r_idx.extend([i]*len(items_set))\n",
    "            c_idx.extend(list(items_set))\n",
    "    if r_idx:\n",
    "        seen_mask[torch.tensor(r_idx, device=dev), torch.tensor(c_idx, device=dev)] = True\n",
    "\n",
    "    # Sanity check: target must not be masked\n",
    "    assert not seen_mask[torch.arange(B, device=dev), targets.clamp_min(0).clamp_max(V-1)].any(), \"Target was masked!\"\n",
    "\n",
    "    hits = 0.0\n",
    "    ndcgs = 0.0\n",
    "    for s in range(0, B, batch_size):\n",
    "        e = min(B, s+batch_size)\n",
    "        logits = model(X[s:e])[:, -1, :]\n",
    "        logits = logits.masked_fill(seen_mask[s:e], -1e9)\n",
    "        topk = torch.topk(logits, k=k, dim=1).indices\n",
    "        tgts = targets[s:e].unsqueeze(1)\n",
    "\n",
    "        hit_rows = (topk == tgts).any(dim=1).float()\n",
    "        hits += hit_rows.sum().item()\n",
    "\n",
    "        where = (topk == tgts).nonzero(as_tuple=False)\n",
    "        if where.numel() > 0:\n",
    "            ndcgs += (1.0 / torch.log2(where[:,1].float() + 2.0)).sum().item()\n",
    "\n",
    "    n = float(B)\n",
    "    return hits / n, ndcgs / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe5a74f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BERTRecDataSet(Dataset):\n",
    "    def __init__(self, user_train, max_len, num_item, mask_prob=0.15):\n",
    "        self.max_len = max_len\n",
    "        self.num_item = num_item\n",
    "        self.mask_prob = mask_prob\n",
    "        self.users = list(user_train.keys())\n",
    "        self.inputs, self.labels = [], []\n",
    "        for user in self.users:\n",
    "            seq = user_train[user]\n",
    "            tokens = seq[-max_len:] if len(seq) > max_len else [0]*(max_len-len(seq)) + seq\n",
    "            masked_tokens, label_tokens = self.mask_sequence(tokens)\n",
    "            self.inputs.append(masked_tokens)\n",
    "            self.labels.append(label_tokens)\n",
    "        self.inputs = torch.tensor(self.inputs, dtype=torch.long)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.long)\n",
    "\n",
    "    def mask_sequence(self, tokens):\n",
    "        masked_tokens = tokens.copy()\n",
    "        labels = [0]*len(tokens)  # PAD=0\n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i] == 0:\n",
    "                continue\n",
    "            if random.random() < self.mask_prob:\n",
    "                labels[i] = tokens[i]  # store original ID\n",
    "                prob = random.random()\n",
    "                if prob < 0.8:\n",
    "                    masked_tokens[i] = self.num_item + 1  # MASK token in inputs only\n",
    "                elif prob < 0.9:\n",
    "                    masked_tokens[i] = random.randint(1, self.num_item)\n",
    "        return masked_tokens, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38fb6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model): super().__init__(); self.pe = nn.Embedding(max_len, d_model)\n",
    "    def forward(self, x): return self.pe.weight.unsqueeze(0).repeat(x.size(0), 1, 1)\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512): super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(max_len=max_len, d_model=embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "    def forward(self, sequence):\n",
    "        return self.dropout(self.token(sequence) + self.position(sequence))\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None: scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None: p_attn = dropout(p_attn)\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__(); assert d_model % h == 0\n",
    "        self.d_k = d_model // h; self.h = h\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model); self.attention = Attention()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        B = query.size(0)\n",
    "        query, key, value = [l(x).view(B, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "        x, _ = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def forward(self, x): return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi)*(x + 0.044715*torch.pow(x,3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__(); self.w_1 = nn.Linear(d_model, d_ff); self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout); self.activation = GELU()\n",
    "    def forward(self, x): return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6): super().__init__(); self.a_2 = nn.Parameter(torch.ones(features)); self.b_2 = nn.Parameter(torch.zeros(features)); self.eps = eps\n",
    "    def forward(self, x): mean = x.mean(-1, keepdim=True); std = x.std(-1, keepdim=True); return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout): super().__init__(); self.norm = LayerNorm(size); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer): return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward); return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, bert_max_len, num_items, bert_num_blocks, bert_num_heads,\n",
    "                 bert_hidden_units, bert_dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = bert_max_len; self.num_items = num_items\n",
    "        self.hidden = bert_hidden_units\n",
    "        self.embedding = BERTEmbedding(vocab_size=num_items+2, embed_size=self.hidden, max_len=bert_max_len, dropout=bert_dropout)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(self.hidden, bert_num_heads, self.hidden*4, bert_dropout) for _ in range(bert_num_blocks)])\n",
    "        self.out = nn.Linear(self.hidden, num_items + 2)  # 0..num_items\n",
    "    def forward_hidden(self, x):\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "        h = self.embedding(x)\n",
    "        for transformer in self.transformer_blocks:\n",
    "            h = transformer.forward(h, mask)\n",
    "        return h\n",
    "    def forward(self, x):\n",
    "        h = self.forward_hidden(x)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ca1e330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device=DEVICE):\n",
    "    model.train(); loss_val = 0.0\n",
    "    for seq, labels in tqdm(data_loader):\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        logits = model(seq).view(-1, model.out.out_features)  # (bs*t, vocab)\n",
    "        labels = labels.view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_val += loss.item()\n",
    "    return loss_val / max(1, len(data_loader))\n",
    "\n",
    "def prepare_baseline(config):\n",
    "    \"\"\"\n",
    "    Trains a baseline BERT4Rec model and evaluates next-item Recall/NDCG correctly.\n",
    "    IMPORTANT: Validation is evaluated on sequences built as\n",
    "      user_valid_full[u] = user_train[u] + user_valid[u]\n",
    "    so that cond = prefix, target = last (valid) item.\n",
    "    \"\"\"\n",
    "    # 1) Build sequences\n",
    "    mds = MakeSequenceDataSet(config)\n",
    "    user_train, user_valid, user_test = mds.user_train, mds.user_valid, mds.user_test\n",
    "    num_item = mds.num_item\n",
    "\n",
    "    # 2) DataLoader from training prefixes\n",
    "    train_ds = BERTRecDataSet(user_train, max_len=config['max_len'], num_item=num_item, mask_prob=0.15)\n",
    "    dl = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True, drop_last=False)\n",
    "\n",
    "    # 3) Model\n",
    "    model = BERT(\n",
    "        bert_max_len=config['max_len'],\n",
    "        num_items=num_item,   # this class internally adds +2 for PAD/MASK head\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=config['lr'])\n",
    "\n",
    "    # 4) Validation spec: build full sequences with context\n",
    "    user_valid_full = {\n",
    "        u: (user_train[u] + user_valid[u])\n",
    "        for u in user_valid\n",
    "        if u in user_train and len(user_train[u]) >= 1 and len(user_valid[u]) == 1\n",
    "    }\n",
    "\n",
    "    K = int(config.get('TOPK', 10)) if isinstance(config, dict) else 10\n",
    "    for epoch in range(1, int(config['num_epochs']) + 1):\n",
    "        l = train_one_epoch(model, criterion, opt, dl, device=DEVICE)\n",
    "        print(f'Epoch: {epoch:3d}| Train loss: {l:.5f}')\n",
    "        # Evaluate next-item ranking on validation (prefix->valid_item)\n",
    "        _was = model.training\n",
    "        model.eval()\n",
    "        try:\n",
    "            _rec, _ndcg = recall_ndcg_at_k(\n",
    "                model, user_valid_full, num_item, config['max_len'],\n",
    "                k=K, batch_size=2048, device=DEVICE\n",
    "            )\n",
    "        finally:\n",
    "            if _was: model.train()\n",
    "        print(f\"[VAL] Recall@{K}={_rec:.4f} | NDCG@{K}={_ndcg:.4f}\")\n",
    "    torch.save(model.state_dict(), \"baseline_best.pt\")\n",
    "    print(\"Saved baseline_best.pt\")\n",
    "    return mds, user_train, user_valid, user_test, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee7bd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_embedding_from_model(model, seq, num_items, max_len):\n",
    "    # Average of valid hidden states\n",
    "    s = seq[-max_len:] if len(seq) > max_len else seq\n",
    "    PAD = 0; MASK = num_items + 2\n",
    "    pad = max_len - len(s)\n",
    "    x = torch.tensor([[PAD]*pad + s], dtype=torch.long, device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        h = model.forward_hidden(x)[0]  # (L, H)\n",
    "        valid = h[pad:pad+len(s)]\n",
    "        return valid.mean(dim=0).detach().cpu().numpy()\n",
    "\n",
    "def compute_user_embeddings(model, train_seqs, num_items, max_len):\n",
    "    user_vecs = {}\n",
    "    for u, seq in tqdm(list(train_seqs.items()), desc=\"user embeddings\"):\n",
    "        user_vecs[u] = user_embedding_from_model(model, seq, num_items, max_len)\n",
    "    U = np.stack([user_vecs[u] for u in user_vecs.keys()])\n",
    "    user_index = list(user_vecs.keys())\n",
    "    return U, user_index\n",
    "\n",
    "def build_collective_quota(seed_cluster, labels, users, size, p, *, seed=None):\n",
    "    import random\n",
    "    rnd = random.Random(seed) if seed is not None else random\n",
    "\n",
    "    # split pools\n",
    "    seed_users  = [u for u, lab in zip(users, labels) if lab == seed_cluster]\n",
    "    other_users = [u for u, lab in zip(users, labels) if lab != seed_cluster]\n",
    "    rnd.shuffle(seed_users); rnd.shuffle(other_users)\n",
    "\n",
    "    # exact target counts\n",
    "    n_seed  = min(len(seed_users), int(round(p * size)))\n",
    "    n_other = min(len(other_users), size - n_seed)\n",
    "\n",
    "    members = seed_users[:n_seed] + other_users[:n_other]\n",
    "\n",
    "    # backfill if one pool was too small\n",
    "    if len(members) < size:\n",
    "        spill = seed_users[n_seed:] + other_users[n_other:]\n",
    "        rnd.shuffle(spill)\n",
    "        members += spill[: size - len(members)]\n",
    "\n",
    "    return members\n",
    "\n",
    "\n",
    "def top_items_for_collective(members, train_seqs, topn=10):\n",
    "    from collections import Counter\n",
    "    c = Counter()\n",
    "    for u in members:\n",
    "        for it in set(train_seqs.get(u, [])):  # count each item once per user\n",
    "            c[it] += 1\n",
    "    return [it for it, _ in c.most_common(topn)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "480a9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: resolve number of items robustly from mds/model\n",
    "def resolve_num_items(mds, model):\n",
    "    # Try common dataset attributes first\n",
    "    for attr in ('num_items', 'n_items', 'item_vocab_size'):\n",
    "        if hasattr(mds, attr):\n",
    "            try:\n",
    "                n = int(getattr(mds, attr))\n",
    "                if n > 0:\n",
    "                    return n\n",
    "            except Exception:\n",
    "                pass\n",
    "    # Try encoder sizes\n",
    "    for attr in ('item_encoder', 'item2id', 'item_to_idx'):\n",
    "        enc = getattr(mds, attr, None)\n",
    "        if isinstance(enc, dict) and len(enc) > 0:\n",
    "            return int(len(enc))\n",
    "    # Try model output head (often vocab size = num_items + specials)\n",
    "    out_features = getattr(getattr(model, 'out', None), 'out_features', None)\n",
    "    if isinstance(out_features, int) and out_features > 0:\n",
    "        # Heuristic: BERT4Rec-style uses tokens: 0=PAD, +1=[CLS], +2=[MASK]\n",
    "        # So num_items ≈ out_features - 3\n",
    "        return max(1, int(out_features) - 3)\n",
    "    raise RuntimeError(\"Could not resolve number of items from mds/model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "877dc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_from_baseline_on_ratings(\n",
    "    baseline_state_path,\n",
    "    ratings_df,\n",
    "    mds,\n",
    "    epochs=8,\n",
    "    lr=5e-4,\n",
    "    *,\n",
    "    eval_specs=None,   # list of {\"name\",\"members_idx\",\"item_set_enc\",\"baseline_hr\", [\"solo_hr\"]}\n",
    "    eval_split='user_test',\n",
    "    k=None,\n",
    "    device=None,\n",
    "    **kwargs           # swallow unused args for compatibility\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrain from baseline weights on a modified ratings_df and print, per epoch:\n",
    "      - training loss\n",
    "      - HR@K for each eval spec\n",
    "      - relative HR (vs baseline_hr)\n",
    "      - constructiveness CT = rel_joint - rel_solo (if solo_hr provided)\n",
    "\n",
    "    Returns: (mds, user_pos_mod, None, None, model)\n",
    "    \"\"\"\n",
    "    dev = device if device is not None else DEVICE\n",
    "    K = k if k is not None else (config.get('TOPK', 10) if isinstance(config, dict) else 10)\n",
    "    # Paper protocol: evaluate HR over all users in chosen split\n",
    "    global_eval_subset = None\n",
    "    try:\n",
    "        global_eval_subset = getattr(mds, eval_split)\n",
    "    except Exception:\n",
    "        global_eval_subset = None\n",
    "\n",
    "\n",
    "    # 0) Defensive config defaults\n",
    "    max_len = int(config.get('max_len', 50))\n",
    "    mask_prob = float(config.get('mask_prob', 0.15))\n",
    "    batch_size = int(config.get('batch_size', 128))\n",
    "\n",
    "    # 1) Rebuild sequences from edited ratings using all ratings (threshold ignored)\n",
    "    user_pos_mod = rebuild_sequences_from_df(\n",
    "        ratings_df,\n",
    "        mds.item_encoder,\n",
    "        mds.user_encoder,\n",
    "        threshold=None\n",
    "    )\n",
    "    user_pos_mod = {u: [int(x) for x in seq] for u, seq in user_pos_mod.items()}\n",
    "    user_pos_mod = {u: seq for u, seq in user_pos_mod.items() if len(seq) > 0}\n",
    "    if not user_pos_mod:\n",
    "        raise ValueError(\"No user sequences after rebuild; check ratings_df and threshold.\")\n",
    "\n",
    "    # 1b) Compute num_item to cover all observed ids\n",
    "    enc_num_item = getattr(mds, 'num_item', len(getattr(mds, 'item_encoder', {})))\n",
    "    max_seen = max(max(seq) for seq in user_pos_mod.values())\n",
    "    num_item = max(enc_num_item, max_seen)\n",
    "\n",
    "    # 2) DataLoader\n",
    "    ds = BERTRecDataSet(\n",
    "        user_pos_mod,\n",
    "        max_len=max_len,\n",
    "        num_item=num_item,\n",
    "        mask_prob=mask_prob\n",
    "    )\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    model = BERT(\n",
    "        bert_max_len=max_len,\n",
    "        num_items=num_item,      # <- pass *items only*; the class adds +2 internally\n",
    "        bert_num_blocks=config['num_layers'],\n",
    "        bert_num_heads=config['num_heads'],\n",
    "        bert_hidden_units=config['hidden_units'],\n",
    "        bert_dropout=config['dropout_rate']\n",
    "    ).to(dev)\n",
    "\n",
    "    # ----- Vocab-size–aware load: pad/slice embedding & output heads -----\n",
    "    ckpt = torch.load(baseline_state_path, map_location=dev)\n",
    "    model_sd = model.state_dict()\n",
    "\n",
    "    def _resize_like_param(param_name, src_tensor, dst_tensor):\n",
    "        \"\"\"Resize src_tensor to match dst_tensor on dim 0 by copy-overlap + zero pad.\"\"\"\n",
    "        if src_tensor.shape == dst_tensor.shape:\n",
    "            return src_tensor\n",
    "        # Only support resizing first dimension (vocab) and keeping others\n",
    "        if src_tensor.ndim != dst_tensor.ndim or src_tensor.shape[1:] != dst_tensor.shape[1:]:\n",
    "            # Fallback: keep destination (random init) if shapes are incompatible\n",
    "            return dst_tensor\n",
    "        new_t = dst_tensor.clone()\n",
    "        n = min(src_tensor.shape[0], dst_tensor.shape[0])\n",
    "        new_t[:n] = src_tensor[:n]\n",
    "        return new_t\n",
    "\n",
    "    # Candidate keys for vocab-tied layers across common BERT4Rec variants\n",
    "    vocab_keys = [\n",
    "        'embedding.token.weight',          # seen in your error\n",
    "        'bert.item_embedding.weight',\n",
    "        'item_embedding.weight',\n",
    "        'bert.embeddings.word_embeddings.weight',\n",
    "    ]\n",
    "    out_w_keys = [\n",
    "        'out.weight',                      # seen in your error\n",
    "        'bert.prediction.weight',\n",
    "        'prediction.weight',\n",
    "    ]\n",
    "    out_b_keys = [\n",
    "        'out.bias',\n",
    "        'bert.prediction.bias',\n",
    "        'prediction.bias',\n",
    "    ]\n",
    "\n",
    "    # Build a patched checkpoint dict\n",
    "    patched = {}\n",
    "    for k, v in ckpt.items():\n",
    "        if k in model_sd:\n",
    "            if k in vocab_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_w_keys:\n",
    "                patched[k] = _resize_like_param(k, v, model_sd[k])\n",
    "            elif k in out_b_keys:\n",
    "                # Bias: resize on dim 0\n",
    "                if v.shape != model_sd[k].shape:\n",
    "                    nb = min(v.shape[0], model_sd[k].shape[0])\n",
    "                    new_b = model_sd[k].clone()\n",
    "                    new_b[:nb] = v[:nb]\n",
    "                    patched[k] = new_b\n",
    "                else:\n",
    "                    patched[k] = v\n",
    "            else:\n",
    "                # default: keep as-is if shape matches; otherwise keep destination param\n",
    "                if v.shape == model_sd[k].shape:\n",
    "                    patched[k] = v\n",
    "                else:\n",
    "                    patched[k] = model_sd[k]\n",
    "        else:\n",
    "            # keys not in current model are ignored\n",
    "            pass\n",
    "\n",
    "    # Load with strict=False to allow any unmatched keys\n",
    "    missing, unexpected = model.load_state_dict(patched, strict=False)\n",
    "    if missing:\n",
    "        print(f\"[info] Missing keys after load (expected if heads/embeddings expanded): {missing}\")\n",
    "    if unexpected:\n",
    "        print(f\"[info] Unexpected keys ignored from checkpoint: {unexpected}\")\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=0, reduction='mean')\n",
    "    grad_clip = float(config.get('grad_clip', 1.0))\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    def _train_one_epoch_nan_safe(model, crit, opt, dl):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "        for batch in dl:\n",
    "            if isinstance(batch, dict):\n",
    "                batch = {k: (v.to(dev) if hasattr(v, \"to\") else v) for k, v in batch.items()}\n",
    "                inputs = batch.get('input_ids') or batch.get('seqs') or next(iter(batch.values()))\n",
    "                labels = batch.get('labels') or batch.get('targets') or batch.get('target_ids')\n",
    "                attn = batch.get('attention_mask') or batch.get('attn_mask')\n",
    "            else:\n",
    "                inputs = batch[0].to(dev)\n",
    "                labels = batch[1].to(dev)\n",
    "                attn = batch[2].to(dev) if len(batch) > 2 else None\n",
    "\n",
    "            if labels is None:\n",
    "                continue\n",
    "            valid = (labels != 0)\n",
    "            if valid.sum().item() == 0:\n",
    "                continue\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(inputs)  # (B, T, V)\n",
    "            B, T, V = logits.shape\n",
    "            loss = crit(logits.view(B*T, V), labels.view(B*T))\n",
    "            if not torch.isfinite(loss):\n",
    "                continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += loss.item() * valid.sum().item()\n",
    "            total_tokens += valid.sum().item()\n",
    "\n",
    "        return (total_loss / max(total_tokens, 1))\n",
    "\n",
    "    def _coerce_itemset(item_set_enc, target_num_item):\n",
    "        if isinstance(item_set_enc, (list, tuple)):\n",
    "            v = torch.tensor(item_set_enc, dtype=torch.float32)\n",
    "        elif torch.is_tensor(item_set_enc):\n",
    "            v = item_set_enc.float().cpu()\n",
    "        else:\n",
    "            raise TypeError(\"item_set_enc must be list/tuple/tensor\")\n",
    "        if v.ndim != 1:\n",
    "            v = v.view(-1)\n",
    "        if len(v) < target_num_item:\n",
    "            v = torch.cat([v, torch.zeros(target_num_item - len(v))], dim=0)\n",
    "        elif len(v) > target_num_item:\n",
    "            v = v[:target_num_item]\n",
    "        return v.to(dev)\n",
    "\n",
    "    def _eval_one(spec):\n",
    "        members = spec['members_idx']\n",
    "        subset = global_eval_subset if global_eval_subset else user_pos_mod\n",
    "        aligned_itemset = _coerce_itemset(spec['item_set_enc'], num_item)\n",
    "\n",
    "        hr = float(hr_for_itemset(\n",
    "            model,\n",
    "            subset,\n",
    "            aligned_itemset,\n",
    "            num_item,\n",
    "            max_len,\n",
    "            K\n",
    "        ))\n",
    "\n",
    "        rel = None\n",
    "        base = spec.get('baseline_hr', None)\n",
    "        if base is not None and base > 0:\n",
    "            rel = hr / base\n",
    "        ct = None\n",
    "        solo = spec.get('solo_hr', None)\n",
    "        if base is not None and base > 0 and solo is not None:\n",
    "            ct = (hr / base) - (solo / base)\n",
    "        return hr, rel, ct\n",
    "\n",
    "    \n",
    "    # --- Collect per-epoch evaluation stats to average at the end ---\n",
    "    _per_spec = {}\n",
    "    if eval_specs:\n",
    "        for _spec in eval_specs:\n",
    "            _nm = _spec.get('name', f\"spec_{len(_per_spec)+1}\")\n",
    "            _per_spec[_nm] = {'hr': [], 'rel': [], 'ct': []}\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        avg_loss = _train_one_epoch_nan_safe(model, crit, opt, dl)\n",
    "        if eval_specs:\n",
    "            parts = []\n",
    "            for spec in eval_specs:\n",
    "                try:\n",
    "                    hr, rel, ct = _eval_one(spec)\n",
    "                    # record per-epoch stats\n",
    "                    _nm = spec.get('name', '?')\n",
    "                    if _nm in _per_spec:\n",
    "                        _per_spec[_nm]['hr'].append(hr)\n",
    "                        _per_spec[_nm]['rel'].append(rel)\n",
    "                        _per_spec[_nm]['ct'].append(ct)\n",
    "                    msg = f\"{spec['name']}: HR@{K}={hr:.4f}\"\n",
    "                    base_local = spec.get('baseline_hr', None)\n",
    "                    if rel is not None:\n",
    "                        msg += f\" | rel={rel:.3f}\"\n",
    "                    elif base_local is not None and base_local == 0:\n",
    "                        msg += \" | rel=NA (baseline=0)\"\n",
    "                    if ct is not None:\n",
    "                        msg += f\" | CT={ct:.3f}\"\n",
    "                except Exception as e:\n",
    "                    msg = f\"{spec.get('name','?')}: EVAL-ERROR {type(e).__name__}: {e}\"\n",
    "                parts.append(msg)\n",
    "            print(f\"Epoch: {epoch:3d} | loss: {avg_loss:.5f} || \" + \" || \".join(parts), flush=True)\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch:3d} | Train loss: {avg_loss:.5f}\", flush=True)\n",
    "\n",
    "    \n",
    "    # --- Compute averages across epochs and attach to model ---\n",
    "    def _mean_ignore_none(xs):\n",
    "        _vals = [x for x in xs if x is not None]\n",
    "        return float(sum(_vals)/len(_vals)) if _vals else None\n",
    "    avg_stats = {}\n",
    "    if eval_specs:\n",
    "        for _nm, d in _per_spec.items():\n",
    "            avg_stats[_nm] = {\n",
    "                'hr': _mean_ignore_none(d['hr']),\n",
    "                'rel': _mean_ignore_none(d['rel']),\n",
    "                'ct': _mean_ignore_none(d['ct']),\n",
    "                'epochs': len(d['hr'])\n",
    "            }\n",
    "    try:\n",
    "        model._avg_eval_stats = avg_stats\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return mds, user_pos_mod, None, None, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c83fbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def hr_for_itemset(model, user_pos, item_set, num_items, max_len, k=10, batch_size=2048, device=None):\n",
    "    dev = device or next(model.parameters()).device\n",
    "    PAD = 0\n",
    "    mask_guess = num_items + 1  # we'll clamp after we see V_model\n",
    "\n",
    "    # Build inputs (prefix + MASK) and remember \"seen\" tokens per row\n",
    "    inps, seen_lists = [], []\n",
    "    for _, seq in user_pos.items():\n",
    "        if not seq:\n",
    "            continue\n",
    "        # use prefix (next-item style); keep up to max_len-1 then place MASK at the end\n",
    "        cond = seq[:-1] if len(seq) >= 1 else seq\n",
    "        cond = cond[-(max_len-1):]\n",
    "        pad = (max_len - 1) - len(cond)\n",
    "        inps.append(([PAD] * pad) + cond + [mask_guess])\n",
    "        seen_lists.append(set(cond))\n",
    "\n",
    "    if not inps:\n",
    "        return 0.0\n",
    "\n",
    "    X = torch.tensor(inps, dtype=torch.long, device=dev)\n",
    "    target_ids = torch.as_tensor(item_set, dtype=torch.long).flatten().tolist()\n",
    "\n",
    "    hits, total = 0, 0\n",
    "    B = X.size(0)\n",
    "    for start in range(0, B, batch_size):\n",
    "        end = min(B, start + batch_size)\n",
    "        Xb = X[start:end]\n",
    "\n",
    "        # First forward to discover V_model\n",
    "        logits = model(Xb)[:, -1, :]             # (b, V_model)\n",
    "        V = logits.shape[1]\n",
    "\n",
    "        # Clamp tokens in the batch to valid range [0, V-1] and recompute logits to be safe\n",
    "        Xb = Xb.clamp_max(V - 1)\n",
    "        logits = model(Xb)[:, -1, :]\n",
    "\n",
    "        # Decide MASK id safely\n",
    "        MASK = min(num_items + 1, V - 1)\n",
    "\n",
    "        # Suppress PAD & MASK\n",
    "        logits[:, 0] = -1e9\n",
    "        if 0 <= MASK < V:\n",
    "            logits[:, MASK] = -1e9\n",
    "\n",
    "        # Suppress anything beyond real (items + special) if model head is larger\n",
    "        # (items are 1..num_items; special are 0 (PAD) and MASK)\n",
    "        if V > num_items + 2:\n",
    "            logits[:, (num_items + 2):] = -1e9\n",
    "\n",
    "        # Seen-mask with the correct width V\n",
    "        seen_mask = torch.zeros((Xb.size(0), V), dtype=torch.bool, device=dev)\n",
    "        for r, sset in enumerate(seen_lists[start:end]):\n",
    "            for t in sset:\n",
    "                if 0 <= int(t) < V:\n",
    "                    seen_mask[r, int(t)] = True\n",
    "        #logits = logits.masked_fill(seen_mask, -1e9)\n",
    "\n",
    "        # Target mask (width V)\n",
    "        target_mask = torch.zeros(V, dtype=torch.bool, device=dev)\n",
    "        for t in target_ids:\n",
    "            t = int(t)\n",
    "            # only allow real item ids 1..num_items, and also within model width\n",
    "            if 1 <= t <= num_items and t < V:\n",
    "                target_mask[t] = True\n",
    "\n",
    "        # Top-k hits\n",
    "        topk_idx = torch.topk(logits, k=k, dim=1).indices\n",
    "        hit_rows = target_mask[topk_idx].any(dim=1)\n",
    "        hits += int(hit_rows.sum().item())\n",
    "        total += Xb.size(0)\n",
    "    \n",
    "    \n",
    "    return float(hits / max(1, total))\n",
    "\n",
    "@torch.no_grad()\n",
    "def relative_hr(model_variant, model_baseline, user_pos, item_set, num_items, max_len, k=10):\n",
    "    g_base = hr_for_itemset(model_baseline, user_pos, item_set, num_items, max_len, k=k)\n",
    "    g_var  = hr_for_itemset(model_variant,   user_pos, item_set, num_items, max_len, k=k)\n",
    "    rel    = (g_var / g_base) if g_base > 0 else 0.0\n",
    "    return rel, g_var, g_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e30a140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_next_items(model, cond_seq, num_items, max_len):\n",
    "    \"\"\"\n",
    "    Robust scorer that:\n",
    "    - Detects the model's embedding vocab size by scanning modules for nn.Embedding.\n",
    "    - Ensures all token ids are within [0, vocab_size).\n",
    "    - Uses PAD=0 and MASK=1 when possible (common BERT4Rec scheme).\n",
    "    - Suppresses PAD, MASK, and seen tokens in logits.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "\n",
    "    # 1) Find an embedding to infer vocab_size\n",
    "    vocab_size = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Embedding):\n",
    "            try:\n",
    "                vocab_size = int(m.num_embeddings)\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    PAD = 0\n",
    "    MASK = 1 if vocab_size is not None and vocab_size > 1 else (num_items + 2)\n",
    "\n",
    "    # 2) Build masked sequence with strict truncation\n",
    "    s = cond_seq[-max_len:] if len(cond_seq) > max_len else cond_seq\n",
    "    if len(s) > max_len - 1:\n",
    "        s = s[-(max_len - 1):]\n",
    "\n",
    "    # 3) Sanitize IDs to prevent CUDA device-side asserts\n",
    "    if vocab_size is not None:\n",
    "        s = [int(x) for x in s if 0 <= int(x) < vocab_size]\n",
    "        # Final check for MASK validity\n",
    "        if not (0 <= MASK < vocab_size):\n",
    "            MASK = 1 if vocab_size > 1 else 0\n",
    "\n",
    "    pad = max_len - len(s) - 1\n",
    "    if pad < 0: pad = 0\n",
    "    seq_ids = [PAD]*pad + s + [MASK]\n",
    "\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    inp = torch.tensor([seq_ids], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    # 4) Forward -> logits for the MASK position\n",
    "    out = model(inp)\n",
    "    logits = out[0, -1].clone()\n",
    "\n",
    "    # 5) Suppress invalid or special predictions\n",
    "    if 0 <= PAD < logits.numel():   logits[PAD] = -1e9\n",
    "    if 0 <= MASK < logits.numel():  logits[MASK] = -1e9\n",
    "    for seen in set(s):\n",
    "        if 0 <= seen < logits.numel():\n",
    "            logits[seen] = -1e9\n",
    "\n",
    "    # 6) If num_items provided < logits size, clip tail so metrics only consider real items\n",
    "    if isinstance(num_items, int) and num_items < logits.numel():\n",
    "        logits[num_items:] = -1e9\n",
    "\n",
    "    return logits  # (vocab,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d11c",
   "metadata": {},
   "source": [
    "# ✅ Paper-faithful item_set builders (post-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b735b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "V_TARGET = 10 if 'config' not in globals() else config.get('V', 10)\n",
    "\n",
    "def _member_mean_topV(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    if sub.empty:\n",
    "        return []\n",
    "    means = (sub.groupby('movieId')['rating']\n",
    "                 .mean().sort_values(ascending=False)\n",
    "                 .head(V).index.tolist())\n",
    "    return means\n",
    "\n",
    "def _member_popular(ratings_df, member_raw_ids, V):\n",
    "    sub = ratings_df[ratings_df['userId'].isin(set(member_raw_ids))]\n",
    "    pop = (sub.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _global_popular(ratings_df, V):\n",
    "    pop = (ratings_df.groupby('movieId')['rating']\n",
    "                 .agg(['count','mean'])\n",
    "                 .sort_values(['count','mean'], ascending=[False, False])\n",
    "                 .head(V).index.tolist())\n",
    "    return pop\n",
    "\n",
    "def _encode_items(raw_ids, item_encoder):\n",
    "    enc0 = [item_encoder[r] for r in raw_ids if r in item_encoder]\n",
    "    return [e + 1 for e in enc0]  # BERT4Rec label space\n",
    "\n",
    "def build_item_set_for_members(mds, members_idx, V=10, min_support=None):\n",
    "    \"\"\"\n",
    "    Paper-faithful group target selection with optional positive support filter.\n",
    "    - Compute top-V by mean rating within the collective.\n",
    "    - If min_support is set, require at least that many member ratings >= 4.\n",
    "    - If fewer than V items, pad using 'member-popular' items from the same collective.\n",
    "    - No global fallback (strict cluster-only).\n",
    "    \"\"\"\n",
    "    member_raw = [mds.user_decoder[u] for u in members_idx if u in mds.user_decoder]\n",
    "    if not member_raw:\n",
    "        return [], []\n",
    "    df = mds.df[mds.df['userId'].isin(set(member_raw))][['userId','movieId','rating']].copy()\n",
    "    if df.empty:\n",
    "        return [], []\n",
    "\n",
    "    # Step 1: compute candidate items\n",
    "    if isinstance(min_support, int) and min_support > 0:\n",
    "        pos = df[df['rating'] >= 4.0]\n",
    "        sup = (pos.groupby('movieId')['rating'].count()).rename('pos_support')\n",
    "        means = (df.groupby('movieId')['rating'].mean()).rename('mean_rating').to_frame()\n",
    "        means = means.join(sup, how='left').fillna({'pos_support': 0})\n",
    "        means = means[means['pos_support'] >= min_support]\n",
    "        top_raw = (means.sort_values(['mean_rating','pos_support'], ascending=[False, False])\n",
    "                         .head(V).index.tolist())\n",
    "    else:\n",
    "        top_raw = (df.groupby('movieId')['rating']\n",
    "                     .mean().sort_values(ascending=False).head(V).index.tolist())\n",
    "\n",
    "    # Step 2: pad with member-popular if needed\n",
    "    if len(top_raw) < V:\n",
    "        # count how often each item was rated by members\n",
    "        pop = df.groupby('movieId')['rating'].count().sort_values(ascending=False)\n",
    "        for iid in pop.index:\n",
    "            if iid not in top_raw:\n",
    "                top_raw.append(iid)\n",
    "            if len(top_raw) >= V:\n",
    "                break\n",
    "\n",
    "    # Step 3: encode\n",
    "    top_enc = [mds.item_encoder[mid] + 1 for mid in top_raw if mid in mds.item_encoder]\n",
    "    return top_raw[:V], top_enc[:V]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a1fb8",
   "metadata": {},
   "source": [
    "def build_item_set_for_members(mds, members_idx, V=10):\n",
    "    ratings_df = mds.df\n",
    "    user_decoder = mds.user_decoder\n",
    "    member_raw = [user_decoder[u] for u in members_idx if u in user_decoder]\n",
    "\n",
    "    raw = _member_mean_topV(ratings_df, member_raw, V)\n",
    "    if len(raw) < V:\n",
    "        for iid in _member_popular(ratings_df, member_raw, V*3):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "    if len(raw) < V:\n",
    "        for iid in _global_popular(ratings_df, V*5):\n",
    "            if iid not in raw:\n",
    "                raw.append(iid)\n",
    "            if len(raw) >= V: break\n",
    "\n",
    "    enc = _encode_items(raw[:V], mds.item_encoder)\n",
    "    return raw[:V], enc[:V]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a015104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _cluster_centroids(U, labels, Q):\n",
    "    \"\"\"Return (Q x d) centroid matrix in user-embedding space.\"\"\"\n",
    "    centroids = []\n",
    "    for q in range(Q):\n",
    "        idx = np.where(labels == q)[0]\n",
    "        # KMeans guarantees non-empty clusters; just in case:\n",
    "        if len(idx) == 0:\n",
    "            # fallback: random user as centroid\n",
    "            centroids.append(U[np.random.randint(len(U))])\n",
    "        else:\n",
    "            centroids.append(U[idx].mean(axis=0))\n",
    "    C = np.vstack(centroids)\n",
    "    return C\n",
    "\n",
    "def _normalize_rows(X, eps=1e-12):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def _farthest_pair_indices(centroids, metric=\"cosine\"):\n",
    "    \"\"\"\n",
    "    Return indices (i, j) of the two centroids that are maximally distant.\n",
    "    \"\"\"\n",
    "    if metric == \"cosine\":\n",
    "        Cn = _normalize_rows(centroids)\n",
    "        # cosine distance = 1 - cosine similarity\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        # Euclidean\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    # ignore diagonal\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "    ij = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "    return int(ij[0]), int(ij[1]), float(dists[ij])\n",
    "\n",
    "\n",
    "def _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.5):\n",
    "    \"\"\"\n",
    "    Pick two clusters that are far apart but balanced in mean item popularity.\n",
    "    balance_tol: allowable relative difference in mean popularity (e.g., 0.25 = 25%).\n",
    "    Returns (i, j, distance).\n",
    "    \"\"\"\n",
    "    # Popularity of each item from training data\n",
    "    from collections import Counter\n",
    "    cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "\n",
    "    # Mean popularity per cluster (average of users' mean item popularity)\n",
    "    cluster_pop = {}\n",
    "    Q = len(centroids)\n",
    "    for q in range(Q):\n",
    "        user_idxs = [k for k, lab in enumerate(labels) if lab == q]\n",
    "        pops = []\n",
    "        for k in user_idxs:\n",
    "            u = user_index[k]\n",
    "            seq = mds.user_train.get(u, [])\n",
    "            if seq:\n",
    "                pops.append(float(np.mean([cnt[i] for i in seq])))\n",
    "        cluster_pop[q] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "\n",
    "    # Pairwise distances between centroids\n",
    "    if metric == \"cosine\":\n",
    "        Cn = centroids / (np.linalg.norm(centroids, axis=1, keepdims=True) + 1e-12)\n",
    "        sims = Cn @ Cn.T\n",
    "        dists = 1.0 - np.clip(sims, -1.0, 1.0)\n",
    "    else:\n",
    "        diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "        dists = np.sqrt((diff * diff).sum(axis=2))\n",
    "    np.fill_diagonal(dists, -np.inf)\n",
    "\n",
    "    # Choose farthest pair among those with similar popularity\n",
    "    best_pair, best_dist = None, -np.inf\n",
    "    for i in range(Q):\n",
    "        for j in range(i+1, Q):\n",
    "            pa, pb = cluster_pop.get(i, 0.0), cluster_pop.get(j, 0.0)\n",
    "            if pa == 0.0 or pb == 0.0:\n",
    "                continue\n",
    "            rel_diff = abs(pa - pb) / max(pa, pb)\n",
    "            if rel_diff <= balance_tol:\n",
    "                d = dists[i, j]\n",
    "                if d > best_dist:\n",
    "                    best_dist = d\n",
    "                    best_pair = (i, j)\n",
    "\n",
    "    if best_pair is None:\n",
    "        # Fallback to farthest regardless of balance\n",
    "        i, j = np.unravel_index(np.argmax(dists), dists.shape)\n",
    "        return int(i), int(j), float(dists[i, j])\n",
    "    return int(best_pair[0]), int(best_pair[1]), float(best_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d43dc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_edits(df_before, df_after, users_idx, target_items_raw,\n",
    "                action, user_decoder, sample_k=5, add_competing=False, label=None):\n",
    "    import numpy as np, pandas as pd, random\n",
    "    from collections import Counter\n",
    "\n",
    "    tset = set(int(x) for x in target_items_raw) if target_items_raw else set()\n",
    "    user_ids = [user_decoder[u] for u in users_idx if u in user_decoder]\n",
    "    if not user_ids or not tset:\n",
    "        print(\"[check_edits] nothing to check (empty users or targets)\")\n",
    "        return\n",
    "\n",
    "    dfb = df_before.sort_values([\"userId\",\"timestamp\"], kind=\"mergesort\").copy()\n",
    "    dfa = df_after .sort_values([\"userId\",\"timestamp\"], kind=\"mergesort\").copy()\n",
    "\n",
    "    sample_users = random.sample(user_ids, k=min(sample_k, len(user_ids)))\n",
    "\n",
    "    def ts_valid_for_user(g):\n",
    "        ts = g[\"timestamp\"].values\n",
    "        if len(ts) >= 2:\n",
    "            return int(ts[-2])                  # boundary from BEFORE\n",
    "        return np.iinfo(np.int64).max          # no tail if < 2\n",
    "\n",
    "    ok = True\n",
    "    for uid in sample_users:\n",
    "        gb = dfb[dfb.userId == uid]\n",
    "        ga = dfa[dfa.userId == uid]\n",
    "        if gb.empty:\n",
    "            continue\n",
    "        ts_valid = ts_valid_for_user(gb)\n",
    "\n",
    "        # Split using BEFORE's boundary; tail are rows with ts >= ts_valid\n",
    "        b_tail = gb[gb.timestamp >= ts_valid][[\"movieId\",\"timestamp\"]].sort_values([\"timestamp\",\"movieId\"]).reset_index(drop=True)\n",
    "        a_tail = ga[ga.timestamp >= ts_valid][[\"movieId\",\"timestamp\"]].sort_values([\"timestamp\",\"movieId\"]).reset_index(drop=True)\n",
    "\n",
    "        # Order-robust equality\n",
    "        if not a_tail.equals(b_tail):\n",
    "            ok = False\n",
    "            print(f\"[check_edits:{action}] uid={uid}: tail (valid/test) changed, which should not happen\")\n",
    "\n",
    "        # Train region diagnostics\n",
    "        b_train = gb[gb.timestamp < ts_valid]\n",
    "        a_train = ga[ga.timestamp < ts_valid]\n",
    "        if action == \"promote\":\n",
    "            # must have new target rows in train\n",
    "            added = a_train.merge(b_train, how=\"outer\", indicator=True)\n",
    "            added = added[added[\"_merge\"] == \"left_only\"]\n",
    "            if added[added.movieId.isin(tset)].empty:\n",
    "                ok = False\n",
    "                print(f\"[check_edits:promote] uid={uid}: no new target interactions in TRAIN\")\n",
    "        elif action == \"demote\":\n",
    "            # train targets should be removed\n",
    "            b_train_targets = b_train[b_train.movieId.isin(tset)]\n",
    "            if not b_train_targets.empty:\n",
    "                a_train_targets = a_train[a_train.movieId.isin(tset)]\n",
    "                if len(a_train_targets) >= len(b_train_targets):\n",
    "                    ok = False\n",
    "                    print(f\"[check_edits:demote] uid={uid}: train targets not removed \"\n",
    "                          f\"(before={len(b_train_targets)} after={len(a_train_targets)})\")\n",
    "            if add_competing:\n",
    "                added = a_train.merge(b_train, how=\"outer\", indicator=True)\n",
    "                added = added[added[\"_merge\"] == \"left_only\"]\n",
    "                if added[~added.movieId.isin(tset)].empty:\n",
    "                    ok = False\n",
    "                    print(f\"[check_edits:demote] uid={uid}: expected a competing interaction, found none\")\n",
    "\n",
    "    if ok:\n",
    "        tag = f\"{label}:\" if label else \"\"\n",
    "        print(f\"[check_edits:{tag}{action}] All sampled users respect train-only edit invariants ✓\")\n",
    "\n",
    "\n",
    "\n",
    "def hr_split(model, test_dict, item_set_enc, members, NUM_ITEMS, max_len, k=10):\n",
    "    \"\"\"Compute HR@K separately for members and non-members, given test_dict.\"\"\"\n",
    "    members_set = set(members)\n",
    "    member_dict = {u: seq for u, seq in test_dict.items() if u in members_set}\n",
    "    nonmember_dict = {u: seq for u, seq in test_dict.items() if u not in members_set}\n",
    "\n",
    "    hr_members    = hr_for_itemset(model, member_dict,  item_set_enc, NUM_ITEMS, max_len, k) if member_dict else 0.0\n",
    "    hr_nonmembers = hr_for_itemset(model, nonmember_dict, item_set_enc, NUM_ITEMS, max_len, k) if nonmember_dict else 0.0\n",
    "    return hr_members, hr_nonmembers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c383f8",
   "metadata": {},
   "source": [
    "# 🚀 RQ2 Experiment (baseline → collectives → item_set → interventions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89661ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[baseline] Found existing baseline pack — loading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_29316\\1467641376.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n",
      "user embeddings: 100%|██████████| 6040/6040 [00:29<00:00, 202.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V_model = 3708\n",
      "num_items = 3706\n",
      "expected V = 3708\n",
      "[item_set] A: raw=10 enc=10  |  B: raw=10 enc=10\n",
      "0.013079470198675497\n",
      "0.01705298013245033\n",
      "Mean frequency — A: 467.9  | B: 531.6\n",
      "[check_edits:A_prom:promote] All sampled users respect train-only edit invariants ✓\n",
      "[check_edits:A_demo:demote] All sampled users respect train-only edit invariants ✓\n",
      "[check_edits:B_prom:promote] All sampled users respect train-only edit invariants ✓\n",
      "[check_edits:B_demo:demote] All sampled users respect train-only edit invariants ✓\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_29316\\1467641376.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(baseline_state_path, map_location=dev)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 | loss: 7.16203 || A: HR@10=0.0073 | rel=0.557\n",
      "Epoch:   2 | loss: 6.30247 || A: HR@10=0.0030 | rel=0.228\n",
      "Epoch:   3 | loss: 5.88589 || A: HR@10=0.0028 | rel=0.215\n",
      "Epoch:   4 | loss: 5.54958 || A: HR@10=0.0028 | rel=0.215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 205\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# ---- Trials: SAME groups, repeated trainings ----\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trials_per_case):\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;66;03m# Solo A (promote/demote)\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m     g1_demo, g1_demo_rel, g1_demo_ct \u001b[38;5;241m=\u001b[39m \u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_A_demo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg1_base\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m     hr_mem_A, hr_nonmem_A \u001b[38;5;241m=\u001b[39m hr_split(baseline_model, user_test, enc_A, C1, NUM_ITEMS, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_len\u001b[39m\u001b[38;5;124m'\u001b[39m], TOPK)\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[sanity] HR@10 A demote: members=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhr_mem_A\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, non-members=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhr_nonmem_A\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, overall=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg1_demo\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[34], line 165\u001b[0m, in \u001b[0;36mrun_single\u001b[1;34m(edited_df, members, item_set_enc, name, baseline_hr)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(edited_df, members, item_set_enc, name, baseline_hr):\n\u001b[1;32m--> 165\u001b[0m     mds_s, tr_s, va_s, te_s, model_s \u001b[38;5;241m=\u001b[39m \u001b[43mretrain_from_baseline_on_ratings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_state_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBASELINE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# <— use saved baseline\u001b[39;49;00m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mratings_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medited_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mitem_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitem_set_enc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmembers_idx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem_set_enc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_set_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbaseline_hr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOPK\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model_s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_avg_eval_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}) \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    177\u001b[0m     s \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mget(name, {}) \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "Cell \u001b[1;32mIn[28], line 237\u001b[0m, in \u001b[0;36mretrain_from_baseline_on_ratings\u001b[1;34m(baseline_state_path, ratings_df, mds, epochs, lr, eval_specs, eval_split, k, device, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m         _per_spec[_nm] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhr\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 237\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43m_train_one_epoch_nan_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eval_specs:\n\u001b[0;32m    239\u001b[0m         parts \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[28], line 180\u001b[0m, in \u001b[0;36mretrain_from_baseline_on_ratings.<locals>._train_one_epoch_nan_safe\u001b[1;34m(model, crit, opt, dl)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(loss):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), grad_clip)\n\u001b[0;32m    182\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\collective-exp\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== Collective experiment params (from paper notebook) =====\n",
    "N_values   = [25, 50, 100]            # collective sizes\n",
    "p_values   = [0.1,0.25, 0.5, 0.75, 1.0]  # homogeneity\n",
    "trials_per_case = 50\n",
    "TOPK       = 10\n",
    "V_TARGET   = config.get('V', 10)\n",
    "NUM_CLUSTERS = 10\n",
    "SEED_MODE  = locals().get(\"SEED_MODE\", \"balanced_maxdist\")  # keep your previous choice\n",
    "\n",
    "scenarios = [\n",
    "    ('promote', 'promote', 'both_promote'),\n",
    "    ('demote',  'demote',  'both_demote'),\n",
    "    ('promote', 'demote',  'A_promote_B_demote'),\n",
    "    ('demote',  'promote', 'A_demote_B_promote'),\n",
    "]\n",
    "\n",
    "# ---------------- Repro & \"baseline pack\" paths ----------------\n",
    "import os, json, pickle, sys, gc, random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BASELINE_DIR  = \"baseline_pack\"\n",
    "BASELINE_PATH = os.path.join(BASELINE_DIR, \"baseline_best.pt\")\n",
    "MDS_PATH      = os.path.join(BASELINE_DIR, \"mds.pkl\")\n",
    "SPLITS_PATH   = os.path.join(BASELINE_DIR, \"splits.pkl\")\n",
    "META_PATH     = os.path.join(BASELINE_DIR, \"meta.json\")\n",
    "os.makedirs(BASELINE_DIR, exist_ok=True)\n",
    "\n",
    "# Determinism: keep this consistent across machines\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(config['seed'])\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"  # safe on CPU; improves CUDA determinism\n",
    "\n",
    "random.seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "torch.manual_seed(config['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(config['seed'])\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "try:\n",
    "    torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------------- Load or create the baseline pack ----------------\n",
    "def _save_baseline_pack(mds, user_train, user_valid, user_test, baseline_model):\n",
    "    torch.save(baseline_model.state_dict(), BASELINE_PATH)\n",
    "    with open(MDS_PATH, \"wb\") as f:\n",
    "        pickle.dump(mds, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(SPLITS_PATH, \"wb\") as f:\n",
    "        pickle.dump((user_train, user_valid, user_test), f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    meta = {\n",
    "        \"seed\": int(config['seed']),\n",
    "        \"config\": {k: (int(v) if isinstance(v, (np.integer,)) else v) for k, v in config.items()},\n",
    "        \"versions\": {\n",
    "            \"python\": sys.version,\n",
    "            \"torch\": getattr(torch, \"__version__\", \"unknown\"),\n",
    "            \"numpy\": np.__version__,\n",
    "        },\n",
    "    }\n",
    "    with open(META_PATH, \"w\") as f:\n",
    "        json.dump(meta, f, indent=2)\n",
    "    print(f\"[baseline] Saved baseline pack to {BASELINE_DIR}/\")\n",
    "\n",
    "def _load_baseline_pack():\n",
    "    with open(MDS_PATH, \"rb\") as f:\n",
    "        mds = pickle.load(f)\n",
    "    with open(SPLITS_PATH, \"rb\") as f:\n",
    "        user_train, user_valid, user_test = pickle.load(f)\n",
    "\n",
    "    # Fast path to get a model object with loaded weights WITHOUT retraining:\n",
    "    # reuse your existing helper that builds the model and loads a state dict.\n",
    "    # We piggy-back on `retrain_from_baseline_on_ratings` with epochs=0.\n",
    "    mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "        baseline_state_path=BASELINE_PATH,\n",
    "        ratings_df=mds.df,              # original, unedited data\n",
    "        mds=mds,\n",
    "        config=config,\n",
    "        epochs=0,                       # <- no training\n",
    "        eval_split=None,\n",
    "        item_set=None,\n",
    "        eval_specs=[],\n",
    "        k=TOPK,\n",
    "    )\n",
    "    # Make sure we keep the original splits we loaded\n",
    "    return mds, user_train, user_valid, user_test, model_s\n",
    "\n",
    "if os.path.exists(BASELINE_PATH) and os.path.exists(MDS_PATH) and os.path.exists(SPLITS_PATH):\n",
    "    print(\"[baseline] Found existing baseline pack — loading.\")\n",
    "    mds, user_train, user_valid, user_test, baseline_model = _load_baseline_pack()\n",
    "else:\n",
    "    print(\"[baseline] No baseline pack found — training once and saving.\")\n",
    "    # Your existing function that prepares data and trains the baseline:\n",
    "    mds, user_train, user_valid, user_test, baseline_model = prepare_baseline(config)\n",
    "    _save_baseline_pack(mds, user_train, user_valid, user_test, baseline_model)\n",
    "\n",
    "# If you want to distribute only the weights (and rebuild splits locally),\n",
    "# you can still rely on BASELINE_PATH below. For determinism, keep seeds as above.\n",
    "\n",
    "# ---------------- Rest of your pipeline (unchanged except path var) ----------------\n",
    "NUM_ITEMS = resolve_num_items(mds, baseline_model)\n",
    "\n",
    "U, user_index = compute_user_embeddings(baseline_model, user_train, NUM_ITEMS, config['max_len'])\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42).fit(U)\n",
    "labels = kmeans.labels_\n",
    "centroids = _cluster_centroids(U, labels, NUM_CLUSTERS)\n",
    "print(\"V_model =\", baseline_model.out.out_features)          # should be num_items+2\n",
    "print(\"num_items =\", NUM_ITEMS)\n",
    "print(\"expected V =\", NUM_ITEMS + 2)\n",
    "results = []\n",
    "\n",
    "for N in N_values:\n",
    "    for p in p_values:\n",
    "        # ---- Build two collectives A and B ONCE per (N, p) ----\n",
    "        if SEED_MODE == \"uniform\":\n",
    "            seedA = np.random.randint(NUM_CLUSTERS)\n",
    "            seedB = (seedA + np.random.randint(1, NUM_CLUSTERS)) % NUM_CLUSTERS\n",
    "        elif SEED_MODE == \"balanced_maxdist\":\n",
    "            i, j, d = _balanced_farthest_pair(centroids, labels, user_index, mds, metric=\"cosine\", balance_tol=0.7)\n",
    "            seedA, seedB = i, j\n",
    "        elif SEED_MODE == \"maxdist\":\n",
    "            i, j, d = _farthest_pair_indices(centroids, metric=\"cosine\")\n",
    "            seedA, seedB = i, j\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SEED_MODE={SEED_MODE}\")\n",
    "\n",
    "        C1 = build_collective_quota(seedA, labels, user_index, size=N, p=p, seed=42)\n",
    "        C2 = build_collective_quota(seedB, labels, user_index, size=N, p=p, seed=42)\n",
    "\n",
    "        # ---- Build fixed item sets from ORIGINAL ratings (via mds.df) ----\n",
    "        raw_A, enc_A = build_item_set_for_members(mds, C1, V=V_TARGET, min_support=3)\n",
    "        raw_B, enc_B = build_item_set_for_members(mds, C2, V=V_TARGET, min_support=3)\n",
    "        \n",
    "        print(f\"[item_set] A: raw={len(raw_A)} enc={len(enc_A)}  |  B: raw={len(raw_B)} enc={len(enc_B)}\")\n",
    "\n",
    "        # ---- Evaluate BASELINE HR@K for each collective on its own item_set ----\n",
    "        subset_ALL = user_test\n",
    "        g1_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_A, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        g2_base = float(hr_for_itemset(baseline_model, subset_ALL, enc_B, NUM_ITEMS, config['max_len'], TOPK))\n",
    "        print(g1_base)\n",
    "        print(g2_base)\n",
    "\n",
    "        from collections import Counter\n",
    "        cnt = Counter(i for seq in mds.user_train.values() for i in seq)\n",
    "        popA = sum(cnt[i] for i in enc_A)/len(enc_A)\n",
    "        popB = sum(cnt[i] for i in enc_B)/len(enc_B)\n",
    "        print(\"Mean frequency — A:\", popA, \" | B:\", popB)\n",
    "\n",
    "        # ---- Precompute edited dataframes ONCE per (N, p) ----\n",
    "        df0 = mds.df.copy()\n",
    "        df_A_prom = edit_ratings(df0, C1, raw_A, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_A_demo = edit_ratings(df0, C1, raw_A, action=\"demote\",  user_decoder=mds.user_decoder)\n",
    "        df_B_prom = edit_ratings(df0, C2, raw_B, action=\"promote\", user_decoder=mds.user_decoder)\n",
    "        df_B_demo = edit_ratings(df0, C2, raw_B, action=\"demote\",  user_decoder=mds.user_decoder)\n",
    "\n",
    "        # Example usage\n",
    "        check_edits(df0, df_A_prom, C1, raw_A, action=\"promote\", user_decoder=mds.user_decoder, label=\"A_prom\")\n",
    "        check_edits(df0, df_A_demo, C1, raw_A, action=\"demote\", user_decoder=mds.user_decoder, label=\"A_demo\")\n",
    "        check_edits(df0, df_B_prom, C2, raw_B, action=\"promote\", user_decoder=mds.user_decoder, label=\"B_prom\")\n",
    "        check_edits(df0, df_B_demo, C2, raw_B, action=\"demote\", user_decoder=mds.user_decoder, label=\"B_demo\")\n",
    "\n",
    "        # Helper to retrain and evaluate a single collective (averaged across epochs on TEST users)\n",
    "        def run_single(edited_df, members, item_set_enc, name, baseline_hr):\n",
    "            mds_s, tr_s, va_s, te_s, model_s = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=BASELINE_PATH,   # <— use saved baseline\n",
    "                ratings_df=edited_df,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=10,\n",
    "                eval_split=\"user_test\",\n",
    "                item_set=item_set_enc,\n",
    "                eval_specs=[{\"name\": name, \"members_idx\": user_test, \"item_set_enc\": item_set_enc, \"baseline_hr\": baseline_hr}],\n",
    "                k=TOPK\n",
    "            )\n",
    "            stats = getattr(model_s, \"_avg_eval_stats\", {}) or {}\n",
    "            s = stats.get(name, {}) or {}\n",
    "            return s.get('hr'), s.get('rel'), s.get('ct')\n",
    "\n",
    "        # Two-collective interventions (apply both edits then retrain once)\n",
    "        def run_joint(df_A_action, df_B_action, solo_A, solo_B):\n",
    "            df_joint = edit_ratings(mds.df.copy(), C1, raw_A, action=df_A_action, user_decoder=mds.user_decoder)\n",
    "            df_joint = edit_ratings(df_joint,        C2, raw_B, action=df_B_action, user_decoder=mds.user_decoder)\n",
    "            mds_j, tr_j, va_j, te_j, model_j = retrain_from_baseline_on_ratings(\n",
    "                baseline_state_path=BASELINE_PATH,   # <— use saved baseline\n",
    "                ratings_df=df_joint,\n",
    "                mds=mds,\n",
    "                config=config,\n",
    "                epochs=10,\n",
    "                eval_split=\"user_test\",\n",
    "                eval_specs=[\n",
    "                    {\"name\": \"A\", \"members_idx\": user_test, \"item_set_enc\": enc_A, \"baseline_hr\": g1_base, \"solo_hr\": solo_A},\n",
    "                    {\"name\": \"B\", \"members_idx\": user_test, \"item_set_enc\": enc_B, \"baseline_hr\": g2_base, \"solo_hr\": solo_B},\n",
    "                ],\n",
    "                k=TOPK,\n",
    "            )\n",
    "            stats = getattr(model_j, \"_avg_eval_stats\", {}) or {}\n",
    "            A = stats.get(\"A\", {}) or {}\n",
    "            B = stats.get(\"B\", {}) or {}\n",
    "            return A.get('hr'), A.get('rel'), A.get('ct'), B.get('hr'), B.get('rel'), B.get('ct')\n",
    "\n",
    "        # ---- Trials: SAME groups, repeated trainings ----\n",
    "        for trial in range(trials_per_case):\n",
    "            # Solo A (promote/demote)\n",
    "            g1_demo, g1_demo_rel, g1_demo_ct = run_single(df_A_demo, C1, enc_A, 'A', g1_base)\n",
    "            hr_mem_A, hr_nonmem_A = hr_split(baseline_model, user_test, enc_A, C1, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 A demote: members={hr_mem_A:.3f}, non-members={hr_nonmem_A:.3f}, overall={g1_demo:.3f}\")\n",
    "            g1_prom, g1_prom_rel, g1_prom_ct = run_single(df_A_prom, C1, enc_A, 'A', g1_base)\n",
    "            # Solo B\n",
    "            g2_demo, g2_demo_rel, g2_demo_ct = run_single(df_B_demo, C2, enc_B, 'B', g2_base)\n",
    "            hr_mem_B, hr_nonmem_B = hr_split(baseline_model, user_test, enc_B, C2, NUM_ITEMS, config['max_len'], TOPK)\n",
    "            print(f\"[sanity] HR@10 B demote: members={hr_mem_B:.3f}, non-members={hr_nonmem_B:.3f}, overall={g2_demo:.3f}\")\n",
    "            g2_prom, g2_prom_rel, g2_prom_ct = run_single(df_B_prom, C2, enc_B, 'B', g2_base)\n",
    "\n",
    "\n",
    "            # Two-collective: both promote / both demote / criss-cross\n",
    "            gA_pp, gA_pp_rel, gA_pp_ct, gB_pp, gB_pp_rel, gB_pp_ct = run_joint(\"promote\", \"promote\", g1_prom, g2_prom)\n",
    "            gA_dd, gA_dd_rel, gA_dd_ct, gB_dd, gB_dd_rel, gB_dd_ct = run_joint(\"demote\", \"demote\", g1_demo, g2_demo)\n",
    "            gA_pd, gA_pd_rel, gA_pd_ct, gB_pd, gB_pd_rel, gB_pd_ct = run_joint(\"promote\", \"demote\", g1_prom, g2_demo)\n",
    "            gA_dp, gA_dp_rel, gA_dp_ct, gB_dp, gB_dp_rel, gB_dp_ct = run_joint(\"demote\", \"promote\", g1_demo, g2_prom)\n",
    "\n",
    "            results.append({\n",
    "                \"N\": N, \"p\": p, \"trial\": trial,\n",
    "                \"gA_base\": g1_base, \"gB_base\": g2_base,\n",
    "                \"gA_prom\": g1_prom, \"gA_demo\": g1_demo,\n",
    "                \"gB_prom\": g2_prom, \"gB_demo\": g2_demo,\n",
    "                \"gA_pp\": gA_pp, \"gB_pp\": gB_pp,\n",
    "                \"gA_dd\": gA_dd, \"gB_dd\": gB_dd,\n",
    "                \"gA_pd\": gA_pd, \"gB_pd\": gB_pd,\n",
    "                \"gA_dp\": gA_dp, \"gB_dp\": gB_dp,\n",
    "                \"rA_prom\": g1_prom_rel, \"ctA_prom\": g1_prom_ct,\n",
    "                \"rA_demo\": g1_demo_rel, \"ctA_demo\": g1_demo_ct,\n",
    "                \"rB_prom\": g2_prom_rel, \"ctB_prom\": g2_prom_ct,\n",
    "                \"rB_demo\": g2_demo_rel, \"ctB_demo\": g2_demo_ct,\n",
    "                \"rA_pp\": gA_pp_rel, \"ctA_pp\": gA_pp_ct,\n",
    "                \"rB_pp\": gB_pp_rel, \"ctB_pp\": gB_pp_ct,\n",
    "                \"rA_dd\": gA_dd_rel, \"ctA_dd\": gA_dd_ct,\n",
    "                \"rB_dd\": gB_dd_rel, \"ctB_dd\": gB_dd_ct,\n",
    "                \"rA_pd\": gA_pd_rel, \"ctA_pd\": gA_pd_ct,\n",
    "                \"rB_pd\": gB_pd_rel, \"ctB_pd\": gB_pd_ct,\n",
    "                \"size_A\": len(C1), \"size_B\": len(C2)\n",
    "            })\n",
    "\n",
    "            # free between trials\n",
    "            gc.collect()\n",
    "            if DEVICE == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "# Save & preview\n",
    "import pandas as pd\n",
    "res_df = pd.DataFrame(results)\n",
    "res_df.to_csv(\"rq2_results_relative_hr.csv\", index=False)\n",
    "display(res_df.head())\n",
    "print(\"Saved rq2_results_relative_hr.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collective-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
